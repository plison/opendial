\chapter{Learning from interactions}
\label{chap:rllearning}

\section{Bayesian Reinforcement Learning}

\footnote{Interestingly, offline and online approaches to planning are not mutually exclusive, but can be combined together to offer ``the best of both worlds''.  The idea is to perform offline planning to precompute a rough policy, and use this policy as a heuristic approximation to guide the search of an online planner \citep{ross2008}. These heuristic approximations can for instance be used to provide lower and upper bounds on the value function, which can be exploited to prune the lookahead tree.}

\subsection{Model-free methods}

\subsection{Model-based methods}

\section{Online planning}

\section{Experiments}

\label{sec:rllearning-experiments}

\subsection{Wizard-of-Oz data collection}

\subsection{User simulator}

\subsection{Experimental setup}

\subsection{Empirical results}

\subsection{Analysis}

\section{Conclusion}