\chapter{Probabilistic rules}
\label{chap:rules}

The previous chapter fleshed out how dialogue could be represented as a stochastic process and described the benefits of using graphical models to efficiently encode the probability and utility models employed in dialogue management. Plain graphical models must however face non-trivial scalability issues when applied to dialogue domains associated with rich conversational contexts. The number of parameters necessary for state update and action selection can indeed increase rapidly with the complexity of the domain models. Alas, only small quantities of genuine training data are available in most dialogue domains, and usually cover only a small fraction of the state-action space of interest. 

To address this discrepancy between the size of the parameter space and the amount of data available to estimate them, we introduce in this chapter a new approach to probabilistic dialogue modelling, based on the notion of \textit{probabilistic rules}.  Probabilistic rules are structured mappings between conditions and effects, and function as \textit{high-level templates} for the construction of a dynamic decision network.  The key advantage of this structured modelling approach is the drastic reduction of the number of parameters compared to traditional representations.  We also argue that these expressive representations are particularly well suited to encode the probability and utility models of dialogue domains, where substantial amounts of expert knowledge can often be leveraged to structure the relationships between variables. 

The chapter is divided in six sections. Section \ref{sec:rmotivation} exposes in general terms how structural assumptions can be applied to reduce the size and complexity of probabilistic models.  Section \ref{sec:formalisation} defines the formalism of probabilistic rules and its main theoretical properties.  These definitions are then connected in Section \ref{sec:ruleinstantiation} to the graphical models described in the previous chapter by showing how the rules are practically instantiated in the Bayesian network representing the dialogue state. Section \ref{sec:processing-workflow} explains how this instantiation procedure is incorporated in the processing workflow for updating the dialogue state and selecting the system actions. Finally, Section \ref{sec:amodelling} addresses some advanced modelling questions and Section \ref{sec:relatedwork} relates the approach to previous work.


\section{Structural leverage}
\label{sec:rmotivation}

The starting point of our approach is the observation that the probability and utility models used in dialogue management often exhibit a fair amount of \textit{internal structure}.  
We have already discussed in the previous chapter one simple instance of this internal structure, namely factored representations based on conditional independences. However, the internal structure of dialogue domains does not limit itself to these basic independence assumptions, and much can be gained by exploiting other types of structural properties, as shall be argued in the next pages. 

%If two sets of random variables $\mathbf{X}$ and $\mathbf{Y}$ are conditionally independent given $\mathbf{Z}$, we can rewrite the probability distribution $P(\mathbf{X}, \mathbf{Y} \, | \, \mathbf{Z})$ as $P(\mathbf{X} \, | \, \mathbf{Z}) (\mathbf{Y} \, | \, \mathbf{Z}) $.  


\subsubsection*{Latent variables}
 
The number of parameters required to estimate the distributions of a graphical model can often be reduced by introducing \textit{latent variables} (i.e. unobserved or hidden variables) that act as intermediaries between the source and target variables. Indeed, many application domains are often best explained by the combination of a small number of distinct factors or influences, each encoded by a separate random variable and associated with a subset of input and output variables. This layer of latent variables is usually never observed directly but contribute to structuring the model.\footnote{The construction of layered computational models is one of the most active research topic in artificial intelligence and machine learning, and form in particular the foundations of deep learning approaches \citep{Bengio:2009}.} In the particular case of medical diagnosis, the relations between predisposing factors and observed symptoms are for instance best described by postulating an intermediary layer of variables -- possible diseases -- that mediates between the predisposing factors and the observed symptoms.  Figure \ref{fig:latentvariables} illustrates how latent variables can be exploited to provide an additional layer of abstraction within a graphical model.

 \begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/latentvariables.pdf}
\caption{Comparison between a model that directly maps variables $\mathbf{Y}$ to $\mathbf{X}$ (left side) and one relying on latent variables $\mathbf{L}$ to serve as intermediaries (right side).}
\label{fig:latentvariables}
\end{figure}

Dialogue models can benefit from the inclusion of such latent variables. The transition function can for instance be modelled in terms of a limited number of latent variables, each responsible for capturing specific aspects of the interaction dynamics.  We shall see in the forthcoming sections that probabilistic rules precisely operate as latent variables when instantiated in the dialogue state. 

\subsubsection*{Partitioning}

A random variable $X$ with parent variables $Y_1,...Y_m$ must specify a separate probability distribution for every possible assignment of values for the parent variables. In other words, the number of parameters required to specify the distribution $P(X \, | \, Y_1,..., Y_m)$ is exponential in the number of parents $m$. Fortunately, the values of these parents variables can be grouped into \textit{partitions} yielding similar outcomes for $X$. One can therefore directly define the conditional probability distributions on these groups rather than on the full enumeration of combined values for the parent variables. Partitioning is an example of \textit{abstraction mechanism} which can be used to reduce the model complexity and improve its ability to generalise to unseen examples. 

%Utility distributions can also partition the values of their dependent variables in a similar way.  

Figure \ref{fig:partitioning} illustrates such partitioning operation for the conditional probability distribution $P(\mathit{Fire} \, | \mathit{Weather}, \mathit{Rain})$.  The space of possible values for the parent variables is defined in this example as $Val(\mathit{Weather}) \times Val(\mathit{Rain})$ and contains 6 possible elements.  We can observe that this space can be split in two partitions: $\mathit{Rain}\!=\mathit{true} \lor \mathit{Weather}\!\neq\mathit{hot}$ and $\mathit{Rain}\!=\mathit{false} \land \mathit{Weather}\!=\mathit{hot}$. This partitioning allows a significant reduction of the number of parameters required for the conditional probability distribution.  It should be noted that grouping value assignments into partitions corresponds to a modelling choice and can degrade the model accuracy if the partitions do not reflect actual similarities in the predicted outcomes.

%As illustration, consider a minimalistic dialogue in a robot learning scenario where the robot can ask the user yes/no questions pertaining to the colour of one specific object (e.g. \utt{Is the object red?}). In this simple scenario, the state is represented with two variables: the user dialogue act $a_u = \{\mathit{yes,no}\}$ and a variable representing the object colour, $\mathit{colour} = \{\mathit{blue,green, ...}\}$ with $n$ possible colours.  The system actions take the form $a_m = \{ \mathit{VerifyColour(c)}: c \in \mathit{colour}\}$, where $VerifyColour(c)$ corresponds to asking the user whether the object is of colour $c$.  Since the object colour remains constant, the transition function for this domain is defined as $P(a_u'|a_m, \mathit{colour})$. The number of parameters required to specify the transition function is $n^2$ (since $a_m$ and $\mathit{colour}$ have $n$ possible values each). 

%However, one can reasonably assume in this example that the particular colour mentioned in the question is irrelevant to predict the next user dialogue act $P(a_u'|a_m,\mathit{colour})$ as long as it matches (or fails to match) the actual colour of the object.  Based on this assumption, one can divide the space $Val(a_m) \times Val(\mathit{colour})$ into two distinct partitions: 
%\begin{enumerate}
%\item One partition in which the verification question corresponds to the actual colour of the object: $\exists c: colour\!=\!c \land a_m\!=\!\mathit{VerifyColour(c)}$.
%\item One partition in which there is a mismatch between the colour mentioned in the question and the actual colour: $\exists c: colour\!=\!c \land a_m\!\neq\!\mathit{VerifyColour(c)}$.
%\end{enumerate}

%By abstracting over the specific colour mentioned in the question, partitioning allows us to drastically reduce the number of parameters required for the conditional probability distribution $P(a_u'|a_m,\mathit{colour})$ from a total of $n^2$ to only $2$. Figure \ref{fig:partitioning} illustrates this reduction. 


 \begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/partitioning.pdf}
\caption{Partitioning for the conditional probability distribution $P(\mathit{Fire} \, | \mathit{Weather}, \mathit{Rain})$.}
\label{fig:partitioning}
\end{figure}

Partitions must be both exhaustive (each combination of values for the parent variables must belong to one partition) and mutually exclusive (a combination of values can only belong to one partition).  As we can observe from the example, partitions can often be concisely expressed via logical conditions on the variable values.  A given assignment of values is then grouped in a partition if it satisfies the condition associated with it.

\subsubsection*{Quantification}

Many dialogue domains are composed of objects or entities related to one another. These domains are often difficult to directly encode by a fixed set of random variables, as the number of entities and relations may vary over time.  Examples of relational structures include: 
\begin{itemize}
\item Collections of physical objects in a visual scene, each described by specific features (colour, shape) and relations with other objects (e.g. spatial relations),
\item Indoor environments topologically structured in rooms and spaces in which to navigate. 
\item Stacks of tasks to complete by the agent, each task being possibly connected to other tasks via precedence or inclusion relationships.
\item Linguistic entities employed in the dialogue acts of the conversational partners, linked with one another through multiple syntactic, semantic, referential or pragmatic relations. 
\end{itemize}

First-order logic provides an excellent basis for representing and manipulating such relational structures, as it offers a principled language for (1) referring to objects connected with one another through functions and relations and (2) describing their properties in a concise way through the use of universal and existential quantifiers.\footnote{We shall not cover in this thesis the mathematical foundations of first-order logic, but the interested reader is invited to refer to e.g. \cite{gamut1991logic} for a formal overview of the logical concepts mentioned throughout this thesis.}

Graphical models represent relational domains by instantiating one random variable for every possible grounding of the functions and predicates defined for the domain for a collection of objects.\footnote{Such operation is akin to \textit{propositionalisation} in the terminology of first-order logic.}  A domain with two objects $o_1$ and $o_2$ and a relation $\mathit{leftOf}(x,y)$ will for instance generate the four groundings $\mathit{leftOf}(o_1,o_2)$, $\mathit{leftOf}(o_2,o_1)$ $\mathit{leftOf}(o_1,o_1)$ and $\mathit{leftOf}(o_2,o_2)$. The definition of probability and utility distributions that can handle the relational semantics of such representation is however problematic. In particular, generic properties and constraints such as $\forall x, \neg \mathit{leftOf}(x,x)$ and $\forall x, y, z, \mathit{leftOf}(x,y) \land \mathit{leftOf}(y,z) \Rightarrow \mathit{leftOf}(x,z)$ can be difficult to enforce at a global level, since classical probabilistic models offer no direct support for quantifiers.  Their expressive power is indeed intrinsically limited to propositional logic. 
 
The unification of first-order logic and probability theory has spanned a new research area called \textit{statistical relational learning} \citep{getoor:srlbook07}. A common trait of most approaches to statistical relational learning is the definition of a logic-based description language which is employed as a template to generate classical probabilistic models given a set of constants. The introduction of quantifiers provide an abstraction mechanism to reduce the complexity of probability and utility models by describing constraints or relations that hold for all possible groundings of a given formula and can therefore apply to large sets of random variables. 

\section{Formalisation}
\label{sec:formalisation}

We now outline a generic description framework for expressing the various types of internal structure we have just detailed.  This description framework revolves around the notion of \textit{probabilistic rules}.The framework was originally presented in \cite{rulebasedmodels-sigdial2012,lison-semdial2012}.

The key idea is to represent distributions with the help of \textit{if ... then ... else} control structures, based on the following skeleton:
\begin{equation*}
\begin{aligned}
& \textbf{if} \ \ (\text{condition 1 holds}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \text{Distribution 1 over possible effects} \\
& \textbf{else if} \ \ (\text{condition 2 holds}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \text{Distribution 2 over possible effects} \\
& ... \\
& \textbf{else} \\
& \;\;\;\;\; \text{Distribution } n \text{ over possible effects} \\ 
\end{aligned}
\label{eq:probrule}
\end{equation*}

Each \textit{if ... then} branch specifies both a \textit{condition} on particular state variables and an associated distribution over possible \textit{effects}.   The \textit{if ... then ... else} structure is read in sequential order, as in programming languages, until a satisfied condition is found, which causes the activation of the corresponding probabilistic effects. 

We first present how probabilistic rules can express conditional probability distributions in terms of structured mappings between input and output variables.  We then show how to  generalise the formalism to utility distributions and extend it with quantification mechanisms.

A terminological note is here in order: we shall use the term \textit{probabilistic rules} as an umbrella term that covers all types of rules in this thesis, while \textit{probability rules} will only refer to rules expressing probability distributions over effects, and \textit{utility rules} to rules expressing utility distributions.

\subsection{Probability rules}
\label{sec:probabirules}

Probability rules take the form of \textit{if ... then ... else} control structures and map a list of conditions on input variables to probabilistic effects on output variables. More formally, a rule is expressed as an ordered list $\langle br_1, ... br_{n}\rangle$, where each branch $br_i$ is a pair $(c_i, P(E_i))$, $c_i$ is a condition and $P(E_i)$ an associated distribution over possible effects.  The distribution $P(E_i)$ is a categorical distribution with possible effects $Val(E_i) = \{e_{(i,1)},... e_{(i,m_i)}\}$, where $m_i$ is the number of alternative effects in $P(E_i)$.  Each effect $e_{(i,j)} \in Val(E_i)$ has a particular probability denoted $p_{(i,j)}$. 

Given these elements, a basic probability rule reads as such:
\begin{equation}
\begin{aligned}
& \textbf{if} \ \ (c_{1}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(E_1\!=\!e_{(1,1)}) = p_{(1,1)} \\
 ... \\
P(E_1\!=\!e_{(1,m_1)}) = p_{(1,m_1)} 
\end{cases} \\[3mm]
& \textbf{else if} \ \ (c_{2}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(E_2\!=\!e_{(2,1)}) = p_{(2,1)} \\
 ... \\
P(E_2\!=\!e_{(2,m_2)}) = p_{(2,m_2)}
\end{cases} \\ 
& ...  \\
& \textbf{else} \\
& \;\;\;\;\; \begin{cases}
P(E_{n}\!=\!e_{(n,1)}) = p_{(n,1)} \\
... \\
P(E_{n}\!=\!e_{(n,m_n)}) = p_{(n,m_n)}
\end{cases}
\end{aligned}
\label{eq:probrule}
\end{equation}

 In the rest of this thesis, we will often use $P(e_{(i,j)})$ as notational convenience for $P(E_i = e_{(i,j)})$.   

%The three following sections describe how the conditions $c_i$, effects $e_{(i,j)}$ and associated effect probabilities $p_{(i,j)}$ are respectively defined. 

\subsubsection*{Conditions}

The conditions $c_i$ are expressed as logical formulae grounded in a subset of random variables defined in the dialogue state. This subset of state variables are the \textit{input variables} of the rule, which we shall denote as $I_1,...I_{k}$. Conditions can be arbitrarily complex logical formulae connected by conjunction, disjunction and negation, and (as we shall see in Section \ref{sec:quantification}) can also include universally quantified variables. The examples $(\mathit{Rain}\!=\mathit{true} \lor \mathit{Weather}\!\neq\mathit{hot})$ and $(\mathit{Rain}\!=\mathit{false} \land \mathit{Weather}\!=\mathit{hot})$ in Figure \ref{fig:partitioning} are instances of valid conditions on the two input variables $Rain$ and $\mathit{Weather}$. 

Given that a rule is defined through a \textit{if ... then ... else} control structure, the partitioning is guaranteed by construction to be exhaustive and mutually exclusive (only one branch will be followed).  When provided with an assignment of values on the input variables, the conditions are tested in sequential order until one is satisfied. When no terminating \textbf{else} block is explicitly specified at the end of a rule, the framework assumes a final \textbf{else} block associated with a void effect to ensure that the partitioning is exhaustive. The last condition $c_n$ is thus guaranteed to be always trivially satisfied irrespective of the input variable values. 

The conditions on the input variables offer a compact partitioning of the state space to mitigate the dimensionality curse.  Without this partitioning in alternative conditions, a rule ranging over input variables $I_1,...I_{k}$ each containing $q$ possible values would need to enumerate $q^{k}$ possible assignments.  Partitioning this space reduces this number to $n$ mutually exclusive partitions, where $n$ corresponds to the number of conditions for the rule and is usually small. 


\subsubsection*{Effects}

Associated to each condition $c_i$ stands a collection of mutually exclusive effects $e_i^{1...m_i}$. Each effect $e_{(i,j)}$ defines a specific assignment of values for a set of variables called the \textit{output variables} of the rule.  An effect is defined as a conjunction of (variable,value) pairs $O_1\!=\!o_1 ... \land O_{l}\!=\!o_{l}$ where $O_1,... O_{l}$ are the output variables (which may already exist or yet to be created) and $o_1,...o_{l}$ the corresponding values for these variables. In the partitioning example from Figure \ref{fig:partitioning}, the output variable is unique and corresponds to $Fire$. We shall however encounter examples of rules with more than one output variable. 

Effects can be void -- that is, represent an empty assignment.  In such case, the effect does not lead to any change in the distribution of the output variables for the rule. 

\subsubsection*{Probabilities}

Each effect $e_{(i,j)}$ is assigned with a probability $p_{(i,j)} = P(E_i = e_{(i,j)})$ that must satisfy the usual probability axioms $p_{(i,j)} \geq 0  \ \forall i,j$ and $\sum_{j = 1}^{m_i} p_{(i,j)} = 1 \ \forall i$.  The probabilities can be either fixed by hand or correspond to parameters to estimate from data.  Chapters \ref{chap:wozlearning} and \ref{chap:rllearning} detail how Dirichlet distributions can be exploited to estimate probability parameters. 

%In the latter case, we adopt a Bayesian learning approach (cf. Section \ref{sec:learning}) and assume the probabilities $p_i^{1...m_i}$ to be drawn from the conjugate prior of categorical distributions, namely Dirichlet distributions.

\subsubsection*{Example}

Rule $r_1$ illustrates a simple example of probability rule:
\begin{align*}
r_1: \ \ \ \ \ & \textbf{if} \ (\mathit{Rain}\!=\!\mathit{false} \land \mathit{Weather}\!=\!\mathit{hot}) \ \textbf{then} \\
& \;\;\;\;\;  \begin{cases}
 P(\mathit{Fire}\!=\!\mathit{true}) = 0.03 \\ 
P(\mathit{Fire}\!=\!\mathit{false}) = 0.97
\end{cases} \\ 
& \textbf{else} \\
& \;\;\;\;\; \begin{cases}
P(\mathit{Fire}\!=\!\mathit{true}) = 0.01 \\
P(\mathit{Fire}\!=\!\mathit{false}) = 0.99
\end{cases} 
\end{align*}

Rule $r_1$ has two input variables: $\mathit{Rain}$ and $\mathit{Weather}$ as well as one output variable $\mathit{Fire}$. The rule specifies that the probability of a fire is 0.03 in case of no rain and a hot weather and 0.01 in all other cases.  The rule structure enables the conditional probability distribution for $\mathit{Fire}$ to be specified with only four probabilities in comparison to twelve for the original CPD (Figure \ref{fig:partitioning}). 

%Here is a first example of probabilistic rule pertaining to the user action model: 
%\begin{align*}
%\textbf{Rule 1}: \ \ & \textbf{if} \ (\exists X: a_m=\mathit{Confirm(X)} \land i_u = \mathit{X})  \ \textbf{then} \\ 
%& \;\;\;\;\; \{[P(a_u' = \mathit{Confirm}) = 0.9]\} \\
%& \textbf{else if} \ (\exists X: a_m=\mathit{Confirm(X)} \land i_u \neq \mathit{X})  \ \textbf{then} \\ 
%& \;\;\;\;\; \{[P(a_u' = \mathit{Disconfirm}) = 0.95]\}
%\end{align*}
%The rule specifies that, if the system requests the user to confirm that his intention is $X$ and his actual intention is indeed $X$, the user is expected to utter a $\mathit{Confirm}$ action with probability 0.9.  Otherwise, the rule produces a void effect -- i.e. it leaves the distribution $P(a_u')$ unchanged. If the intention is different, the user will utter a $\mathit{Disconfirm}$ action with  probability 0.95.   

\subsection{Utility rules}

The rule-based formalism we have outlined can also be used to express utility distributions with only minor notational changes. Utility rules essentially retain the same form as probability rules, with one important exception, namely that the probabilistic effects are replaced by utility distributions over particular assignments of decision variables. 

Formally, a utility rule is an ordered list $\langle br_1, ... br_n\rangle$, where each branch $br_i$ is a pair $(c_i, U_i)$ where $c_i$ is a condition and $U_i$ an associated utility distribution over possible assignments of decision variables. The utility distribution $U_i$ specifies a set of possible decisions $d_{(i,1)},... d_{(i,m_i)}$.  Each decision $d_{(i,j)}$ has a particular utility value denoted $u_{(i,j)}$.  Utility rules can be expressed in the following manner:
\begin{equation}
\begin{aligned}
& \textbf{if} \ \ (c_{1}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
U_1(d_{(1,1)}) = u_{(1,1)} \\
 ... \\
U_1(d_{(1,m_1)}) = u_{(1,m_1)} 
\end{cases} \\[3mm]
& \textbf{else if} \ \ (c_{2}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
U_2(d_{(2,1)}) = u_{(2,1)} \\
 ... \\
U_2(d_{(2,m_2)}) = u_{(2,m_2)} 
\end{cases} \\
& ...  \\
& \textbf{else} \\
& \;\;\;\;\; \begin{cases}
U_n(d_{(n,1)}) = u_{(n,1)} \\
... \\
U_n(d_{(n,m_n)}) = u_{(n,m_n)}
\end{cases}
\end{aligned}
\end{equation}

A utility rule assigns utility values to particular system decisions depending on conditions on the state variables.  As for probability rules, the conditions $c_i$ are defined as arbitrary logical formulae on input variables $I_1,... I_k$.  The decisions $d_{(i,j)}$ are assignments $A_1\!=\!a_1 ... \land A_{l}\!=\!a_{l}$ where the variables $A_1,..A_{l}$ are decision variables and $a_1,...a_{l}$ possible values for these variables. The utility values $u_{(i,j)}$ are real numbers (which may be positive or negative).  

Although most utility rules only include one single decision variable, the possibility to integrate multiple decision variables is helpful in domains where the system can execute multiple actions in parallel. Such situations can arise in human-robot interaction and multi-modal applications, as the system can communicate through both verbal and non-verbal channels and is often able to perform physical actions in addition to communicative acts. 
 
\subsubsection*{Example}

Rule $r_2$ provides a simple example of utility rule:

\begin{align*}
r_2: \ \ & \textbf{if} \ (\mathit{Fire}\!=\!\mathit{true}) \ \textbf{then} \\
& \;\;\;\;\;  \begin{cases}
U(\mathit{Tanker}\!=\!\mathit{drop\mbox{-}water}) = 5 \\
U(\mathit{Tanker}\!=\!\mathit{wait}) = -5
\end{cases} \\
& \textbf{else} \\
& \;\;\;\;\; \begin{cases}
U (\mathit{Tanker}\!=\!\mathit{drop\mbox{-}water}) = -1 \\
U(\mathit{Tanker}\!=\!\mathit{wait}) = 0
\end{cases}
\end{align*}

Rule $r_2$ stipulates the respective utilities of the two possible utility values for the decision variable $\mathit{Tanker}$ depending on the variable $\mathit{Fire}$. 

\subsection{Quantification}
\label{sec:quantification}

Quantification is a powerful mechanism to abstract over particular relational aspects of the domain structure.  Logical variables can be included in the specification of both the conditions and effects of a given rule, and are universally quantified on top of the rule.\footnote{These variables are variables in the sense of first-order logic, and are not to be confused with the random variables of the probabilistic model.}  A rule containing the quantified variables $y_1...y_p$ in its conditions and/or effects is therefore formalised as:
\begin{equation}
\begin{aligned}
\forall \mathbf{y} = y_1, y_2,...y_p: \\
& \textbf{if} \ \ (c_{1}(\mathbf{y})) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(E_1\!=\!e_{(1,1)}(\mathbf{y})) = p_{(1,1)} \\
 ... \\
P(E_1\!=\!e_{(1,m_1)}(\mathbf{y})) = p_{(1,m_1)} 
\end{cases} \\[3mm]
& \textbf{else if} \ \ (c_{2}(\mathbf{y})) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(E_2\!=\!e_{(2,1)}(\mathbf{y})) = p_{(2,1)} \\
 ... \\
P(E_2\!=\!e_{(2,m_2)}(\mathbf{y})) = p_{(2,m_2)} 
\end{cases} \\ 
& ...  \\
& \textbf{else} \\
& \;\;\;\;\; \begin{cases}
P(E_n\!=\!e_{(n,1)}(\mathbf{y})) = p_{(n,1)} \\
... \\
P(E_n\!=\!e_{(n,m_n)}(\mathbf{y})) = p_{(n,m_n)}
\end{cases}
\end{aligned}
\label{eq:rulewithquant}
\end{equation}

The formalisation allows specific elements $y_1,...y_p$ in the conditions and effects to be \textit{underspecified}.  The mapping between conditions and effects specified by the rule holds for every possible assignment of the underspecified variables. Based on this quantification mechanism, probabilistic rules can cover large portions of the state space in a highly compact manner, based on a reduced number of parameters. One of the key advantages of such representation is that it allows for powerful forms of \textit{parameter sharing}, as the effect probabilities $p_{(i,j)}$ in the above rule are made independent of the various possible instantiations of the variables $y_1,...y_p$. Quantification also applies to utility rules in the same manner. 

\subsubsection*{Example}

Rule $r_3$ provides a simple example of probability rule including a quantified variable:
\begin{align*}
r_3: \ \ & \forall y: \\ 
& \textbf{if} \ (\mathit{shape}(y) = \mathit{sphere})  \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(\mathit{graspable}(y)\!=\!\mathit{true}) = 0.9 \\
P(\mathit{graspable}(y)\!=\!\mathit{false}) = 0.1
\end{cases} \\ 
&\textbf{else if} \ (\mathit{shape}(y) = \mathit{cone})  \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(\mathit{graspable}(y)\!=\!\mathit{true}) = 0.2 \\
P(\mathit{graspable}(y)\!=\!\mathit{false}) = 0.8
\end{cases}
\intertext{Rule $r_3$ specifies how the graspability of a given object $y$ depends on its shape (a sphere being easier to grasp than a cone). Similarly, rule $r_4$ defines the utility of a grasping action depending on the task and object graspability:}
r_4: \ \ & \forall y: \\ 
& \textbf{if} \ (task= grasp(y) \land \mathit{graspable}(y) = \mathit{true})  \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
U(a_m\!=\!\mathit{grasp}(y)) = 2 \\
\end{cases} \\
&\textbf{else} \\
& \;\;\;\;\; \begin{cases}
U(a_m\!=\!\mathit{grasp}(y)) = -2 \\
\end{cases} 
\end{align*}
Rule $r_4$ associates an utility of 2 to the action of grasping an object $y$ when it corresponds to the task and is feasible. Grasping the object in any other case results in a negative utility of -2. 

%The careful reader may notice that quantified variables can be placed both inside the name of particular variables, as in $\mathit{graspable}(y)$, as well as inside the variable values, as in $grasp(y)$.  

%Assume you want to predict the next user dialogue act $a_u'$ after a system-initiated question such as ``What colour is the object'' depending on the colour of the object that is referred to. Rule $r_3$ is an example of rule that employs quantification on two variables denoted $o$ and $c$.\footnote{Note the absence of an explicit final \textit{else} branch, which is then by default associated with an empty effect.}
%\begin{align*}
%r_3: \ \ & \forall o, c: \\ 
%& \textbf{if} \ (a_m\!=\!\mathit{WhatIsColour}(o) \land \mathit{colour}(o)\!=\!\mathit{c})  \ \textbf{then} \\ 
%& \;\;\;\;\; \begin{cases} P(a_u'=\mathit{AssertProperty}(o,\mathit{c})) = 0.95 \end{cases}
%\end{align*}

%Rule $r$ assumes the presence in the dialogue state of a random variable $colour(o_i)$ for each object $o_i$ perceived by the system.  The rule specifies that the user is likely (with probability 0.95) to utter a dialogue act such as ``the object is X'' at the next turn, where X is the actual colour of the object. Otherwise no prediction is made. If the dialogue state contains e.g. one object $o_1$ with $P(\mathit{colour}(o_1)=blue)=0.8$ and the last system action was $WhatIsColour(o_1)$, the probability of the user uttering ``the object is blue'' is therefore $0.76$.  

\section{Rule instantiation}
\label{sec:ruleinstantiation}

We represent the dialogue state as a Bayesian network over state variables, in line with other dialogue management approaches such as \cite{Thomson:2010:BUD:1772996.1773040,bui2010}. Rules are then applied at runtime on this dialogue state.  The instantiation is performed by creating a distinct node for every rule to apply. These rule nodes are essentially latent variables that serve as intermediaries between input and output variables.  Albeit the presence of these rules is never directly observed, they help structuring the relations between variables and enable the system designer to decompose complex probability and utility models into smaller parts.   % The instantiated rules are equivalent to a dynamic decision network (cf. previous chapter). 

We describe below the instantiation procedure for each type of rule. For the sake of clarity, we shall first limit our discussion to rules without quantifiers, and then demonstrate how quantifiers can be accounted for in the instantiation process. 

\subsection{Probability rules}
\label{sec:probruleinstantiation}

Let $\mathcal{B}$ be the Bayesian network representing the current dialogue state, and $\mathcal{R}$ a set of rules to apply to this dialogue state.  For each rule $r \in \mathcal{R}$, a distinct chance node is created.\footnote{The original instantiation algorithm presented in \citep{rulebasedmodels-sigdial2012} included two separate nodes: one for the rule condition and one for the effect.  The formalism was later simplified to one single node.} This chance node represents a random variable defined on the possible effects of the rule.  The node is conditionally dependent on the input variables of the rule (i.e. the set of all variables that are mentioned in the rule conditions), and is also connected via outgoing edges to its output variables (i.e. the set of all variables that are mentioned in the rule effects). 

Figure \ref{fig:instantiationprob} illustrates this construction process on a constructed example composed of the two rules $r_5$ and $r_6$.  To simplify the rule representation, we shall usually omit the explicit specification of the probability for the empty effect in the effect distributions.  The remaining probability mass in the rules is thus by default assigned to the empty effect.

The two rules $r_5$ and $r_6$ are applied on the state variables $A$, $B$, $C$ and $D$.  The application of the two rules results in an update of the variable $A$ and the creation of a new variable $E$. The nodes corresponding to the output variables are by convention denoted with a prime to distinguish them from the input nodes.  

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/ruleinstantiation.pdf}
\caption{Example of instantiation for the two probability rules $r_5$ and $r_6$. }
\label{fig:instantiationprob}
\end{figure}

The random variable represented by the node $r_5$ has three possible values, reflecting the effects described in the rule: $Val(r_5) = \{ \{A\!=\!a_1\}, \{A\!=\!a_2\}, \{\cdot\}\}$, where $\{\cdot\}$ denotes the empty effect.  Similarly, the random variable $r_5$ has four alternative effects: $Val(r_6) = \{\{A\!=\!a_2 \land E\!=\!e_2\}, \{A\!=\!a_2 \land E\!=\!e_1\}, \{E=e_2\}, \{\cdot\}\}$. 

We shall also adopt the following terminology to denote the probability distributions created through the instantiation procedure: 
\begin{itemize}
\item The conditional probability distribution associated with rule nodes such as $r_5$ and $r_6$ given their inputs is a \textit{rule distribution}.
\item The conditional probability distribution associated with output variables such $A'$ and $E'$ given the rule nodes that determines is an \textit{output distribution}.
\end{itemize}

\subsubsection*{Rule distributions}

The rule distributions directly reflect the rule semantics.  Formally, the conditional probability distribution of a rule node $r$ given its input variables $I_1,...I_k$ is defined as: 
\begin{align}
& P(r\!=\!e \, | \, I_1\!=\!i_1,... I_k\!=\!i_k) = P(E_i = e) \label{eq:ruledistrib}
 \\ 
& \; \; \; \; \; \; \; \; \text{ where } i = \min_i (c_i \text{ is satisfied with } I_1\!=\!i_1 \land ... I_k\!=\!i_k) \nonumber 
\end{align}
Formally speaking, a condition $c_i$ is said to be satisfied iff the input assignment logically entails that the condition is true, that is :$(I_1\!=\!i_1 \land ... I_k\!=\!i_k) \vdash c_i$. The rule conditions are checked in sequential order until one condition is found to be satisfied. The rule distribution is then simply determined as the effect distribution $P(E_i\!=\!e)$ associated with the first satisfied condition $c_i$.  As the last condition $c_n$ corresponds to the final \textbf{else} block and is therefore always trivially true, there will always be at least one satisfied condition. 

As an example, the rule distribution $P(r_5 \, | \, B\!=\!b_1, C\!=\!c_1)$ for the node $r_5$ in Figure \ref{fig:instantiationprob} is defined as:
\begin{itemize}
\item $P(r_5 = \{A\!=\!a_1\} \, | \, B\!=\!b_1, C\!=\!c_1) = 0.6$
\item  $P(r_5 = \{\cdot\} \, | \, B\!=\!b_1, C\!=\!c_1) = 0.4$
%\item $P(r_5 = \{A\!=\!a_2\} \, | \, B\!=\!b_1, C\!=\!c_1) = 0$
\end{itemize}
Similarly, the distribution $P(r_6 \, | \, C\!=\!c_1, D\!=\!d_1)$ is a distribution with the empty effect $\{\cdot\}$ assigned to a probability 1. 

\subsubsection*{Output distributions} 

An output node $X'$ is conditionally dependent on all the rule nodes that refer to the variable $X$ in their effects.  In addition, output nodes that correspond to the updated version of existing nodes (such as $A'$ in the example of Figure \ref{fig:instantiationprob}) also include a conditional dependence on these existing nodes. The output distribution is a reflection of the combination of effects specified in the parent rules. The conditional probability distribution $P(X'|r_1\!=\!e_1,...r_n\!=\!e_n)$ for an output variable $X'$ with $n$ incoming rule nodes is defined in the following manner:
\begin{align}
&P(X'\!=\!x' \, | \, r_1\!=\!e_1,... r_n\!=\!e_n) = \begin{dcases}
\frac{\sum_{v \in {\mathbf{e}(X)}} \mathbf{1}(x' = v)} { |\mathbf{e}(X)| } & \text{if } \mathbf{e}(X)\!\neq\!\emptyset \\
\mathbf{1}(x' = \mathit{None}) & \text{otherwise}
\end{dcases}
\label{eq:outputdist1}
\end{align}
where the following notation is used: \begin{itemize}
\item $\mathbf{e}$ is the conjunction of all effects, i.e. $\mathbf{e} = e_1 \land ... e_n$.  This conjunction can include more than one assigned value for a particular variable.
\item $\mathbf{e}(X)$ denotes the (possibly empty) list of values specified for the variable $X$ in $\mathbf{e}$. 
\item $\mathbf{1}(b)$ is the indicator function for the Boolean $b$, with $\mathbf{1}(b)=1$ if $b$ is true and $0$ otherwise.
\end{itemize}

Equation \eqref{eq:outputdist1} stipulates that the distribution for $X'$ will follow the values assigned in the effect(s) provided that at least one effect specifies a value for it. If the effects include conflicting assignments, the distribution is spread uniformly over the alternative values. If no effects $e_1,...e_n$ specifies a value for $X$ , the value for $X'$ is set to a default $None$ value with probability 1. 

If the node $X'$ is an update of an existing node $X$, the procedure remains essentially the same as for \eqref{eq:outputdist1}, except in the case where all the effects specify empty assignments for the variable. In such case, the distribution for $X'$ will fall back to the value defined for the existing node $X$ instead of being assigned a $\mathit{None}$ value:
\begin{align}
&P(X'\!=\!x' \, | \, r_1\!=\!e_1,... r_n\!=\!e_n, X\!=\!x) = \begin{dcases} 
\frac{\sum_{v \in {\mathbf{e}(X)}} \mathbf{1}(x' = v)} { |\mathbf{e}(X)| }  & \text{if } \mathbf{e}(X)\!\neq\!\emptyset \\
\mathbf{1}(x' = x) & \text{otherwise}
\end{dcases}\label{eq:outputdist2}
\end{align}

As an example, the output distribution $P(A' \, | \, r_5\!=\!\{\cdot\},r_6\!=\!\{A\!=\!a_2 \land E\!=\!e_2\}, A\!=\!a_3)$ in Figure \ref{fig:instantiationprob} results in a deterministic distribution with a unique value $a_2$ with probability 1, since $\mathbf{e}(A) = \{a_2\}$. If the two rules generate conflicting assignments, the probability mass is divided equally over the alternative values. The output distribution $P(A' \, | \, r_5\!=\!\{A\!=\!a_1\},r_6\!=\!\{A\!=\!a_2 \land E\!=\!e_2\}, A\!=\!a_3)$ provides two alternative values for $A$: $\mathbf{e}(A) = \{a_1,a_2\}$. The output distribution is thus a uniform distribution with two values: $a_1$ and $a_2$, each with probability 0.5. Finally, if all effects are empty, the output distribution is a simple copy of the distribution for the existing variable: $P(A' \, | \, r_5\!=\!\{\cdot\},r_6\!=\!\{\cdot\}, A\!=\!a_3)$ has a unique value $a_3$ with probability 1. 

Output distributions are directly derived from the effects in the rule nodes and are thus entirely parameter-free.  The ordering of the parent rules in the conditional probability distribution is arbitrary. The resulting distribution bears resemblance to the probabilistic Independence of Causal Influence (pICI) described by \cite{diez06}. 


\subsubsection*{Instantiation algorithm} 
\label{sec:utilruleinstantiation}

The procedure for instantiating a rule in a given dialogue state is detailed in Algorithm \ref{algo:instantiateProbRule}. 

\begin{algorithm}[h!]
\caption{: \textsc{InstantiateProbRule} ($\mathcal{B}, \mathit{r}$)}
\begin{algorithmic}[1] \vspace{1mm}
\REQUIRE Bayesian network $\mathcal{B}$ for the current state
\REQUIRE Probability rule $\mathit{r}$ to instantiate in network  \vspace{1mm}
\STATE $I_1,...I_k \leftarrow$ input variables for $\mathit{r}$
\STATE Create chance node $r$ with the rule distribution in Eq. \eqref{eq:ruledistrib}
\STATE Add node $r$ and dependency edges $I_1,...I_k \rightarrow r$ to $\mathcal{B}$ 
\IF {$Val(r) = \{\cdot\}$}
\STATE Prune $r$ from $\mathcal{B}$
\ELSE
\STATE $O_1,...O_l \leftarrow$ output variables mentioned in the effects of $r$
\FORALL {variable $O \in O_1,...O_l$}
\IF {$O'$ not already in $\mathcal{B}$}
\STATE Create chance node $O'$ with the output distribution in Eq. \eqref{eq:outputdist1}-\eqref{eq:outputdist2}
\STATE Add node $O'$ and (in case $O$ exists) dependency edge $O \rightarrow O'$ to $\mathcal{B}$
\ENDIF
\STATE Add dependency edge $r \rightarrow O'$ to $\mathcal{B}$ 
\ENDFOR
\ENDIF
\end{algorithmic}
\label{algo:instantiateProbRule}
\end{algorithm}

The first steps of the instantiation process are to extract in the Bayesian network the input variables of the rule (line 1), create a node corresponding to the rule (line 2) and include its dependency edges in the network (line 3).  The algorithm then checks whether at least one effect in $r$ is non-empty given its conditional dependences (lines 4).  If all effects are empty, the rule node is irrelevant and can be directly pruned (line 5). Otherwise, the output variables are extracted (line 7), and output nodes that do not already exist in the network are created (line 10-11). The final step is to establish dependency edges between the rule node and these output variables (line 13).


\subsection{Utility rules}

Utility rules are instantiated in the Bayesian network according to a similar procedure, with two notable differences compared to probability rules: \begin{itemize}
\item As utility rules define utility distributions, their instantiation correspond to utility nodes instead of chance nodes.
\item Instead of output variables, the rules are associated with decision variables.  The association direction is inverted, as the decision node must be input to the utility node.
\end{itemize} 

The result of the instantiation process is a decision network that incorporates chance nodes (corresponding to the state variables), utility nodes (corresponding to the utility rules) and associated decision nodes. Figure \ref{fig:instantitionutil} illustrates the instantiation of two utility rules $r_7$ and $r_8$. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/ruleinstantiation2.pdf}
\caption{Example of instantiation for the two utility rules $r_7$ and $r_8$.}
\label{fig:instantitionutil}
\end{figure}


The utility distribution associated with each rule is a direct translation of the  \textit{if ... then ... else} rule.  Formally, the utility distribution generated by a rule $r$ with input variables $I_1,...I_k$ and decision variables $A_1,...A_l$ is defined as:
\begin{align}
& U_r(I_1\!=\!i_1,... I_k\!=\!i_k, A_1\!=\!a_1,... A_l\!=\!a_l) = U_i(A_1\!=\!a_1 \land... A_l\!=\!a_l) \label{eq:utildistrib}\\
&  \; \; \; \; \; \; \; \;  \; \; \; \text{ where } i = \min_i (c_i \text{ is satisfied with } I_1\!=\!i_1 \land ... I_k\!=\!i_k) \nonumber
\end{align}

If no utility is explicitly specified for $A_1\!=\!a_1 \land... A_l\!=\!a_l$, the default value is zero. 

As is conventionally assumed in decision networks, the total utility for a given assignment of decision variables is defined as the \textit{sum} of all utilities.  In case $A\!=\!a_1$, $B=\!=\!b_1$ and $C\!=\!c_1$, we can therefore calculate the total utility for the actions $D'\!=\!d_1 \ \land \ E'\!=\!e_1$ to be equal to $2 - 4 = -2$. 


\subsubsection*{Instantiation algorithm} 

The procedure for instantiating a utility rule is similar in most respects to the one already outlined for probability rules. Algorithm \ref{algo:instantiateUtilRule} details the procedure, starting from the extraction of the input variables, the creation of the rule node, and the inclusion of conditional dependences (line 1-3). The algorithm then checks if the utility distribution stipulates a non-zero utility for at least one decision (line 4).  If the answer is negative, the node is essentially irrelevant and can be pruned (line 5).  The decision variables associated with the rule are extracted (line 7), and a corresponding decision node is created if it does not already exists (line 10). Finally, the possible values specified for the decision variable are integrated to the node (line 12), and a dependency edge is established between the decision node and the utility node for the rule (line 13). 

\begin{algorithm}[h!]
\caption{: \textsc{InstantiateUtilRule} ($\mathcal{B}, \mathit{r}$)}
\begin{algorithmic}[1] \vspace{1mm}
\REQUIRE Bayesian network $\mathcal{B}$ for the current state
\REQUIRE Utility rule $\mathit{r}$ to instantiate in network  \vspace{1mm}
\STATE $I_1,...I_k \leftarrow $ input variables for $rule$
\STATE Create utility node $r$ with the utility distribution in Eq. \eqref{eq:utildistrib}
\STATE Add node $r$ and dependency edges $I_1,...I_k \rightarrow r$ to $\mathcal{B}$ 
\IF {utility distribution is empty for all inputs}
\STATE Prune $r$ from $\mathcal{B}$
\ELSE
\STATE $A_1,...A_l \leftarrow$ decision variables mentioned in the effects of $r$
\FORALL {variable $A \in A_1,...A_l$}
\IF {$A'$ not already in $\mathcal{B}$}
\STATE Create decision node $A'$ and add it to $\mathcal{B}$
\ENDIF
\STATE Add in $Val(A')$ the action values specified in the effects of $r$
\STATE Add dependency edge $A' \rightarrow r$ to $\mathcal{B}$ 
\ENDFOR
\ENDIF
\end{algorithmic}
\label{algo:instantiateUtilRule}
\end{algorithm}

\subsection{Quantification}
\label{sec:applicationquantif}

We saw in Section \ref{sec:quantification} that conditions and effect could include universally quantified variables, but have not yet discussed how such underspecified rules could be practically instantiated in the Bayesian network. The general instantiation principle remains unchanged: to each rule corresponds a distinct rule node responsible for the mapping between input and output variables (or decision variables for utility rules). The instantiation procedure must be however extended to accommodate the presence of quantified variables.  The key idea is to find all relevant \textit{groundings} for the quantified variables, and then calculate the effect distribution for each grounding. This method of handling quantifiers by extracting all possible groundings and reasoning at the propositional level is an instance of \textit{ground inference} \citep{getoor:srlbook07}. 

\subsubsection*{Extraction of input variables}

Universally quantified rules may underspecify both the names and values of random variables, as we saw in the examples of Section \ref{sec:quantification}.  Rule $r_3$ includes for instance a reference to an underspecified random variable $\mathit{shape}(y)$.  In order to instantiate the rule, the system must therefore first determine all random variables included in the model that match the underspecified description. If rule $r_3$ is instantiated on a state containing two objects $o_1$ and $o_2$, the resulting input variables will therefore be $\mathit{shape}(o_1)$ and $\mathit{shape}(o_2)$. 

Algorithm \ref{algo:getinputvariables} outlines how this search for matching input variables can proceed. The first step is to extract the initial input variables associated with the rule, which may include underspecified descriptions such as $\mathit{shape}(y)$. The algorithm then loops on each underspecified description to find possible groundings in the random variables of the Bayesian network.  The final result corresponds to the combination of the fully specified input variables for the rule and the groundings for the underspecified variables.  

\begin{algorithm}[h!]
\caption{: \textsc{GetInputVariables} ($\mathcal{B}, \mathit{r}$)}
\begin{algorithmic}[1] \vspace{1mm}
\REQUIRE Bayesian network $\mathcal{B}$ for the current state
\REQUIRE Probability or utility rule $\mathit{r}$ \vspace{1mm}
\STATE $\mathcal{I}_{r} \leftarrow $ Initial (possibly underspecified) input variables for $\mathit{r}$
\STATE $\mathcal{U}_{r} \leftarrow $ Subset of variable names in $\mathcal{I}_{r}$ that are underspecified
\STATE $\mathcal{G}_{r} \leftarrow$ Set of possible groundings for $\mathcal{U}_{r}$, initially empty
\FOR {underspecified variable name $u \in \mathcal{U}_{r}$}
\FOR {random variable $X$ in $\mathcal{B}$}
\IF {$X$ matches $u$}
\STATE $\mathcal{G}_{r} \leftarrow \mathcal{G}_{r} \cup [X]$
\ENDIF
\ENDFOR
\ENDFOR
\RETURN $(\mathcal{I}_{r} \ / \ \mathcal{U}_{r}) \ \cup \  \mathcal{G}_{r}$
\end{algorithmic}
\label{algo:getinputvariables}
\end{algorithm}

 Line 1 in Algorithms \ref{algo:instantiateProbRule} and \ref{algo:instantiateUtilRule} is then replaced by: 
\begin{algorithmic}[1] \vspace{1mm}
\STATE $I_1,...I_k \leftarrow $ \textsc{GetInputVariables} ($\mathcal{B}, \mathit{r}$)
\end{algorithmic}


%Line 1 in Algorithms \ref{algo:instantiateProbRule} and \ref{algo:instantiateUtilRule} is then replaced by: 
%\begin{algorithmic}[1] \vspace{1mm}
%\STATE $I_1,...I_k \leftarrow $ \textsc{GetInputVariables} ($\mathcal{B}, \mathit{r}$)
%\end{algorithmic}

\subsubsection*{Extraction of relevant groundings}

Once the input variables for the rules are retrieved, the next step is to establish the set of relevant groundings $G$ for the universally quantified variables.  The groundings are always determined relative to a particular assignment of values for the (grounded) input variables $I_1,...I_k$.

Given an input assignment $\{I_1\!=\!i_1 \land ... I_k\!=\!i_k\}$, the set of groundings $G$ are derived in a heuristic manner, by checking which ground terms (constants and functions of constants) from the input assignment can function as proper substitutions for the quantified variables.  These ground terms define the domain of discourse for the application of the universal quantifier. The collection of relevant ground terms is then combined into subsets of size $p$, where $p$ corresponds to the number of universally quantified variables $\mathbf{y} = y_1, ... y_p$. These combinations form the groundings $G$ for the input assignment. 


%The groundings are always determined relative to a particular assignment of values for the input variables. Given such input assignment, the groundings are defined as substitutions of quantified variables to ground terms that result in the satisfaction of at least one condition. Let us assume a rule with input variables $I_1,...I_k$ and quantified over $\mathbf{y}$. Given a particular assignment for the input variables $I_1,...I_k$, each condition $c_i$ is checked to find the possible groundings of its quantified variables that lead to the satisfaction of the condition.\footnote{The search for possible groundings is limited to a set of ground terms (constants and functions of constants) determined in a heuristic manner, based on the ground terms used in the parent variables of the rule node.}  The number of groundings can be zero if the condition cannot be satisfied. As illustrated in Algorithm \ref{algo:getgroundings}, the result of this process is a set of groundings $G$ that leads to the satisfaction of at least one condition when substituted to the quantified variables in this condition. The expression $\phi[a/b]$ denotes (as in formal logic) the formula $\phi$ where all instances of $a$ are substituted by $b$.

% specified in the input assignment. For instance, the assignment $\mathit{shape}(o_1)\!=\!\mathit{sphere} \land \mathit{shape}(o_1)\!=\!\mathit{cone}$ includes a total of six ground terms: $o_1$, $o_2$, $\mathit{sphere}$, $\mathit{cone}$, $\mathit{shape}(o_1)$ and $\mathit{shape}(o_1)$.
 
% For instance, the assignment $\mathit{shape}(o_1)\!=\!\mathit{sphere} \land \mathit{shape}(o_1)\!=\!\mathit{cone}$ includes 6 ground terms: $o_1$, $o_2$, $\mathit{sphere}$, $\mathit{cone}$, $\mathit{shape}(o_1)$ and $\mathit{shape}(o_1)$.

 %The possible groundings for a given input must be in this setting restricted to a finite set. In order to retain tractability, the maximum number of groundings is also capped by a threshold.
 %Groundings that satisfy a condition leading to an empty effect can be discarded. 
 
%\begin{algorithm}[h!]
%\caption{: \textsc{GetGroundings} ($\mathit{r}, \{I_1\!=\!i_1 \land ... I_k\!=\!i_k\}$)}
%\begin{algorithmic}[1] \vspace{1mm}
%\REQUIRE $\mathit{r}$: Probability or utility rule 
%\REQUIRE $\{I_1\!=\!i_1 \land ... I_k\!=\!i_k\}$ assignment for the parent variables of the rule  \vspace{1mm}
%\STATE $G \leftarrow \emptyset$
%\FORALL {condition $c_i$ in $\mathit{r}$}
%\STATE Let $\mathbf{y}_i \subset \mathbf{y} $ be the quantified variables present in $c_i$
%\STATE $G_i \leftarrow \{\text{groundings } \mathbf{g} $ such that $ \ c_i [\mathbf{y}_i / \mathbf{g} ] \text{ is satisfied with } I_1\!=\!i_1 \land ... I_k\!=\!i_k \}$
%\STATE $G \leftarrow G \cup G_i$
%\ENDFOR
%\RETURN $G$
%\end{algorithmic}
%\label{algo:getgroundings}
%\end{algorithm}


\subsubsection*{Quantified probability rules}

As we have seen, probability rules are instantiated as chance nodes associated with a rule distribution.  For rules containing universal quantifiers, each grounding in $G$ gives rise to a particular distribution over effects.  The instantiation procedure generates a distinct effect distribution for each grounding $\mathbf{g}_j$: 
\begin{align}
& P(r_{\mathbf{g}_j}\!=\!e' \, | \, I_1\!=\!i_1,... I_k\!=\!i_k) = P(E_i = e) \label{eq:quantifruledistrib}
 \\
& \; \; \; \; \; \; \; \; \text{where } i = \min_i (c_i[\mathbf{y} / \mathbf{g}_j]\text{ is satisfied with } I_1\!=\!i_1 \land ... I_k\!=\!i_k) \nonumber \\ 
& \; \; \; \; \; \; \; \; \text{and } e' = e [\mathbf{y} / \mathbf{g}_j] \nonumber
\end{align}

The expression $\phi[a/b]$ denotes (as in formal logic) the formula $\phi$ where all instances of $a$ are substituted by $b$.

Empty and redundant effect distributions are discarded. The grounding procedure results in a  collection of distributions $ \langle P(r_{\mathbf{g}_1}),... P(r_{\mathbf{g}_p}) \rangle$.    The final conditional probability distribution for the rule node is then defined as the joint distribution over these grounding-specific distributions: 
\begin{align}
& P(r\!=\![e_1 \land ... e_{q}] \, | \, I_1\!=\!i_1,... I_k\!=\!i_k) = \prod_{j=1}^{q} P(r_{\mathbf{g}_j}\!=\!e_j)
\end{align}

%The values of the rule node are thus conjunctions of effects $[e_1 \land ... e_q]$.  

%They can however include conflicting assignments, which are in such case automatically ``resolved'' in the output distribution by assigning the probability mass uniformly to the alternative values (as explained in the previous section).

Figure \ref{fig:quantinstantitionprob} shows how the rule $r_3$ is instantiated in a state with two objects $o_1$ and $o_2$, each associated with a random variable describing its shape.

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/quantruleinstantiation.pdf}
\caption{Instantiation of the probability rule $r_3$ on a state with two objects $o_1$ and $o_2$.}
\label{fig:quantinstantitionprob}
\end{figure}

As an example, the probability distribution $P(r_3 \, | \, \mathit{shape}(o_1)\!=\!\mathit{sphere}, \mathit{shape}(o_2)\!=\!\mathit{cone})$ has two relevant groundings $y\!=\!o_1$ and $y\!=\!o_2$, from which four alternative effects are derived: \begin{itemize}
\item $\{\mathit{graspable}(o_1)\!=\!true \land \mathit{graspable}(o_2)\!=\!true \} $ with probability $0.9 \times 0.2 = 0.18$ 
\item $\{\mathit{graspable}(o_1)\!=\!true \land \mathit{graspable}(o_2)\!=\!false\}$ with probability $0.9 \times 0.8 = 0.72$ 
\item $\{\mathit{graspable}(o_1)\!=\!false \land \mathit{graspable}(o_2)\!=\!true \}$ with probability $0.1 \times 0.2 = 0.02$ 
\item $\{\mathit{graspable}(o_1)\!=\!false \land \mathit{graspable}(o_2)\!=\!false\}$ with probability $0.1 \times 0.8 = 0.08$. 
\end{itemize} 

%The effects specified for $r_3$ mention two output variables: $\mathit{graspable}(o_1)$ and $\mathit{graspable}(o_1)$.  These two variables are thus created as children of the node $r_3$. 

\subsubsection*{Quantified utility rules}

Utility rules can be similarly extended to accommodate quantified variables. As for probability rules, the instantiation of universally quantified utility rules proceeds by determining a set of groundings and generating a particular utility distribution for each.\footnote{The extraction of groundings is slightly modified for utility rules in order to integrate the ground terms appearing in both the input and decision assignments.}
 For a utility rule with input variables $I_1,...I_k$, decision variables $A_1,...A_l$ and quantified variables $\mathbf{y}$,
 
 \begin{align}
& U_{\mathbf{g}_j}(I_1\!=\!i_1,... I_k\!=\!i_k, A_1\!=\!a_1',... A_l\!=\!a_l') = U_i(A_1\!=\!a_1 \land... A_l\!=\!a_l) 
 \\
& \; \; \; \; \; \; \; \;   \; \; \;\text{where } i = \min_i (c_i[\mathbf{y} / \mathbf{g}_j]\text{ is satisfied with } I_1\!=\!i_1 \land ... I_k\!=\!i_k) \nonumber \\
& \; \; \; \; \; \; \; \;   \; \;  \text{and } a_1' = a_1[\mathbf{y} / \mathbf{g}_j], ... \ \ a_l' = a_l[\mathbf{y} / \mathbf{g}_j] \nonumber
\end{align}

After discarding empty and redundant distributions, the result is a set of utility distributions $ \langle U_{\mathbf{g}_1},... U_{\mathbf{g}_p} \rangle$. The total utility distribution for the rule is then constructed by adding up the grounding-specific utility distributions:   
\begin{align}
& U_{r}(I_1,... I_k, A_1\,... A_l) = \sum_{j=1}^{q} U_{\mathbf{g}_j}(I_1,... I_k, A_1,... A_l) \label{eq:quantifruledistrib}
\end{align}


The result of instantiating rule $r_4$ on a state with two objects $o_1$, $o_2$ with associated random variables $\mathit{graspable}(o_1)$ and $\mathit{graspable}(o_2)$ is shown in Figure  \ref{fig:quantinstantitionutil}.  

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/quantutilruleinstantiation.pdf}
\caption{Instantiation of the quantified utility rule $r_9$ on a state with two objects $o_1$ and $o_2$.}
\label{fig:quantinstantitionutil}
\end{figure}

The utility distribution $U_{r_4}(\mathit{task}\!=\!\mathit{grasp}(o_1), \mathit{graspable}(o_1) \!=\!\mathit{true}, \mathit{graspable}(o_2)\!=\!\mathit{false}, a_m)$ assigns the action $a_m\!=\!\mathit{grasp}(o_1)$ to a utility of 2, while the action $a_m\!=\!\mathit{grasp(o_2)}$ is assigned to a utility of -2. 

\subsubsection*{Tractability aspects}

Although the use of universally quantifiers greatly improves the expressivity of probabilistic rules, they also tend to increase the in- and out-degrees of rule nodes (that is, the cardinality of their parents and children nodes). Approximate inference techniques are thus necessary to handle this conditional structure in a tractable manner. Sampling methods such as likelihood weighting have in practice proved to work well in this setting (cf. Section \ref{sec:inference}).

It should be stressed that the groundings are always extracted \textit{given a specific value assignment} for the input variables. By restricting the groundings to this limited domain of discourse, we ensure that the number of alternative effects remains bounded and avoid the generation of spurious effects.   This instantiation procedure was found to be much more efficient than copying the rule in distinct nodes, as investigated in earlier implementations of the formalism \citep{relational-apl2012}.  

 %In order to keep the number of groundings under control, the specification of probabilistic rules should nevertheless refrain from introducing more universally quantified variables than necessary.

%This procedure departs from other frameworks such as Markov Logic Networks, where the functions and predicates are duplicated for every possible grounding of variables in the domain \citep{Richardson:2006}.


\section{Processing workflow}
\label{sec:processing-workflow}

The two previous sections detailed how probability and utility rules are internally defined, and how they can be instantiated as nodes of a graphical model. We are now ready to explain how collections of rules are practically applied at runtime to update the dialogue state and perform action selection. The general workflow is strongly inspired by information-state approaches to dialogue management \citep{Larsson:2000:ISD:973935.973943,Buckley:2006}, as the dialogue state serves as a central blackboard monitored by various groups of rules that are ``triggered'' upon relevant changes. 

%We start by describing how dialogue domains can be organised in collections of rules called ``models''. We then show how these models are triggered to update state variables and express the utility of particular actions. We also demonstrate the general processing workflow on a detailed example. 


\subsection{Domain representation}

Dialogue domains can consist of multiple probability and utility rules. These rules are internally grouped in collections of rules called \textit{models}. A model is simply a collection of rules that is associated with one or more ``trigger'' variables that specify when the model is to be instantiated. Each model is attached the dialogue state and monitors it to detect changes affecting their trigger variables. When these trigger variables are changed at runtime by another module, they lead to the instantiation of the model rules. Formally, a model $m$ is defined as a pair $\langle \mathcal{T}_m, \mathcal{R}_m \rangle$ where $\mathcal{T}_m$ corresponds to the trigger variables and $\mathcal{R}_m$ to the rules included in the model.

A dialogue domain is represented as a pair $\langle \mathcal{B}_0, \mathcal{M} \rangle$, where $\mathcal{B}_0$ is the initial dialogue state  and $\mathcal{M}$ the set of rule-based models attached to it. The organisation of rules into models allows the system designer to structure the application pipeline in a modular manner. Each model can be intuitively viewed as a distinct component responsible for a particular inference or decision step. 

Section \ref{sec:domain-specification} explains how dialogue domains are practically encoded in the \opendial architecture. 

\subsection{Update algorithm} 

The software architecture adopted in this thesis takes the form of an event-driven, blackboard architecture \citep{jaspis2004,Buckley:2006} revolving around a dialogue state $\mathcal{B}$ represented as a Bayesian network.  As in information state approaches, this dialogue state is read and written by the various modules integrated in the dialogue system. 

The update procedure is shown in Algorithm \ref{algo:stateupdate}. The procedure is started upon the reception of new variables to incorporate in the state, such as new user inputs processed by the ASR/NLU components. The first step is to insert the variables in the dialogue state and possibly relate them to their predicted values (lines 2-3). The algorithm then triggers the instantiation of the relevant domain models (line 4), leading to a recursive chain of updates.  If the expanded dialogue state contains decision and utility variables, the algorithms searches for the optimal action, selects it, and activates the models that are triggered as a result  (lines 6-8). Finally, the updated state is reduced by pruning away unnecessary nodes and incorporating the evidence (line 10). 


\begin{algorithm}[h]
\caption{: \textsc{UpdateState} ($\mathcal{B}, \mathbf{X}$)}
\begin{algorithmic}[1] \vspace{1mm}
\REQUIRE Bayesian network $\mathcal{B}$ for the current state
\REQUIRE New random variables $\mathbf{X}$ to insert in the state \vspace{1mm}
\STATE Initialise evidence $\mathbf{e} \leftarrow \emptyset$
\STATE Insert $\mathbf{X}'$ to the current state $\mathcal{B}$ 
\STATE $\mathcal{B}, \mathbf{e} \leftarrow $ \textsc{IntegratePredictions}($\mathcal{B}, \mathbf{e}, \mathbf{X}'$)
\STATE $\mathcal{B}, \mathbf{e} \leftarrow$ \textsc{TriggerModels} ($\mathcal{B}, \mathbf{e},  \mathbf{X}'$) \vspace{1mm}
\WHILE {$\mathcal{B}$ contains decision variables}
\STATE $\mathbf{a}^* \leftarrow $ \textsc{SelectAction} ($\mathcal{B}, \mathbf{e}$)
\STATE Assign $\mathbf{A}' = \mathbf{a}^*$
\STATE $\mathcal{B}, \mathbf{e} \leftarrow$ \textsc{TriggerModels} ($\mathcal{B}, \mathbf{e}, \mathbf{A}'$)
\ENDWHILE \vspace{1mm}
\STATE $\mathcal{B} \leftarrow \textsc{PruneState} (\mathcal{B}, \mathbf{e})$ \vspace{1mm}
\end{algorithmic}
\label{algo:stateupdate}
\end{algorithm}

We now describe each of these steps in detail.


\subsubsection*{Connecting predictions and observations}

Rules are sometimes used to provide predictions on variables that will be observed in the next time steps.\footnote{This is notably the case for the user action model $P(a_u \, | \, i_u, a_m)$, which estimate the relative probabilities for the next dialogue action from the user. The prediction provide a prior on the future observation of the user action.} In order to distinguish random variables that express a prediction on a future outcome from those that reflect an actual (although possibly uncertain) observation, we denote predictive variables with a subscript $p$. A variable $X_p$ is thus a prediction for the future observation of the variable $X$. 

\begin{wrapfigure}[11]{r}{48mm}
\vspace{-2mm}
\centering
\includegraphics[scale=0.25]{imgs/prediction.pdf} 
\vspace{-2mm}
\caption{Equivalence node $eq_{X}$ with parents $X$ and $X_p$.}
\label{fig:prediction}
\end{wrapfigure}

Prediction and observation variables must be connected with one another at runtime.  In the case where the observation is known with certainty, this connection can simply be represented as an assignment of evidence values.  However, dialogue often include observations that are themselves uncertain and represent ``soft'' or virtual evidence.  Several techniques are available to practically encode this evidence. The method adopted in this thesis is to add a new boolean-valued chance node, subsequently called the \textit{equivalence node} $eq_{X}$, that is conditionally dependent on both $X$ and $X_p$, as shown in Figure \ref{fig:prediction}. The conditional probability distribution of $eq_X$ is deterministic (graphically depicted by a double circle around the chance node): 
\begin{equation}
P(eq_{X}\!=\!true \, | \, X\!=\!x, X_p\!=\!x_p) = \begin{cases}
1 & \text{if } x = x_p \\
0 & \text{otherwise}
\end{cases} \label{eq:equivdistrib}
\end{equation}

The use of a distinct node to express the evidence is motivated by the fact that $X$ and $X_p$ can have arbitrary incoming and outgoing edges with other variables. 

The assignment $eq_{X} \!=\! true$ is added to the evidence. The posterior distribution given the evidence allows the prediction to act as a prior for the observed distribution:
\begin{align}
&P(X = x \, | eq_{X}\!=\!true) \nonumber \\
&=  \alpha \ P(X\!=\!x)  \sum_{x_p \in Val(X_p)} P(eq_{X}\!=\!true | X\!=\!x, X_p \!=\!x_p ) P(X_p\!=\!x_p) \\
&= \alpha \ P(X\!=\!x) \ P(X_p\!=\!x) \label{eq:equivalence}
\end{align}

The inclusion of an equivalence node between $X$ and $X_p$ with evidence  $[eq_{X}\!=\!true]$ modifies the distribution of the variables $X$ and$X_p$ as well as their respective parents/children nodes.  Algorithm \ref{algo:integratepredictions} illustrates the process of integrating predictions for the variables $\mathit{Vars}$. 

\begin{algorithm}[h]
\caption{: \textsc{IntegratePredictions} ($\mathcal{B}, \mathbf{e}, \mathit{Vars}$)}
\begin{algorithmic}[1] \vspace{1mm}
\FORALL {$X \in \mathit{Vars}$}
\IF {there is a corresponding prediction variable $X_p \in \mathcal{B}$}
\STATE Create equivalence node $eq_{X}$ with distribution in Eq. \eqref{eq:equivdistrib}
\STATE Insert $eq_{X}$ in $\mathcal{B}$ with parents $\mathit{X}$ and $\mathit{X}_p$
\STATE Add assignment $[eq_{X}\!=\!true]$ to evidence $\mathbf{e}$
\ENDIF
\ENDFOR
\RETURN $\mathcal{B}, \mathbf{e}$
\end{algorithmic}
\label{algo:integratepredictions}
\end{algorithm}



\subsubsection*{Model instantiation}

After inserting the new variables in the dialogue state and connecting them to their predicted values, the next step in the processing workflow is to trigger the relevant domain models . 

Algorithm \ref{algo:triggerModels} summarises the steps involved in the instantiation of the domain models. The algorithm takes three arguments: a dialogue state $\mathcal{B}$ represented as a Bayesian network, an assignment of evidence values and a list of random variables that have been recently updated in the dialogue state. The algorithm loops on all domain models and instantiates the ones that are triggered by the updated variables. The rules are instantiated one by one, following the procedure we have outlined in the previous section. Once all models are traversed, the output variables of the instantiated rules become updated variables themselves, and the procedure is repeated until no more models can be applied.  To avoid the occurrence of infinite triggering cycles, models are limited to one instantiation per update. The algorithm returns both the dialogue state expanded with new variables, and the evidence assignment attached to the equivalence nodes. 


\begin{algorithm}[h]
\caption{: \textsc{TriggerModels} ($\mathcal{B}, \mathbf{e}, \mathit{UpdatedVars}$)}
\begin{algorithmic}[1] \vspace{1mm}
\WHILE {$\mathit{UpdatedVars} \neq \emptyset$}
\STATE $\mathit{NewVars} \leftarrow \emptyset$
\FORALL {models $m$}
\IF {$\mathit{UpdatedVars} \cap \mathcal{T}_m \neq \emptyset$ and $m$ has not yet been applied}
\FORALL {rule $r \in \mathcal{R}_m$}
\IF {$r$ is a probability rule}
\STATE $\mathcal{B} \leftarrow \textsc{InstantiateProbRule}(\mathcal{B},r)$
\ELSIF {$r$ is a utility rule}
\STATE $\mathcal{B} \leftarrow \textsc{InstantiateUtilRule}(\mathcal{B},r)$
\ENDIF
\STATE Let $\mathcal{O}_r$ be the new output variables created by rule $r$
\STATE $\mathit{NewVars} \leftarrow \mathit{NewVars} \cup \mathcal{O}_r$
\STATE $\mathcal{B}, \mathbf{e} \leftarrow $ \textsc{IntegratePredictions}($\mathcal{B}, \mathbf{e}, \mathcal{O}_r)$
\ENDFOR
\ENDIF
\ENDFOR 
\STATE $\mathit{UpdatedVars} \leftarrow \mathit{NewVars}$
\ENDWHILE 
\RETURN $\mathcal{B}, \mathbf{e}$
\end{algorithmic}
\label{algo:triggerModels}
\end{algorithm}


\subsubsection*{Action selection}

Whenever the new dialogue state contains utility and decision nodes, the system must decide on the action to perform.  Algorithm \ref{algo:actionselection} illustrates how actions can be selected on the basis of the current dialogue state augmented with the decision and utility nodes created by the utility rules. The algorithm searches for the assignment of action values that maximise the current utility given the dialogue state and the evidence and returns it. This utility maximisation is based on standard inference algorithms for decision networks such as likelihood weighting (cf. Section \ref{sec:inference}). 

The utility nodes are removed from the state once the decision is made. The action selection procedure described here only takes into account the current (immediate) utility and does not rely on any forward planning.  Chapter \ref{chap:rllearning} demonstrates how this procedure can be extended to perform online planning on a limited horizon.  \\

\begin{algorithm}[h]
\caption{: \textsc{SelectAction} ($\mathcal{B}, \mathbf{e}$)}
\begin{algorithmic}[1] \vspace{1mm}
\STATE Let $\mathbf{A}'$ be the set of all decision variables in $\mathcal{B}$
\STATE Find optimal value $\mathbf{a}^* = \argmax_{\mathbf{a}} U(\mathbf{A}' = \mathbf{a}, \mathbf{e})$
\STATE Remove utility nodes from the state $\mathcal{B}$
\RETURN $\mathbf{a}^*$
\end{algorithmic}
\label{algo:actionselection}
\end{algorithm}


\subsubsection*{State pruning}

The instantiation of the domain models results in the integration of numerous new nodes in the dialogue state. However, many nodes in this expanded Bayesian network  only serve as intermediaries and do not directly express meaningful information about the current state of the dialogue. The last step is therefore to reduce the dialogue state to its minimal size, by removing all intermediary nodes -- including rule nodes, outdated versions of state variables, equivalence nodes and predictive nodes that are attached to them -- in order to only retain current state variables. The accumulated evidence is also integrated in the posterior distribution of the state variables.

The procedure is outlined in Algorithm \ref{algo:pruneState}. The first step is to determine which nodes to keep (line 1-6).  Only the most recent versions of state variables are retained. 
The nodes are then added one by one in a new dialogue state $\mathcal{B}'$.  The parents of each variable is determined, and its conditional probability distribution is calculated given the evidence.  The parents of a state variable are the closest ancestors of the variable within the subset of nodes in $\mathit{NodesToKeep}$, and its conditional probability distribution is determined as $P_{\mathcal{B}}(N \, | \, \mathit{Parents}, \mathbf{e})$.  This posterior distribution is calculated via sampling techniques. This is done by sampling all nodes in $\mathcal{B}$, then deriving the distributions $P_{\mathcal{B}}(N \, | \, \mathit{Parents}, \mathbf{e})$ on the basis of the collected samples. 

\begin{algorithm}[h]
\caption{: \textsc{PruneState} ($\mathcal{B}, \mathbf{e}$)}
\begin{algorithmic}[1] \vspace{1mm}
\STATE $\mathit{NodesToKeep} \leftarrow \emptyset$
\FORALL {node $N \in \mathcal{B}$}
\IF {$N$ is a state variable and $\nexists \ N' \in \mathcal{B}$}
\STATE $\mathit{NodesToKeep} \leftarrow \mathit{NodesToKeep} \cup [N]$ 
\ENDIF
\ENDFOR
\STATE Create new state $\mathcal{B}' \leftarrow \emptyset$
\FORALL {node $N \in  \mathit{NodesToKeep}$}
\STATE Add node $N$ to $\mathcal{B}'$ (with primes removed from node name)
\STATE $\mathit{Parents} \leftarrow \{M \in \mathit{NodesToKeep} : M \text{ is an ancestor of } N \text{ and there is } $ \\ $\phantom{a}$  \; \; \; \; \; \; \; \; \;  a path $M \rightarrow^+  N \text{ without node in } \mathit{NodesToKeep} \}$ 
\STATE Add dependency edges between $\mathit{Parents}$ and $N$ in $\mathcal{B}'$
\STATE Assign distributions $P_{\mathcal{B}'}(N \, | \, \mathit{Parents}) \leftarrow P_{\mathcal{B}}(N \, | \, \mathit{Parents}, \mathbf{e})$
\ENDFOR
\RETURN $\mathcal{B}'$
\end{algorithmic}
\label{algo:pruneState}
\end{algorithm}


Figure \ref{fig:pruning} illustrates the input and output of the pruning process.  Note that the primes attached to the labels of output variables are deleted from the random variable names.

\begin{figure}[h]
\centering
\includegraphics[scale=0.20]{imgs/pruning.pdf}
\caption{Illustration of the state pruning process. Only the nodes $A'$, $B$, $C$, $D$ and $E'$ are retained. The dotted lines denote the correspondence between nodes.}
\label{fig:pruning}
\end{figure}


\subsection{Detailed example}
\label{sec:detailedexample}

We now describe a minimal but complete example of workflow for a short interaction. 

\subsubsection*{Description}

Assume a domain similar to the one shown in Figure \ref{fig:fsa}, where a user can request a robot to move forward, backward, left, right, or stop.  The set of dialogue acts $a_u$ that can be recognised by the system is the following: 
\begin{center}
$\{\mathit{Request(Forward)}$, $\mathit{Request(Backward)}$, $\mathit{Request(Left)}$, \\ $\mathit{Request(Right)}$, $\mathit{Request(Stop)}$, $\mathit{Other}\}$. \\
\end{center}
The corresponding system actions $a_m$ are: 
\begin{center}
$\{\mathit{Move(Forward)}$, $\mathit{Move(Backward)}$, $\mathit{Move(Left)}$, \\ $\mathit{Move(Right)}$, $\mathit{Move(Stop)}$, $\mathit{AskRepeat}\}$. 
\end{center}
The objective of the system is to fulfil the user command if it is reasonably confident regarding which action to execute.  Otherwise, the system asks the user to repeat. 

\subsubsection*{Domain specification}

The domain specification designed for this constructed example is constituted of an empty initial state and the following two rule-based models: \begin{itemize}
% = \langle \langle a_u \rangle, \langle r_9, r_{10} \rangle \rangle$
\item Model $m_1$ is triggered by $a_u$ and includes the two utility rules $r_{9}$ and $r_{10}$:
\begin{align*}
r_{9}: \ \ & \forall y: \\ 
& \textbf{if} \ (a_u = Request(y)) \ \textbf{then} \\ 
& \; \; \begin{cases} 
U(a_m = Move(y)) = 2 \\ 
\end{cases} \\
& \textbf{else} \\ 
& \; \; \begin{cases} 
U(a_m = Move(y)) = -2 \\ 
\end{cases} \\[4mm]
r_{10}: \ \ &  \; \; \begin{cases} U(a_m = \mathit{AskRepeat}) = 0.5 \end{cases}
\end{align*}

Rule $r_{9}$ specifies that the utility of executing the action corresponding to the user command is 2, with a penalty of $-2$ when the wrong action is executed. Rule $r_{10}$ assign a utility of 0.5 for asking a clarification question.\footnote{As the action selection process presented thus far does not perform forward planning, the utilities provided in this example correspond to long-term expected utilities (Q-values in the reinforcement learning terminology).}

%= \langle \langle a_m \rangle, \langle r_{11} \rangle \rangle
\item Model $m_2$ is triggered by $a_m$ and has one single predictive rule $r_{11}$: 
\begin{align*}
r_{11}: \ \ & \forall y: \\ 
& \textbf{if} \ (a_m = \mathit{AskRepeat} \land a_u=y) \ \textbf{then} \\ 
& \; \;  \begin{cases} 
P(a_{u\mbox{-}p} = y) = 0.9 \\ 
\end{cases}
\end{align*}
Rule $r_{11}$ specifies that the probability that the user will repeat his last utterance when asked by the system to do so is expected to be $0.9$.
\end{itemize}

\subsubsection*{Processing workflow}

We now detail the processing workflow associated with the following constructed interaction:
\begin{dialogue} 
\speak{User } Now move forward \\ $\phantom{b}$ $\tilde{a}_u = \langle (\mathit{Request(Forward)}, 0.6), (\mathit{Request(Backward)}), 0.4)\rangle$  \\[-3mm]
\speak{System } Could you please repeat? \\[-3mm]
\speak{User } Please move forward! \\ $\phantom{b}$ $\tilde{a}_u = \langle (\mathit{Request(Forward)}, 0.7), (\mathit{Other}, 0.2), (\mathit{Request(Backward)}, 0.1) \rangle$ \\[-3mm]
\speak{System } OK, moving forward!
\end{dialogue}
The (constructed) recognition hypotheses $\tilde{a}_u$ produced by the ASR/NLU components are written underneath the user utterance. 

Figure \ref{fig:detailedexample} details the steps involved in the state update procedure that follows from the reception of dialogue act hypotheses from the natural language understanding component. 

%Note that the probability and utility distributions shown in the figure are marginalised on their dependent variables.

\begin{figure}[p]
\centering
\includegraphics[scale=0.25]{imgs/detailedexample.pdf}
\caption{Detailed example of processing workflow. }
\label{fig:detailedexample}
\end{figure}
 
Step 1 inserts the new dialogue act hypotheses on the dialogue state.  This insertion triggers 
the utility model $m_1$. The instantiation results in Step 2 in the creation of two utility nodes and one decision node.  The optimal action to perform in such case is $\mathit{AskRepeat}$, which is selected by the system in Step 3. The action selection triggers model $m_2$ in Step 4, which creates a prediction node $a_{u\mbox{-}p}'$ expressing the expected probability distribution for the next user dialogue act. The state is finally pruned of the intermediary rule node in Step 5.  System components such as NLG can react on the updated state and generate the proper linguistic realisation of the system action. The system then waits for the user input, which is shown in Step 6.  The relation between the predicted and actual user response leads in Step 7 to the creation of an equivalence node, and the inclusion of the assignment $eq_{a_u} = true$ in the evidence. We notice that the combination of the prior distribution over predicted values and the actual distribution over dialogue act hypotheses increases the probability of $a_u' = \mathit{Request(Forward)}$. Step 8 triggers the model $m_1$ based on the new user input.  The optimal action is this case is $\mathit{Move(Forward)}$, which is selected in Step 9.  This selection triggers model $m_2$, but rule $r_{11}$ only generates in this case an empty effect and is therefore directly deleted. Finally, the state is pruned of its intermediary nodes in Step 10, retaining only the last user and system actions $a_u$ and $a_m$. 

In comparison to the finite-state solution present in Figure \ref{fig:fsa}, we observe that the rule-structured approach defined by models $m_1$ and $m_2$ allows the dialogue manager to 
accumulate evidence over time and prime the recognition hypotheses of the user dialogue act $a_u$ based on the previous dialogue act.  This accumulation of evidence is absent from the FSA, due its rigid state representation and lack of memory. 

\section{Advanced modelling}
\label{sec:amodelling}

Dialogue domains often include random variables with values expressed via specific data structures such as lists or strings. The rule-based formalism described in the previous sections can be easily complemented with special-purpose tools to efficiently operate on these data structures. We first explain how conditions and effects can be defined on variables that represent lists, and then discuss how rules can manipulate strings. 

\subsection{Operations on lists}

Some state variables are best represented as lists of elements. For instance, the dialogue state may include random variables that enumerate  the $n$ most recent dialogue acts in the interaction history, the stack of tasks that remain to perform, or the list of visual objects perceived by the system.  The range of values for such state variables is the power set of its possible elements. 


Special-purpose operators for the manipulation of such lists can be integrated in both the conditions and effects of probabilistic rules: 
\begin{itemize}
\item Rule conditions can include operators to check the presence or absence of particular elements in a list, such as $a \in A$ or $a \notin A$. 
\item Rule effects can also be augmented to manipulate elements from a list.  Three new types of effects are created to this end, in addition to the traditional assignment of output values: \textit{add effects} (adding an element to a list), \textit{delete effects} (deleting an element from a list) and \textit{clear effects} (clearing all elements of a list). The resulting lists are sorted by insertion order. 
\end{itemize}

Figure \ref{fig:seteffects} illustrates two rules that apply these new effects to update a state variable $A$. 
 
\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/seteffects.pdf}
\caption{Example of rules using add/delete effects to manipulate lists.}
\label{fig:seteffects}
\end{figure}

These new effects can be incorporated to the framework through a simple modification of the output distribution. Let $\mathbf{e}$ denote as before the conjunction of all effects $e_1 \land ... e_n$. In addition to the previously defined set of values $\mathbf{e}(X)$ assigned for the variable $X$, we construct two new sets of values $\mathbf{e}_{add}(X)$ and $\mathbf{e}_{del}(X)$ that represent the values that are respectively added and deleted for the variable $X$ through the new effects we just described. Note that $\mathbf{e}_{del}(X)$ may include all values for $X$ if the clear effect is applied. 

The output distribution in Equation \eqref{eq:outputdist2} is then rewritten as:
\begin{align}
&P(X'\!=\!x' \, | \, r_1\!=\!e_1,... r_n\!=\!e_n, X\!=\!x) = \hspace{5cm} \nonumber \\ & \; \; \; \; \; \; \; \; \; \;  \begin{dcases} 
\frac{\sum_{v \in {\mathbf{e}(X)}} \mathbf{1}(x' = v)} { |\mathbf{e}(X)| }  & \text{if } \mathbf{e}(X)\!\neq\!\emptyset \\
\mathbf{1}(x' = \left(\mathbf{e}_{add}(X) \cup \left(x \; / \; \mathbf{e}_{del}(X)\right)\right)) & \text{otherwise} \\
\end{dcases}\label{eq:outputdist4}
\end{align} 

The output distribution associated for a new variable (cf. Equation \eqref{eq:outputdist1}) can be rewritten in a similar manner.

%The assignment effects in $\mathbf{e}(X)$ and the add/delete effects in $\mathbf{e}_{add}(X)$ and $\mathbf{e}_{delete}(X)$ being mutually incompatible, the assignment effects are assumed to take precedence.

%Lists are defined to be equal if they include the same elements in the same order.

%Other types of collections such as sets with no duplicate elements can be exploited in the same manner. 

\subsection{Operations on strings}

Many of the data structures present in the dialogue state are strings -- the most prominent ones being the last user utterance $u_u$ and the last system utterance $u_m$. The integration of special-purpose functionalities for manipulating strings within the conditions and effects of probabilistic rules is therefore desirable. In particular, rules can be extended to perform template-based string matching operations.  The idea is to include a new type of conditions that checks whether a string matches a given template.  Both full and partial matching can be employed. Templates are allowed to include slots to fill. These slots are conceptually similar to the quantified variables discussed in Sections \ref{sec:quantification} and \ref{sec:applicationquantif}. A successful match will thus generate values for the filled slots, which will be included as part of the groundings for the rule. 

Figure \ref{fig:stringmanip} illustrate how such rules are applied in practice.  $\{OBJ\}$ denotes a slot that is to be filled through matching the template with the value specified in $u_u$. 
\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/stringmanip.pdf}
\caption{Example of rule using string matching operations.}
\label{fig:stringmanip}
\end{figure}

\section{Relation to previous work}
\label{sec:relatedwork}

The idea of using structural knowledge in probabilistic models has been explored in many directions, both in the fields of decision-theoretic planning and reinforcement learning \citep{Hauskrecht98,Pineau2004,KerstingR04,lang10jair,Otterlo2012} and in statistical relational learning \citep{Jaeger01,Richardson:2006,getoor:srlbook07}.  The introduced structure may be hierarchical, relational, or both. As in our approach, most of these frameworks rely on the use of expressive representations serving as templates for the generation of classical probabilistic models.  The surveys of \cite{Otterlo2006,Otterlo2012} provide a complete overview of relational and first-order logical approaches for reinforcement learning in Markov decision processes, covering both model-free and model-based methods.  While the formalisation presented in this thesis and the aforementioned approaches share many insights, they also reveal several interesting differences: 

\begin{itemize}

\item Probabilistic rules are primarily tailored for dialogue management tasks and seek to capture dialogue domains by striking a balance between propositional and first-order logic. The formalism deliberately eschews the complexity of full-scale first-order probabilistic inference to ensure that the domains models can be applied under real-time constraints. This design choice sets it apart from other frameworks such as Markov Logic Networks which can express arbitrary first-order formulae but are often tedious to instantiate due to the size and complexity of the resulting models.\footnote{See however \cite{Kennington:2012} for an approach that attempts to apply Markov Logic Networks to natural language understanding tasks.} 

\item Probabilistic rules are also designed to operate under partially observable settings, as state uncertainty is a pervasive and unavoidable aspect of verbal interactions.  By contrast, most previous work on relational probabilistic models are limited to fully observable environments, with the exception of some limited theoretical studies by \cite{Wang:2010,SannerK10}. 

\item Finally, the presented framework posits that the \textit{if ... then ... else} structures of probabilistic rules are best encoded by the system designers based on their expert knowledge of the domain, while the rule parameters can be estimated empirically. We therefore exclude the problem of structure learning from the scope of this thesis, as opposed to several approaches in which the domain rules and constraints are extracted via machine learning techniques \citep{PasulaZK07,Kok:2009}.

\end{itemize}

Probabilistic rules also bear similarities with planning description languages such as the Planning Domain Description Language \citep[PDDL, see ][]{mcdermott1998} and its probabilistic extension, the Probabilistic Problem Description Language \citep[PPDDL, see ][]{younes2004ppddl1}.  These languages are structured through action schemas that specify how (parametrised) actions can yield particular effects under various conditions. As in probabilistic rules, these languages try to carefully balance between the language expressivity and the complexity of the planning algorithm, based on a subset of first-order logic. A relational extension of PDDL, named RDDL, has also been introduced in recent planning competitions \citep{Sanner:RDDL}. The learning techniques presented by \cite{PasulaZK07} to estimate transition functions based on noisy indeterministic deictic rules is directly related to our approach, as is the recent work \cite{lang10jair} on probabilistic noisy planning rules.   Both frameworks define conditions associated with probabilistic distributions over effects. Their approaches are however restricted to fully observable settings. 

In the dialogue management literature, most structural approaches rely on a clear-cut task decomposition into goals and sub-goals \citep{Allen:2000:AGD:973935.973937,Steedman-Petrick:07,Bohus:2009:RDM:1518321.1518367}, where the completion of each goal is assumed to be fully observable, discarding any remaining uncertainty.  Our own work on multi-policy dialogue management in \cite{multipolicy-sigdial2011} relaxes the assumption of perfect knowledge of task completion, handling multiple policies as a problem of probabilistic inference over activation variables.  Probabilistic rules can be considered an extension of this early work, where the structural knowledge is not confined to task decomposition but is extended to generic rules over state variables.  

The formalism presented in this chapter is strongly inspired by information-state approaches to dialogue management \citep{Larsson:2000,Bos2003}, which are also based on a shared state representation that is updated according to a rich repository of rules.  \cite{Ginzburg2012} also models conversational phenomena by way of update operations that are encoded with rules mapping conditions to effects. However, contrary to the framework presented here, the rules specified in these approaches are generally deterministic and do not include learnable parameters. The action selection mechanism is also conceptualised slightly differently, as information-state frameworks rely on rules that directly select the most appropriate action given the current state. Probabilistic rules adopt by contrast a decision-theoretic approach that divides action selection in two stages: rules first provide utility distributions for the system action, and the system then searches for the action that yields the maximum expected utility. 

The literature on dialogue policy optimisation with reinforcement learning also contains several approaches dedicated to dimensionality reduction for large state-action spaces, such as function approximation \citep{Henderson:2008}, hierarchical reinforcement learning \citep{Cuayahuitl:2010} and summary POMDPs \citep{Young:2010}.  Many of these techniques have already been discussed in Section \ref{sec:application-dm} and will therefore not be repeated here. Most current approaches in dialogue policy optimisation focus on large but weakly structured state spaces (generally encoded as large lists of features), which are suited for slot-filling dialogue applications but are difficult to transfer to more open-ended or relational domains.  The idea of state space partitioning, implemented here via high-level conditions, has also been explored in recent papers \citep[see e.g. ][]{Williams2010}. \cite{Crook:2010} explored the introduction of complex user goal states including disjunction and negation operators. \cite{Heriberto2011} describes a policy optimisation approach based on logic-based representations of the state-action space for relational MDPs. The main difference with our approach lies in his reduction of the belief state to fully observable variables whereas we retain the partial observability associated with each variable.  The work of \cite{Mehta:2010,Raux2011} demonstrated how tree-structured Bayesian networks called probabilistic ontology trees can improve belief tracking performance.  The tree structure is derived in their work from a hierarchical concept structure .  Finally, \cite{neill2011} describe a procedure for dialogue strategy selection based on probabilistic logic.

\section{Conclusion}

This chapter presented the formalism of probabilistic rules, which forms the core of the modelling approach developed in this thesis. We started by arguing that dialogue models are often highly structured, and that this structure can be leveraged by (1) introducing latent variables, (2) partitioning value assignments for the parent variables, and (3) making use of quantification. We then explained how these structural insights can be transferred into a new framework -- probabilistic rules -- that combines concepts borrowed from both first-order logic and probability theory in order to get ``the best of both worlds'', i.e. a representation formalism that is both richly expressive and capable of capturing uncertain knowledge.  These rules are practically defined as \textit{if...then...else} control structures that associate high-level conditions on input variables to probabilistic effects on output variables.  Multiple extensions of the formalism have been developed to e.g. encode utility distributions, enclose universal quantifiers, and efficiently manipulate data structures such as lists and templates. 

At runtime, these rules are instantiated in the Bayesian network representing the current dialogue state. The instantiation procedure creates a latent node for each rule, which is conditionally dependent on the input variables of the rule.  For probability rules, this node is a chance node that expresses a probability distribution over the possible effects of the rule. Utility rules are similarly instantiated with a utility node expressing the utility distribution for specific decision variables.  Universally quantified variables can be included in the conditions and effects of the rules, allowing particular aspects of the rule to be underspecified. The rules are grouped into models that are attached to the dialogue state and are triggered upon relevant state updates. 

Probability and utility rules effectively function as high-level templates for the definition of a dynamic decision network. The expressive power of these rules allows them to efficiently encode complex relations between variables, and thereby reduce the number of parameters to estimate.  We have however not yet detailed how this parameter estimation is practically performed. The next two chapters provide answers to this important question. 
