\chapter{Probabilistic rules}
\label{chap:rules}

The previous chapter described how dialogue could be modelled in a probabilistic manner, and demonstrated how graphical models can help reduce the complexity of probability and utility models by exploiting independence properties between variables. But despite these attractive properties, graphical models still suffer from severe scalability problems when faced with dialogue domains designed for rich conversational contexts.  Conditional dependences between variables can lead to a rapid increase in the number of parameters associated with the model. Alas, only limited amounts of training data are available for most dialogue domains.  Estimating the model parameters in such setting is therefore a particularly challenging task. 

To address this issue, we introduce in this chapter a new approach to probabilistic dialogue modeling, based on the notion of \textit{probabilistic rules}.  Probabilistic rules are structured mappings between conditions and effects, and function as \textit{high-level templates} for the construction of a dynamic decision network.  The key advantage of this structured modelling approach is the drastic reduction of the number of parameters compared to traditional representations.  We also argue that these expressive representations are particularly well suited to encode the probability and utility models used in dialogue management, where substantial amounts of expert knowledge can be leveraged to structure the relationships between variables. 

The chapter is divided in six sections. Section \ref{sec:rmotivation} exposes in general terms how structural assumptions can be used to refine the representation of probabilistic models. 
Section \ref{sec:formalisation} defines the formalism of probabilistic rules and its main theoretical properties.  We also illustrate with a few examples how the formalism can be applied.  These definitions are then connected in Section \ref{sec:ruleinstantiation} to the graphical models described in the previous chapter, by showing how probabilistic rules are practically instantiated in the Bayesian network representing the dialogue state. Section \ref{sec:processing-workflow} then explains how this instantiation procedure is incorporated in the processing workflow for updating the dialogue state and selecting the system actions. Finally, Section \ref{sec:amodelling} addresses some advanced modelling issues and Section \ref{sec:relatedwork} relates the approach to previous work.


\section{Structural leverage}
\label{sec:rmotivation}

The starting point of our approach is the observation that the probability and utility models used in dialogue management often exhibit a fair amount of \textit{internal structure}.  
We have already discussed in the previous chapter one simple instance of this internal structure: factored representations based on conditional independences. However, the internal structure of dialogue domains does not limit itself to these basic independence assumptions, and much can be gained by exploiting additional structural properties. 

%If two sets of random variables $\mathbf{X}$ and $\mathbf{Y}$ are conditionally independent given $\mathbf{Z}$, we can rewrite the probability distribution $P(\mathbf{X}, \mathbf{Y} \, | \, \mathbf{Z})$ as $P(\mathbf{X} \, | \, \mathbf{Z}) (\mathbf{Y} \, | \, \mathbf{Z}) $.  


\subsubsection*{Latent variables}
 
The number of parameters required to estimate the distributions of a graphical model can often be reduced by introducing \textit{latent variables} (i.e. unobserved or hidden variables) that act as intermediaries between the source and target variables. Indeed, many application domains are often best explained by the combination of a small number of distinct factors or influences, each encoded by a separate random variable and associated with a subset of input and output variables. This layer of latent variables is usually never observed directly but contribute to structuring the model.\footnote{The use of layered computational models is one of the most active research topic in many areas of artificial intelligence and machine learning, and form in particular the foundations of deep learning approaches \citep{Bengio:2009}.} In the particular case of medical diagnosis, the relations between predisposing factors and observed symptoms are for instance best described by postulating an intermediary layer of variables -- possible diseases -- that mediate between the predisposing factors and the observed symptoms.  Figure \ref{fig:latentvariables} illustrates how latent variables can be exploited to provide an additional layer of abstraction within a graphical model.

 \begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/latentvariables.pdf}
\caption{Comparison between a model that directly maps variables $\mathbf{Y}$ to $\mathbf{X}$ (left side) and one relying on latent variables $\mathbf{L}$ to serve as intermediaries (right side).}
\label{fig:latentvariables}
\end{figure}

Dialogue models can often benefit from introduction of such latent variables. The transition function can in particular be modelled in terms of a limited number of latent variables, each responsible for capturing specific aspects of the interaction dynamics. 

\subsubsection*{Partitioning}

A random variable $X$ with parent variables $Y_1,...Y_m$ must specify a separate probability distribution for every possible assignment of values for the parent variables. In other words, the number of parameters required to specify the distribution $P(X \, | \, Y_1,..., Y_m)$ is exponential in the number of parents $m$. Fortunately, the values of these parents variables can be grouped into \textit{partitions} yielding similar outcomes for $X$. One can therefore directly define the conditional probability distributions on these groups rather than on the full enumeration of combined values for the parent variables. Partitioning is an important abstraction mechanism to reduce the complexity of the model distributions and improve their ability to generalise to unseen examples. 

%Utility distributions can also partition the values of their dependent variables in a similar way.  

Figure \ref{fig:partitioning} provides an example of such partitioning for the conditional probability distribution $P(\mathit{Fire} \, | \mathit{Weather}, \mathit{Rain})$.  The space of possible values for the parent variables is defined in this example as $Val(\mathit{Weather}) \times Val(\mathit{Rain})$ and contains 6 possible elements.  We can observe that this space can be split in two partitions: $\mathit{Rain}\!=\mathit{true} \lor \mathit{Weather}\!\neq\mathit{hot}$ and $\mathit{Rain}\!=\mathit{false} \land \mathit{Weather}\!=\mathit{hot}$. This partitioning allows a significant reduction of the number of parameters required for the conditional probability distribution.  It is important to note that grouping value assignments into partitions corresponds to a modelling choice and can degrade the model accuracy if the partitions do not reflect actual similarities in the predicted outcomes.

%As illustration, consider a minimalistic dialogue in a robot learning scenario where the robot can ask the user yes/no questions pertaining to the colour of one specific object (e.g. \utt{Is the object red?}). In this simple scenario, the state is represented with two variables: the user dialogue act $a_u = \{\mathit{yes,no}\}$ and a variable representing the object colour, $\mathit{colour} = \{\mathit{blue,green, ...}\}$ with $n$ possible colours.  The system actions take the form $a_m = \{ \mathit{VerifyColour(c)}: c \in \mathit{colour}\}$, where $VerifyColour(c)$ corresponds to asking the user whether the object is of colour $c$.  Since the object colour remains constant, the transition function for this domain is defined as $P(a_u'|a_m, \mathit{colour})$. The number of parameters required to specify the transition function is $n^2$ (since $a_m$ and $\mathit{colour}$ have $n$ possible values each). 

%However, one can reasonably assume in this example that the particular colour mentioned in the question is irrelevant to predict the next user dialogue act $P(a_u'|a_m,\mathit{colour})$ as long as it matches (or fails to match) the actual colour of the object.  Based on this assumption, one can divide the space $Val(a_m) \times Val(\mathit{colour})$ into two distinct partitions: 
%\begin{enumerate}
%\item One partition in which the verification question corresponds to the actual colour of the object: $\exists c: colour\!=\!c \land a_m\!=\!\mathit{VerifyColour(c)}$.
%\item One partition in which there is a mismatch between the colour mentioned in the question and the actual colour: $\exists c: colour\!=\!c \land a_m\!\neq\!\mathit{VerifyColour(c)}$.
%\end{enumerate}

%By abstracting over the specific colour mentioned in the question, partitioning allows us to drastically reduce the number of parameters required for the conditional probability distribution $P(a_u'|a_m,\mathit{colour})$ from a total of $n^2$ to only $2$. Figure \ref{fig:partitioning} illustrates this reduction. 


 \begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/partitioning.pdf}
\caption{Partitioning for the conditional probability distribution $P(\mathit{Fire} \, | \mathit{Weather}, \mathit{Rain})$.}
\label{fig:partitioning}
\end{figure}

Partitions must be both exhaustive (each combination of values for the parent variables must belong to one partition) and mutually exclusive (a combination of values can only belong to one partition).  As we can observe from the example, partitions can often be concisely expressed via logical conditions on the variable values.  A given assignment of values will then be grouped in a partition if it satisfies the condition associated with it.

\subsubsection*{Quantification}

Many dialogue domains are composed of objects or entities related to one another. These domains are often difficult to directly encode by a fixed set of random variables, as the number of entities and relations may vary over time.  Examples of such relational structure include: 
\begin{itemize}
\item Collections of physical objects in a visual scene, each described by specific features (colour, shape) and relations with other objects (e.g. spatial relations),
\item Indoor environments topologically structured in rooms and spaces in which to navigate. 
\item Stacks of tasks to complete by the agent, each task being possibly connected to other tasks via precedence or inclusion relationships.
\end{itemize}

First-order logic provides an excellent basis for representing and manipulating such relational structures, as it offers a rich language for (1) referring to objects connected with one another through functions and relations and (2) describing their properties in a concise way through the use of universal and existential quantifiers.\footnote{We shall not cover in this thesis the mathematical foundations of first-order logic, but the interested reader is invited to refer to e.g. \cite{gamut1991logic} for a formal overview of the logical concepts mentioned throughout this thesis.}

Graphical models can represent such relational domains by instantiating one random variable for every possible grounding of the functions and predicates defined for the domain.\footnote{Such operation is equivalent to \textit{propositionalisation} in the terminology of first-order logic.}  A domain with two objects $o_1$ and $o_2$ and a relation $\mathit{leftOf}(x,y)$ will for instance generate the four groundings $\mathit{leftOf}(o_1,o_2)$, $\mathit{leftOf}(o_2,o_1)$ $\mathit{leftOf}(o_1,o_1)$ and $\mathit{leftOf}(o_2,o_2)$. The definition of probability and utility distributions that can handle the relational semantics of such representation is however not straightforward, as each grounded random variable must be associated with a separate distribution. In particular, the enforcement of generic properties and constraints such as $\forall x, \neg \mathit{leftOf}(x,x)$ and $\forall x, y, z, \mathit{leftOf}(x,y) \land \mathit{leftOf}(y,z) \Rightarrow \mathit{leftOf}(x,z)$ is difficult, since classical probabilistic models offer unfortunately no direct support for quantifiers.  Their expressive power is indeed intrinsically limited to propositional logic. 
 
The unification of first-order logic and probability theory has spanned a new research area called \textit{statistical relational learning} \citep{getoor:srlbook07}. A common trait of most approaches to statistical relational learning is the definition of a logic-based description language which is employed as a template to generate classical probabilistic models given a set of constants. The introduction of quantifiers provide an abstraction mechanism to reduce the complexity of a given probabilistic models by describing constraints or relations that hold for all possible groundings of a given formula and can therefore apply to large sets of random variables. 

\section{Formalisation}
\label{sec:formalisation}

We now outline a generic description framework for expressing the various types of internal structure we have just detailed.  This description framework revolves around the notion of \textit{probabilistic rules}.  The framework was originally presented in \cite{rulebasedmodels-sigdial2012,lison-semdial2012}.  

The key idea is to represent distributions with the help of \textit{if ... then ... else} control structures, based on the following skeleton:
\begin{equation*}
\begin{aligned}
& \textbf{if} \ \ (\text{condition 1 holds}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \text{Distribution 1 over possible effects} \\
& \textbf{else if} \ \ (\text{condition 2 holds}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \text{Distribution 2 over possible effects} \\
& ... \\
& \textbf{else} \\
& \;\;\;\;\; \text{Distribution } n \text{ over possible effects} \\ 
\end{aligned}
\label{eq:probrule}
\end{equation*}

Each \textit{if ... then} branch in the rule specifies both a condition on particular state variables, and an associated distribution over possible effects.   The \textit{if ... then ... else} structure is read in sequential order, as in programming languages, until a satisfied condition is found, in which case the probabilistic effects assigned with it are enacted.

We first present how rules can express conditional probability distributions in terms of structured (and parametrised) mappings between input and output variables.  We then show how to  generalise the formalism to utility distributions and extend it with quantification mechanisms.

\subsection{Basic definition}

Probabilistic rules take the form of \textit{if ... then ... else} control structures and map a list of conditions on input variables to probabilistic effects on output variables. More formally, a rule is expressed as an ordered list $\langle br_1, ... br_n\rangle$, where each branch $br_i$ is a pair $(c_i, P(E_i))$, $c_i$ is a condition and $P(E_i)$ an associated distribution over possible effects.  The distribution $P(E_i)$ is a categorical distribution with possible effects $Val(E_i) = \{e_i^1,... e_i^{m_i}\}$, where $m_i$ is the number of alternative effects in $P(E_i)$.  Each effect $e_i^j \in Val(E_i)$ has a particular probability denoted $p_i^j$.  The probabilities $p_i^{1...m_i}$ must satisfy the usual probability axioms $p_i^j \geq 0$ and $\sum_{j = 1}^m p_i^j = 1 \ \forall i,j$. 

Given these elements, a basic probabilistic rule reads as such:
\begin{equation}
\begin{aligned}
& \textbf{if} \ \ (c_{1}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(E_1\!=\!e_1^1) = p_1^1 \\
 ... \\
P(E_1\!=\!e_1^{m_1}) = p_1^{m_1} 
\end{cases} \\[3mm]
& \textbf{else if} \ \ (c_{2}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(E_2\!=\!e_2^1) = p_2^1, \\
 ... \\
P(E_2\!=\!e_2^{m_2}) = p_2^{m_2}
\end{cases} \\ 
& ...  \\
& \textbf{else} \\
& \;\;\;\;\; \begin{cases}
P(E_n\!=\!e_n^1) = p_n^1, \\
... \\
P(E_n\!=\!e_n^{m_n}) = p_n^{m_n}
\end{cases}
\end{aligned}
\label{eq:probrule}
\end{equation}

 In the rest of this thesis, we will often write $P(e_i^j)$ as a notational convenience for $P(E_i = e_i^j)$. 

%The three following sections describe how the conditions $c_i$, effects $e_i^j$ and associated effect probabilities $p_i^j$ are respectively defined. 

\subsubsection*{Conditions}

The conditions $c_i$ are expressed as logical formulae grounded in a subset of the state variables. This subset of state variables are the \textit{input variables} of the rule. Conditions can be arbitrarily complex logical formulae connected by conjunction, disjunction and negation, and can also include free variables. The examples of partition  $(\mathit{Rain}\!=\mathit{true} \lor \mathit{Weather}\!\neq\mathit{hot})$ and $(\mathit{Rain}\!=\mathit{false} \land \mathit{Weather}\!=\mathit{hot})$ in Figure \ref{fig:partitioning} are instances of valid conditions on the two input variables $Rain$ and $\mathit{Weather}$. 

Given that a rule is defined through a \textit{if ... then ... else} control structure, the partitioning is guaranteed by construction to be exhaustive and mutually exclusive (only one branch will be followed).  When provided with an assignment of values on the input variables, the conditions are tested in sequential order until one is satisfied. When no terminating \textbf{else} statement is explicitly put at the end of a rule, the framework inserts a final empty (i.e. always satisfied) condition associated with a void effect to ensure that the partitioning is exhaustive. 

The conditions on the input variables effectively provides a compact partitioning of the state space to mitigate the dimensionality curse.  Without this partitioning in alternative conditions, a rule ranging over $v$ input variables each containing $w$ possible values would need to enumerate $w^v$ possible assignments.  Partitioning this space with conditions reduces this number to $n$ mutually exclusive partitions, where $n$ is usually small. 


\subsubsection*{Effects}

Associated to each condition $c_i$ stands a list of alternative, mutually exclusive effects $e_i^{1...m_i}$. Each effect $e_i^j$ defines a specific assignment of values for another subset of the state variables.  An effect is defined as a conjunction of (variable,value) pairs $X_1\!=\!x_1 \land X_2\!=\!x_2 \land ... X_k\!=\!x_k$ where $X_1,... X_k$ denote particular random variables in the dialogue state and $x_1,...x_k$ corresponding values for these variables.   The variables $X_1,...,X_k$ are the \textit{output variables} of the rules. In the partitioning example from Figure \ref{fig:partitioning}, the output variable is unique and corresponds to $Fire$. We shall however encounter some examples of rules with more than one output variable. It is also worth nothing that the sets of input and output variables can overlap, in which case the output variables represent an updated version of the existing variables. 

Effects can be void -- that is, represent an empty assignment.  In such case, the effect does not lead to any change in the model distribution for the state variables. 

\subsubsection*{Probabilities}

Each effect $e_i^j$ is assigned with a probability $p_i^j = P(E_i = e_i^j)$.  These probabilities can be either 
fixed by hand or correspond to parameters to estimate from data. In the latter case, we adopt a Bayesian learning approach (cf. Section \ref{sec:learning}) and assume the probabilities $e_i^{1...m_i}$ to be drawn from the conjugate prior of categorical distributions: Dirichlet distributions. Chapters \ref{chap:wozlearning} and \ref{chap:rllearning} detail how Dirichlet distributions can be exploited to estimate probability parameters. 

It often happens that only the distribution $P(E_i)$ only includes one single effect with probability 1.  In such case, the rule corresponds to a deterministic distribution. 

\subsubsection*{Example}

Rule $r_1$ illustrates a simple example of probabilistic rule:
\begin{align*}
r_1: \ \ \ \ \ & \textbf{if} \ (\mathit{Rain}\!=\!\mathit{false} \land \mathit{Weather}\!=\!\mathit{hot}) \ \textbf{then} \\
& \;\;\;\;\;  \begin{cases}
 P(\mathit{Fire}\!=\!\mathit{true}) = 0.03 \\ 
P(\mathit{Fire}\!=\!\mathit{false}) = 0.97
\end{cases} \\ 
& \textbf{else} \\
& \;\;\;\;\; \begin{cases}
P(\mathit{Fire}\!=\!\mathit{true}) = 0.01 \\
P(\mathit{Fire}\!=\!\mathit{false}) = 0.99
\end{cases} 
\end{align*}

Rule $r_1$ has two input variables: $\mathit{Rain}$ and $\mathit{Weather}$ as well as one output variable $\mathit{Fire}$. The rule specifies that the probability of a fire is 0.03 in case of no rain and a hot weather and 0.01 in all other cases.  The reliance on high-level conditions enables the conditional probability distribution to be specified with only four probabilities in comparison to 12 for the original conditional probability distribution shown in Figure \ref{fig:partitioning}. 

%Here is a first example of probabilistic rule pertaining to the user action model: 
%\begin{align*}
%\textbf{Rule 1}: \ \ & \textbf{if} \ (\exists X: a_m=\mathit{Confirm(X)} \land i_u = \mathit{X})  \ \textbf{then} \\ 
%& \;\;\;\;\; \{[P(a_u' = \mathit{Confirm}) = 0.9]\} \\
%& \textbf{else if} \ (\exists X: a_m=\mathit{Confirm(X)} \land i_u \neq \mathit{X})  \ \textbf{then} \\ 
%& \;\;\;\;\; \{[P(a_u' = \mathit{Disconfirm}) = 0.95]\}
%\end{align*}
%The rule specifies that, if the system requests the user to confirm that his intention is $X$ and his actual intention is indeed $X$, the user is expected to utter a $\mathit{Confirm}$ action with probability 0.9.  Otherwise, the rule produces a void effect -- i.e. it leaves the distribution $P(a_u')$ unchanged. If the intention is different, the user will utter a $\mathit{Disconfirm}$ action with  probability 0.95.   

\subsection{Utility distributions}

The rule-based formalism we have outlined can also be used to express utility distributions with only minor notational changes. Utility rules essentially retain the same form as probabilistic rules, with one notable exception, namely that the probabilistic effects are replaced by utility distributions over particular assignments of decision variables. 

Formally, a utility rule is an ordered list $\langle br_1, ... br_n\rangle$, where each branch $br_i$ is a pair $(c_i, U(D_i))$ where $c_i$ is a condition and $U(D_i)$ an associated utility distribution over possible assignments of decision variables. The utility distribution $U(D_i)$ specifies a set of possible decisions  $Val(D_i) = \{d_i^1,... d_i^{m_i}\}$.  Each decision $d_i^j \in Val(D_i)$ has a particular utility value denoted $u_i^j$.  Utility rules can be expressed in the following manner:
\begin{equation}
\begin{aligned}
& \textbf{if} \ \ (c_{1}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
U(D_1\!=\!d_1^1) = u_1^1 \\
 ... \\
U(D_1\!=\!d_1^{m_1}) = u_1^{m_1} 
\end{cases} \\[3mm]
& \textbf{else if} \ \ (c_{2}) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
U(D_2\!=\!d_2^1) = u_2^1 \\
 ... \\
U(D_2\!=\!d_2^{m_2}) = u_2^{m_2} 
\end{cases} \\
& ...  \\
& \textbf{else} \\
& \;\;\;\;\; \begin{cases}
U(D_n\!=\!d_n^1) = u_n^1, \\
... \\
U(D_n\!=\!d_n^{m_n}) = u_n^{m_n}
\end{cases}
\end{aligned}
\end{equation}

A utility rule assigns utility values to particular system decisions depending on conditions on the state variables.  As for probability rules, the conditions $c_i$ are defined as arbitrary logical formulae on a subset of state variables.  The decisions $d_i^j$ are assignments $X_1\!=\!x_1 \land X_2\!=\!x_2 \land ... \land X_n\!=\!x_n$ where the variables $X_1,..X_n$ are decision variables and $x_1,...x_n$ possible values for these variables. The utility values $u_i^j$ are real numbers. 

Although most utility rules only include one single decision variable, the possibility to integrate multiple decision variables is helpful in domains where the system can execute multiple actions in parallel. Such situations can arise in human-robot interaction and multi-modal applications, as the system can communicate through both verbal and non-verbal channels and is often able to perform physical actions in addition to communicative acts. 
 
\subsubsection*{Example}

Rule $r_2$ provides a simple example of utility rule:

\begin{align*}
r_2: \ \ & \textbf{if} \ (\mathit{Fire}\!=\!\mathit{true}) \ \textbf{then} \\
& \;\;\;\;\;  \begin{cases}
U(\mathit{Tanker}\!=\!\mathit{drop\mbox{-}water}) = 5 \\
U(\mathit{Tanker}\!=\!\mathit{wait}) = -5
\end{cases} \\
& \textbf{else} \\
& \;\;\;\;\; \begin{cases}
U (\mathit{Tanker}\!=\!\mathit{drop\mbox{-}water}) = -1 \\
U(\mathit{Tanker}\!=\!\mathit{wait}) = 0
\end{cases}
\end{align*}

Rule $r_2$ stipulates the respective utilities of the two possible utility values for the decision variable $\mathit{Tanker}$ depending on the variable $\mathit{Fire}$. 

\subsection{Quantification}
\label{sec:quantification}

Quantification can serve as an important mechanism to abstract over particular relational aspects of the domain structure.  Logical variables can be included in the specification of both the conditions and effects of a given rule, and are universally quantified on top of the rule.\footnote{These variables are variables in the sense of first-order logic, and are not to be confused with the random variables of the probabilistic model.}  A rule containing the free variables $z_1,z_2,...z_k$ in its conditions and/or effects is therefore written as:
\begin{equation}
\begin{aligned}
\forall \mathbf{z} = z_1, z_2,...z_k: \\
& \textbf{if} \ \ (c_{1}(\mathbf{z})) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(E_1\!=\!e_1^1(\mathbf{z})) = p_1^1 \\
 ... \\
P(E_1\!=\!e_1^{m_1}(\mathbf{z})) = p_1^{m_1} 
\end{cases} \\[3mm]
& \textbf{else if} \ \ (c_{2}(\mathbf{z})) \ \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(E_2\!=\!e_2^1(\mathbf{z})) = p_2^1 \\
 ... \\
P(E_2\!=\!e_2^{m_2}(\mathbf{z})) = p_2^{m_2} 
\end{cases} \\ 
& ...  \\
& \textbf{else} \\
& \;\;\;\;\; \begin{cases}
P(E_n\!=\!e_n^1(\mathbf{z})) = p_n^1, \\
... \\
P(E_n\!=\!e_n^{m_n}(\mathbf{z})) = p_n^{m_n}
\end{cases}
\end{aligned}
\label{eq:rulewithquant}
\end{equation}

The formalisation allows specific elements $z_1,...z_k$ in the conditions and effects to be \textit{underspecified}.  The mapping between conditions and effects specified by the rule is therefore instantiated for every possible assignment of the underspecified variables. Thanks to this quantification mechanism, probabilistic rules can cover large portions of the state space in a highly compact manner, based on a reduced number of parameters. One of the key advantages of such representation is it allows for powerful forms of \textit{parameter sharing}, as the effect probabilities $p_i^j$ in the above rule are made independent of the various possible instantiations of the variables $z_1,...z_k$. Quantification can also be used for utility rules in the same manner. 

\subsubsection*{Example}

Rule $r_3$ provides a simple example of probability rule including a quantified variable:
\begin{align*}
r_3: \ \ & \forall z: \\ 
& \textbf{if} \ (\mathit{shape}(z) = \mathit{sphere})  \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(\mathit{graspable}(z)\!=\!\mathit{true}) = 0.9 \\
P(\mathit{graspable}(z)\!=\!\mathit{false}) = 0.1
\end{cases} \\ 
&\textbf{else if} \ (\mathit{shape}(z) = \mathit{cone})  \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
P(\mathit{graspable}(z)\!=\!\mathit{true}) = 0.2 \\
P(\mathit{graspable}(z)\!=\!\mathit{false}) = 0.8
\end{cases}
\intertext{Rule $r_3$ specifies how the graspability of a given object $z$ depends on its shape. Similarly, rule $r_4$ defines the utility of a grasping action depending on the task and object graspability:}
r_4: \ \ & \forall z: \\ 
& \textbf{if} \ (task= grasp(z) \land \mathit{graspable}(z) = \mathit{true})  \ \textbf{then} \\ 
& \;\;\;\;\; \begin{cases}
U(a_m\!=\!\mathit{grasp}(z)) = 2 \\
\end{cases} 
\end{align*}

The careful reader will notice that underspecified variables can occur both inside the name of a random variable, as in $\mathit{graspable}(z)$, and inside the values of random or decision variables, as in $grasp(z)$.  

%Assume you want to predict the next user dialogue act $a_u'$ after a system-initiated question such as ``What colour is the object'' depending on the colour of the object that is referred to. Rule $r_3$ is an example of rule that employs quantification on two variables denoted $o$ and $c$.\footnote{Note the absence of an explicit final \textit{else} branch, which is then by default associated with an empty effect.}
%\begin{align*}
%r_3: \ \ & \forall o, c: \\ 
%& \textbf{if} \ (a_m\!=\!\mathit{WhatIsColour}(o) \land \mathit{colour}(o)\!=\!\mathit{c})  \ \textbf{then} \\ 
%& \;\;\;\;\; \begin{cases} P(a_u'=\mathit{AssertProperty}(o,\mathit{c})) = 0.95 \end{cases}
%\end{align*}

%Rule $r$ assumes the presence in the dialogue state of a random variable $colour(o_i)$ for each object $o_i$ perceived by the system.  The rule specifies that the user is likely (with probability 0.95) to utter a dialogue act such as ``the object is X'' at the next turn, where X is the actual colour of the object. Otherwise no prediction is made. If the dialogue state contains e.g. one object $o_1$ with $P(\mathit{colour}(o_1)=blue)=0.8$ and the last system action was $WhatIsColour(o_1)$, the probability of the user uttering ``the object is blue'' is therefore $0.76$.  

\section{Rule instantiation}
\label{sec:ruleinstantiation}

The dialogue state is represented in this thesis as a Bayesian network over state variables, in line with other dialogue management approaches such as \cite{Thomson:2010:BUD:1772996.1773040,bui2010}. Rules are then applied at runtime upon this dialogue state.  Probabilistic rules are employed to create or update particular state variables, whereas utility rules define decision variables and their corresponding utility values. 

The instantiation is performed by creating a distinct node for every rule to apply. These rule nodes are essentially latent variables that serve as intermediaries between input and output variables.  Albeit the occurrence of these rules is never directly observed, they help structuring the relations between variables and enable the system designer to decompose complex probability and utility models into smaller parts.   % The instantiated rules are equivalent to a dynamic decision network (cf. previous chapter). 

We describe below the instantiation procedure for each type of rule. For the sake of clarity, we shall first limit our discussion to rules without quantifiers, and then show quantifiers can be accounted for in the instantiation process. 

\subsection{Probability rules}
\label{sec:probruleinstantiation}

Let $\mathcal{B}$ be the Bayesian network representing the current dialogue state, and $\mathcal{R}$ a set of rules to apply to this dialogue state.  For each rule $r \in \mathcal{R}$, a distinct chance node is created.  This chance node represents a random variable defined on the possible effects of the rule.  The node is conditionally dependent on the input variables of the rule (i.e. the set of all variables that are mentioned in the rule conditions), and is also connected via outgoing edges to its output variables (i.e. the set of all variables that are mentioned in the rule effects). 

Figure \ref{fig:instantiationprob} illustrates this construction process on a constructed example.  The two rules $r_5$ and $r_6$ are applied on the state variables $A$, $B$, $C$ and $D$.  The application of the two rules results in an update of the variable $A$ and the creation of a new variable $E$. The random variable represented by the node $r_5$ has three possible values, reflecting the effects described in the rule: $Val(r_5) = \{ [A\!=\!a_1], [A\!=\!a_2], \{\cdot\}\}$.  Note that $\{\cdot\}$ denotes the empty effect.  Similarly, the random variable $r_5$ has four alternative effects: $Val(r_6) = \{[A\!=\!a_2 \land E\!=\!e_2], [A\!=\!a_2 \land E\!=\!e_1], [E=e_2], \{\cdot\}\}$. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/ruleinstantiation.pdf}
\caption{Example of instantiation for the two probability rules $r_5$ and $r_6$. The remaining probability mass in the rule specifications is by default assigned to the empty effect. }
\label{fig:instantiationprob}
\end{figure}

The output variables are by convention denoted with a prime to distinguish them from input variables.  We shall also adopt the following terminology to name the probability distributions created through the instantiation procedure: 
\begin{itemize}
\item The conditional probability distribution associated with rule nodes such as $r_5$ and $r_6$ given their inputs is a \textit{rule distribution}.
\item The conditional probability distribution associated with output variables such $A'$ and $E'$ given the rule nodes that determines it is a \textit{output distribution}.
\end{itemize}

\subsubsection*{Rule distributions}

The rule distributions are a direct reflection of the rule semantics.  Formally, the conditional probability distribution of a rule $r$ given its input variables $I_1,...I_n$ is defined as: 
\begin{align}
& P(r\!=\!e \, | \, I_1\!=\!i_1,... I_n\!=\!i_n) = P(E_i = e) \label{eq:ruledistrib}
 \\ 
& \; \; \; \; \; \; \; \; \text{ where } i = \min_i (c_i \text{ is satisfied with } I_1\!=\!i_1 \land ... I_n\!=\!i_n) \nonumber 
\end{align}
In other words, the rule conditions are checked in sequential order until one condition $c_i$ is satisfied for the given input.  The distribution for the rule is then simply determined as the effect distribution $P(E_i\!=\!e)$ associated with the satisfied condition $c_i$. 

As an example, the rule distribution $P(r_5 \, | \, B\!=\!b_1, C\!=\!c_1)$ for the node $r_5$ in Figure \ref{fig:instantiationprob} is defined as:
\begin{itemize}
\item $P(r_5 = \{A\!=\!a_1\} \, | \, B\!=\!b_1, C\!=\!c_1) = 0.6$
\item  $P(r_5 = \{\cdot\} \, | \, B\!=\!b_1, C\!=\!c_1) = 0.4$
\item $P(r_5 = \{A\!=\!a_2\} \, | \, B\!=\!b_1, C\!=\!c_1) = 0$
\end{itemize}
Similarly, the distribution $P(r_6 \, | \, C\!=\!c_1, D\!=\!d_1)$ is a deterministic distribution with the empty effect $\{\cdot\}$ assigned to a probability 1. 

\subsubsection*{Output distributions} 

An output variable $X'$ is conditionally dependent on all the rule nodes that refer to it in their effects.  In addition, output variables that result in the update of existing variables (such as $A'$ in the example of Figure \ref{fig:instantiationprob}) also include a conditional dependence on these existing variables. The output distribution is a reflection of the combination of effects specified in the parent rules. The conditional probability distribution $P(X'|r_1\!=\!e_1,...r_n\!=\!e_n)$ for an output variable $X'$ with $n$ incoming rule nodes is defined in the following manner:
\begin{align}
&P(X'\!=\!x' \, | \, r_1\!=\!e_1,... r_n\!=\!e_n) = \begin{dcases}
\frac{\sum_{v \in {\mathbf{e}(X)}} \mathbf{1}(x' = v)} { |\mathbf{e}(X)| } & \text{if } \mathbf{e}(X)\!\neq\!\emptyset \\
\mathbf{1}(x' = \mathit{None}) & \text{otherwise}
\end{dcases}
\label{eq:outputdist1}
\end{align}
where the following notation is used: \begin{itemize}
\item $\mathbf{e}$ is the conjunction of all effects, i.e. $\mathbf{e} = e_1 \land ... e_n$.  Note that this conjunction can include more than one assigned value for a particular variable.
\item $\mathbf{e}(X)$ denotes the (possibly empty) set of values specified for the variable $X$ in $\mathbf{e}$. 
\item $\mathbf{1}(p)$ is the indicator function for the Boolean $p$, with $\mathbf{1}(p)=1$ if $p$ is true and $0$ otherwise.
\end{itemize}

Equation \eqref{eq:outputdist1} stipulates that the distribution for $X'$ will follow the values assigned in the effect(s) provided that at least one effect specifies a non-empty value for it. If the effects include conflicting assignments, the distribution is spread uniformly over the alternative values. If all effects $e_1,...e_n$ are empty assignments, the value for $X'$ is set to a default $None$ value.

If the variable $X'$ is an update of an existing variable $X$, the procedure remains essentially the same as for \eqref{eq:outputdist1}, except in the case where all the effects specify empty assignments for the variable. In such case, the distribution for $X'$ will fall back to the value defined for the existing variable $X$ instead of being assigned a $\mathit{None}$ value:
\begin{align}
&P(X'\!=\!x' \, | \, r_1\!=\!e_1,... r_n\!=\!e_n, X\!=\!x) = \begin{dcases} 
\frac{\sum_{v \in {\mathbf{e}(X)}} \mathbf{1}(x' = v)} { |\mathbf{e}(X)| }  & \text{if } \mathbf{e}(X)\!\neq\!\emptyset \\
\mathbf{1}(x' = x) & \text{otherwise}
\end{dcases}\label{eq:outputdist2}
\end{align}

As an example, the output distribution $P(A' \, | \, r_5\!=\!\{\cdot\},r_6\!=\!\{A\!=\!a_2 \land E\!=\!e_2\}, A\!=\!a_3)$ for the variable $A'$ in Figure \ref{fig:instantiationprob} results in a deterministic distribution with a unique value $a_2$ with probability 1.  If the two rules generate conflicting assignments, the probability mass is divided equally over the alternative values. The distribution $P(A' \, | \, r_5\!=\!\{A\!=\!a_1\},r_6\!=\!\{A\!=\!a_2 \land E\!=\!e_2\}, A\!=\!a_3)$ is thus a uniform distribution with two values: $a_1$ and $a_2$, each with probability 0.5. Finally, if all effects are empty, the output distribution is a simple copy of the distribution for the existing variable: $P(A' \, | \, r_5\!=\!\{\cdot\},r_6\!=\!\{\cdot\}, A\!=\!a_3)$ has a unique value $a_3$ with probability 1. 

Output distributions are directly derived from the effects in the rule nodes and are thus entirely parameter-free.  The ordering of the parent rules in the conditional probability distribution is arbitrary. The resulting distribution bears resemblance to the probabilistic Independence of Causal Influence (pICI) described by \cite{diez06}. 


\subsubsection*{Instantiation algorithm} 
\label{sec:utilruleinstantiation}

The procedure for instantiating a rule in a given dialogue state is detailed in Algorithm \ref{algo:instantiateProbRule}. 

\begin{algorithm}[h!]
\caption{: \textsc{InstantiateProbRule} ($\mathcal{B}, r$)}
\begin{algorithmic}[1] \vspace{1mm}
\REQUIRE $\mathcal{B}$: Bayesian network for the current state
\REQUIRE $r$: Probability rule to instantiate in network  \vspace{1mm}
\STATE $\mathcal{I}_{r} \leftarrow$ input variables for rule $r$
\STATE Create chance node $R$ with the rule distribution in Eq. \eqref{eq:ruledistrib}
\STATE Add node $R$ and dependency edges $\mathcal{I}_{r} \rightarrow R$ to $\mathcal{B}$ 
\IF {$Val(R) = \{\cdot\}$}
\STATE Prune $R$ from $\mathcal{B}$
\ELSE
\STATE $\mathcal{O}_{r} \leftarrow$ output variables mentioned in the effects of $R$
\FORALL {output variable $O \in \mathcal{O}_r$}
\IF {$O'$ not already in $\mathcal{B}$}
\STATE Create chance node $O'$ with the output distribution in Eq. \eqref{eq:outputdist1}-\eqref{eq:outputdist2}
\STATE Add node $O'$ and (in case $O$ exists) dependency edge $O \rightarrow O'$ to $\mathcal{B}$
\ENDIF
\STATE Add dependency edge $R \rightarrow O'$ to $\mathcal{B}$ 
\ENDFOR
\ENDIF
\end{algorithmic}
\label{algo:instantiateProbRule}
\end{algorithm}

The first steps of the instantiation process are to extract in the Bayesian network the input variables of the rule (line 1), create a node corresponding to the rule (line 2) and include its conditional dependences in the network (line 3).  The algorithm then checks whether at least one effect in $R$ is non-empty given its conditional dependences (lines 4).  If all effects are empty, the rule node is irrelevant and can be directly pruned (line 5). Otherwise, the output variables are extracted (line 7), and output nodes that do not already exist in the network are created (line 10-11). The final step is to establish dependency edges between the rule node and these output variables (line 13).


\subsection{Utility rules}

Utility rules are instantiated in the Bayesian network following a similar procedure, with two notable differences compared to probability rules: \begin{itemize}
\item As utility rules define utility distributions, their instantiation correspond to utility nodes instead of chance nodes.
\item Instead of output variables, the rules are associated with decision variables.  The association direction is inverted, as the decision node must be input to the utility node.
\end{itemize} 

The result of the instantiation process is a decision network that incorporates chance nodes (corresponding to the state variables), utility nodes (corresponding to the utility rules) and associated decision nodes. Figure \ref{fig:instantitionutil} illustrates the instantiation of two utility rules $r_7$ and $r_8$. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/ruleinstantiation2.pdf}
\caption{Example of instantiation for the two utility rules $r_7$ and $r_8$.}
\label{fig:instantitionutil}
\end{figure}


The utility distribution associated with each rule is a direct translation of the  \textit{if ... then ... else} rule.  Formally, the utility distribution generated by a rule $r$ with input variables $I_1,...I_n$ and decision variables $A_1,...A_m$ is defined as:
\begin{align}
& U_r(I_1\!=\!i_1,... I_n\!=\!i_n, A_1\!=\!a_1,... A_m\!=\!a_m) = \nonumber \\ 
& \; \; \; \; \; \; \; \; U(D_i = \{A_1\!=\!a_1 \land... A_m\!=\!a_m\}) \label{eq:utildistrib}\\
&  \; \; \; \; \; \; \; \; \text{ where } i = \min_i (c_i \text{ is satisfied with } I_1\!=\!i_1 \land ... I_n\!=\!i_n) \nonumber
\end{align}

If no utility is explicitly specified for $\{A_1\!=\!a_1 \land... A_m\!=\!a_m\}$, the default value is zero. 

As is conventionally assumed in decision models, the total utility for a given assignment of decision variables is defined as the \textit{sum} of all utilities.  The total utility for the actions $D'\!=\!d_1 \ \land \ E'\!=\!e_1$ in the case where $A\!=\!a_1$, $B=\!=\!b_1$ and $C\!=\!c_1$ is for instance equal to $2 - 4 = -2$. 


\subsubsection*{Instantiation algorithm} 

The procedure for instantiating a utility rule is similar in most respects to the one already outlined for probability rules. Algorithm \ref{algo:instantiateUtilRule} details the procedure, starting from the extraction of the input variables, the creation of the rule node, and the inclusion of conditional dependences (line 1-3). The algorithm then checks if the utility distribution stipulates a non-zero utility for at least one action (line 4).  If the answer is negative, the node is essentially irrelevant and can be pruned (line 5).  The decision variables associated with the rule are extracted (line 7), and a corresponding decision node is created if it does not already exists (line 10). Finally, the possible values specified for the decision variable are integrated to the node (line 12), and a dependency edge is established between the decision node and the utility node for the rule (line 13). 

\begin{algorithm}[h!]
\caption{: \textsc{InstantiateUtilRule} ($\mathcal{B}, r$)}
\begin{algorithmic}[1] \vspace{1mm}
\REQUIRE $\mathcal{B}$: Bayesian network for the current state
\REQUIRE $r$: Utility rule to instantiate in network  \vspace{1mm}
\STATE $\mathcal{I}_{r} \leftarrow $ input variables for rule $r$
\STATE Create utility node $R$ with the utility distribution in Eq. \eqref{eq:utildistrib}
\STATE Add node $R$ and dependency edges $\mathcal{I}_{r} \rightarrow R$ to $\mathcal{B}$ 
\IF {utility distribution is empty for all inputs}
\STATE Prune $R$ from $\mathcal{B}$
\ELSE
\STATE $\mathcal{D}_{r} \leftarrow$ decision variables defined for $R$
\FORALL {decision variable $D \in \mathcal{D}_r$}
\IF {$D'$ not already in $\mathcal{B}$}
\STATE Create decision node $D'$ and add it to $\mathcal{B}$
\ENDIF
\STATE Add in $Val(D')$ all possible values specified for it in the effects of $R$
\STATE Add dependency edge $D' \rightarrow R$ to $\mathcal{B}$ 
\ENDFOR
\ENDIF
\end{algorithmic}
\label{algo:instantiateUtilRule}
\end{algorithm}

\subsection{Application of quantifiers}
\label{sec:applicationquantif}

We saw in Section \ref{sec:quantification} that the conditions and effects of a rule could include universally quantified variables, but have not yet discussed how such underspecified rules could be practically instantiated in the Bayesian network. The general instantiation principle remains unchanged: to each rule corresponds a distinct rule node responsible for the mapping between input and output variables (or decision variables for utility rules). The instantiation procedure must be however extended to accommodate the presence of quantified variables. First, the extraction of input variables for the rule (line 1 in Algorithms \ref{algo:instantiateProbRule} and \ref{algo:instantiateUtilRule}) must account for underspecified names of random variables. Second and most importantly, the rule distributions must be adapted to combine the effects of multiple groundings. 

\subsubsection*{Extraction of input variables}

Rules that encompass quantifiers may underspecify both the names and values of random variables, as we saw in the examples of Section \ref{sec:quantification}.  Rule $r_3$ includes for instance a reference to an underspecified random variable $\mathit{shape}(z)$, where $z$ is universally quantified.  In order to instantiate the rule, the system must therefore search for all random variables included in the model that match the underspecified description. If rule $r_3$ is instantiated on a state containing two objects $o_1$ and $o_2$, the resulting input variables will therefore be $\mathit{shape}(o_1)$ and $\mathit{shape}(o_2)$. 

Algorithm \ref{algo:getinputvariables} outlines how this search for matching input variables can proceed. The first step is to extract the initial input variables for the rule (that may include underspecified descriptions such as $\mathit{shape}(z)$), and loops on each underspecified description to find possible groundings in the random variables of the Bayesian network.  The final result is defined as the combination of the fully specified variables for the rule and the possible groundings for the underspecified variables. 

\begin{algorithm}[h!]
\caption{: \textsc{GetInputVariables} ($\mathcal{B}, r$)}
\begin{algorithmic}[1] \vspace{1mm}
\REQUIRE $\mathcal{B}$: Bayesian network for the current state
\REQUIRE $r$: Probability or utility rule  \vspace{1mm}
\STATE $\mathcal{I}_{r} \leftarrow $ Initial (possibly underspecified) input variables for rule $r$
\STATE $\mathcal{U}_{r} \leftarrow $ Subset of variable names in $\mathcal{I}_{r}$ that are underspecified
\STATE $\mathcal{G}_{r} \leftarrow$ Set of possible groundings for $\mathcal{U}_{r}$, initially empty
\FOR {underspecified variable name $u \in \mathcal{U}_{r}$}
\FOR {random variable $X$ in $\mathcal{B}$}
\IF {$X$ matches $u$}
\STATE $\mathcal{G}_{r} \leftarrow \mathcal{G}_{r} \cup [X]$
\ENDIF
\ENDFOR
\ENDFOR
\RETURN $(\mathcal{I}_{r} / \mathcal{U}_{r}) \ \cup \  \mathcal{G}_{r}$
\end{algorithmic}
\label{algo:getinputvariables}
\end{algorithm}

\subsubsection*{Rule distribution with quantifiers}

We have seen in Section \ref{sec:probruleinstantiation} that probability rules were instantiated as a distinct node associated with a rule distribution, and that this node was connected to a set of output variables, each of which defined by an output distribution.  

 We can extend rule distributions defined in Equation \eqref{eq:ruledistrib} to incorporate quantified variables.  Let us assume a rule $r$ with input variables $I_1,...I_n$ and quantified over $\mathbf{z}$.  Given a particular assignment for the input variables $I_1,...I_n$, each rule condition $c_i$ is checked to find all possible groundings of $\mathbf{z}$  that makes $c_i$ satisfied.  Note that the number of groundings can be zero if the condition cannot be satisfied. The result of this operation is a set of groundings $G= \mathbf{g}_1,...\mathbf{g}_{|G|}$ where each grounding must satisfy least one condition given the input assignment.\footnote{This method of handling quantifiers by extracting all possible groundings is an instance of \textit{ground inference} \citep{getoor:srlbook07}. The possible groundings for a given input must be in this setting restricted to a finite set. In order to retain tractability, the maximum number of groundings is also capped by a threshold.} Algorithm \ref{algo:getgroundings} illustrates how groundings can be extracted given a rule and a particular input assignment for it. The expression $\phi[a/b]$ denotes (as in mathematical logic) the formula $\phi$ where all instances of the variable $a$ are substituted by $b$.
 
 %Groundings that satisfy a condition leading to an empty effect can be discarded. 
 
\begin{algorithm}[h!]
\caption{: \textsc{GetGroundings} ($r, \{I_1\!=\!i_1 \land ... I_n\!=\!i_n\}$)}
\begin{algorithmic}[1] \vspace{1mm}
\REQUIRE $r$: Probability or utility rule 
\REQUIRE $\{I_1\!=\!i_1 \land ... I_n\!=\!i_n\}$ assignment for the parent variables of the rule  \vspace{1mm}
\STATE $G \leftarrow \emptyset$
\FORALL {condition $c_i$ in rule $r$}
\STATE Let $\mathbf{z}_i$ be the quantified variables in $c_i$
\STATE $G_i \leftarrow \{\text{groundings } \mathbf{g}_k \text{ of } \mathbf{z}_i: \ I_1\!=\!i_1 \land ... I_n\!=\!i_n \vdash c_i [\mathbf{g}_k / \mathbf{z}_i ]\}$
\STATE $G \leftarrow G \cup G_i$
\ENDFOR
\RETURN $G$
\end{algorithmic}
\label{algo:getgroundings}
\end{algorithm}
 
 Groundings are therefore specific values for the universally quantified variables $\mathbf{z}$ such that the input assignment entails the satisfaction of at least one corresponding grounded condition (that is, a rule condition in which the occurrences of $\mathbf{z}$ are replaced by the grounded values). Each grounding $\mathbf{g}_k \in G$ gives rise to a particular effect distribution, and the total effect for the rule is the conjunction of effects $[e_1 \land ... e_{|G|}]$ for all possible groundings.  The probability of each conjunction is defined as the product of probabilities for the grounding-specific effects:
\begin{align}
& P(r\!=\![e_1 \land ... e_{|G|}] \, | \, I_1\!=\!i_1,... I_n\!=\!i_n) = \prod_{k=1}^{|G|} P(E_i = e_k[\mathbf{g}_k / \mathbf{z}]) \label{eq:quantifruledistrib}
 \\
& \; \; \; \; \; \; \; \; \text{where } i = \min_i (c_i[\mathbf{z} / \mathbf{g}_k]\text{ is satisfied with } I_1\!=\!i_1 \land ... I_n\!=\!i_n) \nonumber
\end{align}
The effects $e_1 \land ... e_{|G|}$ must be fully instantiated (i.e. without any remaining free variables).  %They can however include conflicting assignments, which are in such case automatically ``resolved'' in the output distribution by assigning the probability mass uniformly to the alternative values (as explained in the previous section).

Figure \ref{fig:quantinstantitionprob} shows how the rule $r_3$ is instantiated in a state with two objects $o_1$ and $o_2$, each associated with a random variable describing its shape.

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/quantruleinstantiation.pdf}
\caption{Instantiation of the probability rule $r_3$ on a state with two objects $o_1$ and $o_2$.}
\label{fig:quantinstantitionprob}
\end{figure}

Given an input assignment such as $shape(o_1) = \mathit{sphere}$ and $shape(o_2) = \mathit{cone}$, the probability distribution $P(r_3 \, | \, \mathit{shape}(o_1)\!=\!\mathit{sphere}, \mathit{shape}(o_1)\!=\!\mathit{cone})$ that reflects the specification in the rule includes a total of four alternative effects: \begin{itemize}
\item $\{\mathit{graspable}(o_1)\!=\!true \land \mathit{graspable}(o_2)\!=\!true \} $ with probability $0.9 \times 0.2 = 0.18$ 
\item $\{\mathit{graspable}(o_1)\!=\!true \land \mathit{graspable}(o_2)\!=\!false\}$ with probability $0.9 \times 0.8 = 0.72$ 
\item $\{\mathit{graspable}(o_1)\!=\!false \land \mathit{graspable}(o_2)\!=\!true \}$ with probability $0.1 \times 0.2 = 0.02$ 
\item $\{\mathit{graspable}(o_1)\!=\!false \land \mathit{graspable}(o_2)\!=\!false\}$ with probability $0.1 \times 0.8 = 0.08$. 
\end{itemize}

We observe that the effects specified for $r_3$ mention two output variables: $\mathit{graspable}(o_1)$ and $\mathit{graspable}(o_1)$.  These two variables are therefore created as children of the node $r_3$. 

\subsubsection*{Utility distribution with quantifiers}

Rule-based utility distributions can be similarly extended to accommodate quantified variables. As for probability rules, quantified utility rules determine a set of groundings that satisfy at least one condition in the rule.  Each grounding $\mathbf{g}_k$ generates a particular utility distribution.  For a utility rule $r$ with input variables $I_1,...I_n$, decision variables $A_1,...A_m$ and quantified variables $\mathbf{z}$, one can find a set of groundings $G= \mathbf{g}_1,...\mathbf{g}_{|G|}$ that satisfy at least a condition. The total utility distribution is then constructed by adding up the utility distributions for each grounding:

\begin{align}
& U_{r}(I_1\!=\!i_1,... I_n\!=\!i_n, A_1\!=\!a_1,... A_m\!=\!a_m) = \nonumber \\ 
&  \; \; \; \; \; \; \; \;  \sum_{k=1}^{|G|} U(D_i = \{A_1\!=\!a_1[\mathbf{g}_k / \mathbf{z}] \land... A_m\!=\!a_m[\mathbf{g}_k / \mathbf{z}]\}) \label{eq:quantifruledistrib}
 \\
& \; \; \; \; \; \; \; \; \text{where } i = \min_i (c_i[\mathbf{z} / \mathbf{g}_k]\text{ is satisfied with } I_1\!=\!i_1 \land ... I_n\!=\!i_n) \nonumber
\end{align}

The result of instantiating rule $r_4$ on a state with two objects $o_1$ with associated random variables $\mathit{graspable}(o_1)$ and $\mathit{graspable}(o_2)$ is shown in Figure \ref{fig:quantinstantitionutil}. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/quantutilruleinstantiation.pdf}
\caption{Instantiation of the quantified utility rule $r_9$ on a state with two objects $o_1$ and $o_2$.}
\label{fig:quantinstantitionutil}
\end{figure}

For a given input such as $\mathit{task}=\mathit{grasp}(o_1) \ \land \ \mathit{graspable}(o_1) = \mathit{true} \ \land \ \mathit{graspable}(o_2) = \mathit{false}$, one grounding $z=o_1$ is found to satisfy the first condition of the rule.  Given this grounding, the action  $a_m=\mathit{grasp}(o_1)$ is assigned a utility of 2. 

\subsubsection*{Remarks on tractability}

Although the use of quantifiers greatly increases the expressivity of probability and utility rules, they also raise tractability concerns, as the number of parents and children variables can grow quickly with the number of groundings extracted for the rule. In particular, the use of underspecified variable names leads to a rapid increase in the number of parent variables for the rule node. The specification of such rules should therefore be carefully engineered to ensure the inference problem remains tractable.  In practice, most of the quantified rules used in our experiments were designed in such a way that the number of extracted groundings were limited to a few instances.   It should be stressed that the groundings $\mathbf{g}_1,...\mathbf{g}_{|G|}$ are always extracted \textit{given a specific value assignment} for the input variables.  As the number of groundings satisfied for a given input is usually limited to one or two groundings, this instantiation procedure was found to be much more efficient than copying the rule in distinct nodes as initially investigated in \cite{relational-apl2012}.  

\section{Processing workflow}
\label{sec:processing-workflow}

The two previous sections have detailed how probability and utility rules are internally defined, and how they can be instantiated as nodes of a decision network. We are now ready to explain how collections of rules are practically applied at runtime to update the dialogue state and perform action selection. The general workflow is strongly inspired by information-state approaches to dialogue management \citep{Larsson:2000:ISD:973935.973943,Buckley:2006}, as the dialogue state serves as a central blackboard monitored by various groups of rules that are ``triggered'' upon relevant changes. 

We start by describing how dialogue domains can be organised in collections of rules called ``models''. We then show how these models are triggered to update state variables and express the utility of particular actions. We also demonstrate the general processing workflow on a detailed example. 


\subsection{Domain representation}

To ease the specification of dialogue domains, the probabilistic rules are grouped into collections called \textit{models}.  A model is simply a collection of rules that is associated with one or more ``trigger'' variables.  The update of such trigger variables in the dialogue state leads to the instantiation of all the rules included in the model. Formally, a model $m$ is defined as a pair $\langle \mathcal{T}_m, \mathcal{R}_m \rangle$ where $\mathcal{T}_m$ is a set of variables names that trigger the instantiation of the model, and $\mathcal{R}_m$ are the rules included in the model.

A dialogue domain is defined as a pair $\langle \mathcal{B}_0, \mathcal{M} \rangle$, where $\mathcal{B}_0$ is the initial dialogue state  and $\mathcal{M}$ the set of rule-based models attached to it. The organisation of rules into models allows the system designer to structure the application pipeline in a modular manner. Each model can be intuitively viewed as a distinct component responsible for a particular inference or decision step. Many reasoning tasks can be structured in terms of probabilistic rules.  Beyond the usual probability and utility models for dialogue management, probabilistic rules have also been applied in our work to natural language understanding and generation tasks.  The dialogue domain designed for the user experiments in Chapter \ref{chap:user-evaluation} included for instance a total of 6 models: one dialogue act classification model, (triggered by the user utterance $u_u$), one action utility model (triggered by the user dialogue act $a_u$), three probability models to predict the effects of the system action on the context, the user intention and the next user action (all triggered by the system action $a_m$), and a generation model (triggered by the system action $a_m$).  The association of trigger variables to the rule-based models provides a simple and flexible a way to define the processing pipeline for the application.  Variables can function as triggers for more than one model, allowing models to be instantiated in parallel.

As argued in \cite{lison-semdial2012}, the expressive power of probabilistic rules allow them to capture the structure of many dialogue processing tasks.  Compared to traditional architectures in which the components are developed separately and rely on ad hoc representation formats, the use of a shared formalism to encode the domain models yields several advantages:
\begin{description}
\item [Transparency: ] The reliance on a common representation format provides a unified, transparent semantics for the dialogue state, since all state variables are described and related to one another through a principled framework grounded in probabilistic modelling.  This makes it possible to derive a semantic interpretation for the dialogue state as a whole -- in terms e.g. of a joint probability distribution over the state variables. 

\item [Domain portability: ]  As all domain-specific knowledge is declaratively specified in the rules, the system architecture is essentially reduced to a generic platform for rule instantiation and probabilistic inference.  This declarative design greatly enhances the system portability across domains, since adapting a system to a new domain only requires a rewrite or extension of the domain-specific rules, without having to reprogram a single component.  This stands in sharp contrast with ``black-box'' types of architectures where much of the task- and domain-specific knowledge is encoded in procedural form within the component workflow.

\item [Flexible workflow: ] Probability rules can design very flexible processing pipelines where state variables are allowed to depend or influence each other in any order and direction.  Models can be easily inserted or extended without requiring any change to the underlying platform. Furthermore, several models can be triggered concurrently on the same input/output variables.\footnote{Output distributions can indeed handle effect specifications arising from multiple, sometimes conflicting sources, as we have seen in Section \ref{sec:probruleinstantiation}.} This allows the system to take advantage of multiple, complementary modelling strategies while ensuring that the dialogue state remains consistent. 

\item [Joint optimisation: ] Finally, the use of a unified modelling formalism allows domain models to be optimised jointly instead of being tuned in isolation from one another. Joint optimisation has recently gained much attention in the dialogue system community to overcome the fragmentation of current system architectures and attempt to directly optimise the end-to-end conversational behaviour of the system \citep{Lemon:2011}. 

\end{description}

It should be noted that the architecture does not in any way preclude the integration of other types of processing modules in addition to rule-structured models, as long as these modules can read and update the dialogue state when relevant changes are detected. 

\subsection{Update algorithm} 

The software architecture adopted in this thesis takes the form of an event-driven, blackboard architecture \citep{jaspis2004,Buckley:2006} revolving around a dialogue state $\mathcal{B}$ represented as a Bayesian network.  As in information state approaches, this dialogue state is read and written by the various modules integrated in the dialogue system. 

The general procedure to update the dialogue state upon the reception of new variables is provided in Algorithm \ref{algo:stateupdate}. State update is performed whenever a chance occurs on one or more state variables, due to e.g. to new inputs processed by the ASR or NLU component. The first step of the algorithm is to incorporate the new variable in the dialogue state and possibly relate them to their predicted values (lines 2-3 of Algorithm \ref{algo:stateupdate}, see below for more details). The algorithm then triggers the instantiation of the relevant domain models (line 4), leading to a recursive chain of updates.  If the expanded dialogue state contains decision and utility variables, the algorithms searches for the optimal action, selects it, and activates the models that are triggered as a result of this action  (lines 6-8). Finally, outdated and intermediary nodes are pruned away from the expanded state and the evidence is incorporated in the state distribution (line 10). 

We now describe each of these steps in detail.

\begin{algorithm}[h]
\caption{: \textsc{UpdateState} ($\mathcal{B}, \mathbf{X}$)}
\begin{algorithmic}[1] \vspace{1mm}
\REQUIRE $\mathcal{B}$: Bayesian network for the current state
\REQUIRE $\mathbf{X}$: New random variables to insert in the state \vspace{1mm}
\STATE Initialise evidence $\mathbf{e} \leftarrow \emptyset$
\STATE Insert $\mathbf{X}'$ to the current state $\mathcal{B}$ 
\STATE $\mathcal{B}, \mathbf{e} \leftarrow $ \textsc{IntegratePredictions}($\mathcal{B}, \mathbf{e}, \mathbf{X}'$)
\STATE $\mathcal{B}, \mathbf{e} \leftarrow$ \textsc{TriggerModels} ($\mathcal{B}, \mathbf{e},  \mathbf{X}'$) \vspace{1mm}
\WHILE {$\mathcal{B}$ contains decision variables}
\STATE $\mathbf{a}^* \leftarrow $ \textsc{SelectAction} ($\mathcal{B}, \mathbf{e}$)
\STATE Assign $\mathbf{A}' = \mathbf{a}^*$
\STATE $\mathcal{B}, \mathbf{e} \leftarrow$ \textsc{TriggerModels} ($\mathcal{B}, \mathbf{e}, \mathbf{A}'$)
\ENDWHILE \vspace{1mm}
\STATE $\mathcal{B} \leftarrow \textsc{PruneState} (\mathcal{B}, \mathbf{e})$ \vspace{1mm}
\end{algorithmic}
\label{algo:stateupdate}
\end{algorithm}

\subsubsection*{Connection between predictions and observations}

Rules are sometimes used to provide predictions on variables that will be observed in the next time steps.\footnote{This is notably the case for the user action model $P(a_u \, | \, i_u, a_m)$, which estimate the relative probabilities for the next dialogue action from the user. The prediction provide a prior on the future observation of the user action, and effectively prime the user actions that are most likely given a particular conversational situation.} In order to distinguish random variables that express a prediction on a future outcome from those that reflect an actual (although possibly uncertain) observation, we denote predictive variables with a subscript $p$. A variable $X_p$ is thus a prediction for the future observation of the variable $X$. 

\begin{wrapfigure}[9]{r}{48mm}
\vspace{-6mm}
\centering
\includegraphics[scale=0.25]{imgs/prediction.pdf} 
\caption{Equivalence node $eq_{X}$ with parents $X$ and $X_p$.}
\label{fig:prediction}
\end{wrapfigure}

Prediction and observation variables must be connected with one another at runtime.  In the case where the observation is known with certainty, this connection can simply be represented as an assignment of evidence values.  However, dialogue domains often include observations that are themselves uncertain and represent ``soft evidence'' \citep{Pan06beliefupdate}.  The method adopted in this thesis is to add a new chance node, subsequently called the \textit{equivalence node} $eq_{X}$, with boolean values that is conditionally dependent on both $X$ and $X_p$, as shown in Figure \ref{fig:prediction}. The addition of a distinct node to express the evidence is made necessary in our case since both $X$ and $X_p$ can have arbitrary incoming and outgoing edges with other variables in the dialogue state.  The conditional probability distribution of $eq_X$ is deterministic:
\begin{equation}
P(eq_{X}\!=\!true \, | \, X\!=\!x, X_p\!=\!x_p) = \begin{cases}
1 & \text{if } x = x_p \\
0 & \text{otherwise}
\end{cases} \label{eq:equivdistrib}
\end{equation}

The assignment $eq_{X} \!=\! true$ is added to the evidence. The posterior distribution given the evidence allows the prediction to act as a prior for the observed distribution:
\begin{align}
&P(X = x \, | eq_{X}\!=\!true) \nonumber \\
&=  \alpha \ P(X\!=\!x)  \sum_{x_p \in Val(X_p)} P(eq_{X}\!=\!true | X\!=\!x, X_p \!=\!x_p ) P(X_p\!=\!x_p) \\
&= \alpha \ P(X\!=\!x) \ P(X_p\!=\!x) \label{eq:equivalence}
\end{align}

Algorithm \ref{algo:integratepredictions} illustrates the process of integrating predictions for the variables $\mathit{Vars}$. 

\begin{algorithm}[h]
\caption{: \textsc{IntegratePredictions} ($\mathcal{B}, \mathbf{e}, \mathit{Vars}$)}
\begin{algorithmic}[1] \vspace{1mm}
\FORALL {$V \in \mathit{Vars}$}
\IF {there is a corresponding prediction variable $V_p \in \mathcal{B}$}
\STATE Create equivalence node $eq_{V}$ with distribution in Eq. \eqref{eq:equivdistrib}
\STATE Insert $eq_{V}$ in $\mathcal{B}$ with parents $\mathit{V}$ and $\mathit{V}_p$
\STATE Add assignment $[eq_{V}\!=\!true]$ to evidence $\mathbf{e}$
\ENDIF
\ENDFOR
\RETURN $\mathcal{B}, \mathbf{e}$
\end{algorithmic}
\label{algo:integratepredictions}
\end{algorithm}

\subsubsection*{Model instantiation}

After inserting the new variables in the dialogue state and connecting them to their predicted values, the next step in the processing workflow is to trigger the relevant domain models . 

Algorithm \ref{algo:triggerModels} summarises the steps involved in the instantiation of the domain models following an update of the dialogue state. The algorithm takes three arguments: a dialogue state $\mathcal{B}$ represented as a Bayesian network, an assignment of evidence values and a list of random variables that have been recently updated in the dialogue state. The algorithm loops on all domain models and instantiates the ones that are triggered by the updated variables. Once all models are checked, the output variables of the instantiated rules become updated variables themselves, and the procedure is repeated until no more model can be applied.  To avoid infinite triggering cycles, models are limited to one instantiation per state update. The algorithm returns both the dialogue state expanded with new variables, and the evidence assignment attached to the equivalence nodes. 


\begin{algorithm}[h]
\caption{: \textsc{TriggerModels} ($\mathcal{B}, \mathbf{e}, \mathit{UpdatedVars}$)}
\begin{algorithmic}[1] \vspace{1mm}
\WHILE {$\mathit{UpdatedVars} \neq \emptyset$}
\STATE $\mathit{NewVars} \leftarrow \emptyset$
\FORALL {models $m$}
\IF {$\mathit{UpdatedVars} \cap \mathcal{T}_m \neq \emptyset$ and $m$ has not yet been applied}
\FORALL {rule $r \in \mathcal{R}_m$}
\IF {$r$ is a probability rule}
\STATE $\mathcal{B} \leftarrow \textsc{InstantiateProbRule}(\mathcal{B},r)$
\ELSIF {$r$ is a utility rule}
\STATE $\mathcal{B} \leftarrow \textsc{InstantiateUtilRule}(\mathcal{B},r)$
\ENDIF
\STATE Let $\mathcal{O}_r$ be the new output variables created by rule $r$
\STATE $\mathit{NewVars} \leftarrow \mathit{NewVars} \cup \mathcal{O}_r$
\STATE $\mathcal{B}, \mathbf{e} \leftarrow $ \textsc{IntegratePredictions}($\mathcal{B}, \mathbf{e}, \mathcal{O}_r)$
\ENDFOR
\ENDIF
\ENDFOR 
\STATE $\mathit{UpdatedVars} \leftarrow \mathit{NewVars}$
\ENDWHILE 
\RETURN $\mathcal{B}, \mathbf{e}$
\end{algorithmic}
\label{algo:triggerModels}
\end{algorithm}


\subsubsection*{Action selection}

In case the new dialogue state contains utility and decision nodes, the system must decide on the action to perform.  Algorithm \ref{algo:actionselection} illustrates how actions can be automatically selected on the basis of the current dialogue state augmented with decision and utility variables. The algorithm searches for the assignment of action values that maximise the current utility given the dialogue state and the evidence and returns it. This utility maximisation is based on standard inference algorithms for decision networks such as generalised variable elimination and likelihood weighting (cf. Section \ref{sec:inference}). 

The utility nodes are removed from the state once the decision is made. The action selection procedure shown in Algorithm \ref{algo:actionselection} only takes into account the current (immediate) utility and does not rely on any forward planning.  Chapter \ref{chap:rllearning} demonstrates how action selection can be extended to perform online planning on a limited horizon. 

\begin{algorithm}[h]
\caption{: \textsc{SelectAction} ($\mathcal{B}, \mathbf{e}$)}
\begin{algorithmic}[1] \vspace{1mm}
\STATE Let $\mathbf{A}'$ be the set of all decision variables in $\mathcal{B}$
\STATE Find optimal value $\mathbf{a}^* = \argmax_{\mathbf{a}} U(\mathbf{A}' = \mathbf{a}, \mathbf{e})$
\STATE Remove utility nodes from the state $\mathcal{B}$
\RETURN $\mathbf{a}^*$
\end{algorithmic}
\label{algo:actionselection}
\end{algorithm}


\subsubsection*{State pruning}

The instantiation of the domain models in the dialogue state results in the integration of numerous new nodes in the dialogue state. However, many nodes in the expanded Bayesian network that results from this instantiation only serve as intermediaries and do not directly express any meaningful information about the current state of the dialogue. The last step of state update is therefore to reduce the dialogue state to its minimal size, by removing all intermediary nodes -- including rule nodes, outdated versions of state variables, equivalence nodes and predictive nodes that are attached to them -- in order to only retain current state variables. The accumulated evidence is also integrated in the posterior distribution of the state variables.

The procedure is outlined in Algorithm \ref{algo:pruneState}. The first step is to determine which nodes to keep (line 1-6).  Only the most recent versions of state variables are retained. 
The nodes are then added one by one in a new dialogue state $\mathcal{B}'$.  The parents of each variable is determined, and its conditional probability distribution is calculated given the evidence.  The parents of a state variable are the closest ancestors of the variable within the subset of nodes in $\mathit{NodesToKeep}$, and its conditional probability distribution is determined as $P_{\mathcal{B}}(N \, | \, \mathit{Parents}, \mathbf{e})$.  This posterior distribution is calculated via sampling techniques. This is technically done by sampling all nodes in $\mathcal{B}$, then deriving the distributions $P_{\mathcal{B}}(N \, | \, \mathit{Parents}, \mathbf{e})$ on the basis of the collected samples. 

\begin{algorithm}[h]
\caption{: \textsc{PruneState} ($\mathcal{B}, \mathbf{e}$)}
\begin{algorithmic}[1] \vspace{1mm}
\STATE $\mathit{NodesToKeep} \leftarrow \emptyset$
\FORALL {node $N \in \mathcal{B}$}
\IF {$N$ is a state variable and $\nexists \ N' \in \mathcal{B}$}
\STATE $\mathit{NodesToKeep} \leftarrow \mathit{NodesToKeep} \cup [N]$ 
\ENDIF
\ENDFOR
\STATE Create new state $\mathcal{B'} \leftarrow \emptyset$
\FORALL {node $N \in  \mathit{NodesToKeep}$}
\STATE Add node $N$ to $\mathcal{B'}$ (with primes removed from node name)
\STATE $\mathit{Parents} \leftarrow \{M \in \mathit{NodesToKeep} : M \text{ is an ancestor of } N \text{ and there is } $ \\ $\phantom{a}$  \; \; \; \; \; \; \; \; \;  a path $M \rightarrow^+  N \text{ without node in } \mathit{NodesToKeep} \}$ 
\STATE Add dependency edges between $\mathit{Parents}$ and $N$ in $\mathcal{B}'$
\STATE Assign distributions $P_{\mathcal{B}'}(N \, | \, \mathit{Parents}) \leftarrow P_{\mathcal{B}}(N \, | \, \mathit{Parents}, \mathbf{e})$
\ENDFOR
\RETURN $\mathcal{B}'$
\end{algorithmic}
\label{algo:pruneState}
\end{algorithm}


Figure \ref{fig:pruning} illustrates the input and output of the pruning process.  Only the nodes $A'$, $B$, $C$, $D$ and $E'$ are retained. Note that that primes attached to the labels of output variables are deleted from the random variable names.

\begin{figure}[h]
\centering
\includegraphics[scale=0.20]{imgs/pruning.pdf}
\caption{Illustration of the state pruning process. The left side shows a dialogue state before pruning, and the right side the corresponding state following the pruning operation. The dotted lines denote the correspondence between nodes.}
\label{fig:pruning}
\end{figure}


\subsection{Detailed example}

We now describe a minimal but complete example of workflow for a short interaction. 

\subsubsection*{Description}

Assume a domain similar to the one shown in Figure \ref{fig:fsa}, where a user can request a robot to move forward, backward, left, right, or stop.  The set of dialogue acts $a_u$ that can be recognised by the system is the following: 
\begin{center}
$\{\mathit{Request(Forward)}$, $\mathit{Request(Backward)}$, $\mathit{Request(Left)}$, \\ $\mathit{Request(Right)}$, $\mathit{Request(Stop)}$, $\mathit{Other}\}$. \\
\end{center}
The corresponding system actions $a_m$ are: 
\begin{center}
$\{\mathit{Move(Forward)}$, $\mathit{Move(Backward)}$, $\mathit{Move(Left)}$, \\ $\mathit{Move(Right)}$, $\mathit{Move(Stop)}$, $\mathit{AskRepeat}\}$. 
\end{center}
The objective of the system is to fulfil the user command if it is reasonably certain of which action to execute.  Otherwise, the system asks the user to repeat. 

\subsubsection*{Domain specification}

The domain specification designed for this constructed example is constituted of an empty initial state and the following two rule-based models: \begin{itemize}
\item Model $m_1 = \langle \langle a_u \rangle, \langle r_9, r_{10} \rangle \rangle$ is triggered by $a_u$ and includes two utility rules $r_{9}$ and $r_{10}$:
\begin{align*}
r_{9}: \ \ & \forall z: \\ 
& \textbf{if} \ (a_u = Request(z)) \ \textbf{then} \\ 
& \begin{cases} 
U(a_m = Move(z)) = 2 \\ 
\end{cases} \\
& \textbf{else} \\ 
& \begin{cases} 
U(a_m = Move(z)) = -2 \\ 
\end{cases} \\[4mm]
r_{10}: \ \ &  \begin{cases} U(a_m = \mathit{AskRepeat}) = 0.5 \end{cases}
\end{align*}

Rule $r_{9}$ specifies that the utility of executing the action corresponding to the user command is 2, with a penalty of $-2$ when the wrong action is executed. Rule $r_{10}$ assign a utility of 0.5 for asking a clarification question.\footnote{As the action selection process presented thus far does not perform forward planning, the utilities provided in this example correspond to long-term expected utilities (Q-values in the reinforcement learning terminology).}

\item Model $m_2 = \langle \langle a_m \rangle, \langle r_{11} \rangle \rangle$ is triggered by $a_m$ and includes the predictive rule $r_{11}$: 
\begin{align*}
r_{11}: \ \ & \forall z: \\ 
& \textbf{if} \ (a_m = \mathit{AskRepeat} \land a_u=z) \ \textbf{then} \\ 
& \begin{cases} 
P(a_{u\mbox{-}p} = z) = 0.9 \\ 
\end{cases}
\end{align*}
Rule $r_{11}$ specifies that the probability that the user will repeat his last utterance when asked by the system to do so is expected to be $0.9$.
\end{itemize}

\subsubsection*{Processing workflow}

We now show the processing workflow associated with the following constructed interaction:
\begin{dialogue} 
\speak{User } Now move forward \\ $\phantom{b}$ $\tilde{a}_u = \langle (\mathit{Request(Forward)}, 0.6), (\mathit{Request(Backward)}), 0.4)\rangle$  \\[-3mm]
\speak{System } Could you please repeat? \\[-3mm]
\speak{User } Please move forward! \\ $\phantom{b}$ $\tilde{a}_u = \langle (\mathit{Request(Forward)}, 0.7), (\mathit{Other}, 0.2), (\mathit{Request(Backward)}, 0.1) \rangle$ \\[-3mm]
\speak{System } OK, moving forward!
\end{dialogue}

The recognition hypotheses $\tilde{a}_u$ produced by the system are written underneath the user utterance.  Figure \ref{fig:detailedexample} details the steps involved in the state update procedure that follows from the reception of dialogue act hypotheses from the natural language understanding component. 

\begin{figure}[p]
\centering
\includegraphics[scale=0.25]{imgs/detailedexample.pdf}
\caption{Detailed example of processing workflow. }
\label{fig:detailedexample}
\end{figure}
 
 
Step 1 inserts the new dialogue act hypotheses on the dialogue state.  This insertion triggers 
the utility model $m_1$. The instantiation results in Step 2 in the creation of two utility nodes and one decision node.  The optimal action to perform in such case is $\mathit{AskRepeat}$, which is selected by the system in Step 3. The action selection triggers model $m_2$ in Step 4, which creates a prediction node $a_{u\mbox{-}p}'$ expressing the expected probability distribution for the next user dialogue act. The state is finally pruned of the intermediary rule node in Step 5.  System components such as NLG can react on the updated state and generate the proper linguistic realisation of the system action. The system then waits for the user input, which is shown in Step 6.  The relation between the predicted and actual user response leads in Step 7 to the creation of an equivalence node, and the inclusion of the assignment $eq_{a_u} = true$ in the evidence. We notice that the combination of the prior distribution over predicted values and the actual distribution over dialogue act hypotheses increases the probability of $a_u' = \mathit{Request(Forward)}$. Step 8 triggers the model $m_1$ based on the new user input.  The optimal action is this case is $\mathit{Move(Forward)}$, which is selected in Step 9.  This selection triggers model $m_2$, but rule $r_{12}$ only generates in this case an empty effect and is therefore directly deleted. Finally, the state is pruned of its intermediary nodes in Step 10, leaving only the last user and system actions $a_u$ and $a_m$. 

\section{Advanced modelling}
\label{sec:amodelling}

Dialogue domains often include random variables whose values are expressed via specific data structures such as lists or strings. The rule-based formalism described in the previous sections can be complemented with special-purpose tools to efficiently operate on these data structures.  We describe in this section of these advanced modelling questions.  We first explain how conditions and effects can be defined on variables that represent lists, and then discuss how rules can manipulate strings. 

\subsection{Operations on lists}

Some state variables are best represented as lists of elements. For instance, the dialogue state may include random variables that enumerate e.g. the $n$ most recent dialogue acts in the interaction history, the stack of tasks that remain to perform, or the list of visual objects perceived by the system.  The range of values for such state variables is the power set of its possible elements. 

Both the conditions and effects of probabilistic rules can be adapted to include special-purpose operations for lists:
\begin{itemize}
\item Rule conditions can include include operators to check the presence or absence of particular elements in a list, such as $a \in A$ or $a \notin A$. 
\item Rule effects can also be augmented to manipulate elements from a list.  Three new types of effects are created to this end, in addition to the traditional assignment of output values: \textit{add effects} (adding an element to a list), \textit{delete effects} (deleting an element from a list) and \textit{clear effects} (clearing all elements of a list). The resulting lists are sorted by insertion order. 
\end{itemize}

Figure \ref{fig:seteffects} illustrates two rules that make use of add and delete effects to update the distribution defined for the state variable $A$. 
 
\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/seteffects.pdf}
\caption{Example of rules using add/delete effects to manipulate lists.}
\label{fig:seteffects}
\end{figure}

Output distributions $P(X'\!=\!x' \, | \, r_1\!=\!e_1,... r_n\!=\!e_n)$ must be adapted to take  these three new types of effects into account.  Let $\mathbf{e}$ denote as before the conjunction of all effects $e_1 \land ... e_n$. In addition to the classical set of values $\mathbf{e}(X)$ assigned for the variable $X$, we define two new sets of values $\mathbf{e}_{add}(X)$ and $\mathbf{e}_{del}(X)$ that represent the values that are respectively added and deleted for the variable $X$ through the new effects mentioned above, as well as a boolean value $\mathbf{e}_{clear}(X)$ which indicates whether at least an effect requires the existing values for $X$ to be cleared.

Given these definitions, the output distribution defined in Equation \eqref{eq:outputdist1} for a new output variable can then be rewritten as: 
\begin{align}
&P(X'\!=\!x' \, | \, r_1\!=\!e_1,... r_n\!=\!e_n) = \hspace{10cm} \nonumber \\ & \; \; \; \; \;\; \; \; \; \; \begin{dcases} 
\frac{\sum_{v \in {\mathbf{e}(X)}} \mathbf{1}(x' = v)} { |\mathbf{e}(X)| }&  \text{if } \mathbf{e}(X)\!\neq\!\emptyset \\
\mathbf{1}(x' = \mathbf{e}_{add}(X))  &  \text{if } \mathbf{e}(X)\!=\!\emptyset \ \text{ and } \ \mathbf{e}_{add}(X)\!\neq\!\emptyset \\
\mathbf{1}(x' = \mathit{None}) & \text{otherwise}
\end{dcases}\label{eq:outputdist3}
\end{align}

Lists are defined to be equal if they include the same elements in the same order. The traditional assignment effects in $\mathbf{e}(X)$ and the add effects $\mathbf{e}_{add}(X)$ are mutually incompatible. In case both $\mathbf{e}(X)$ and $\mathbf{e}_{add}(X)$ are non-empty, the assignment effects are by convention assumed to take precedence. 

In case the output variable replaces an existing variable, the output distribution becomes:
\begin{align}
&P(X'\!=\!x' \, | \, r_1\!=\!e_1,... r_n\!=\!e_n, X\!=\!x) = \hspace{10cm} \nonumber \\ & \; \; \; \; \; \; \; \; \; \;  \begin{dcases} 
\frac{\sum_{v \in {\mathbf{e}(X)}} \mathbf{1}(x' = v)} { |\mathbf{e}(X)| }  & \text{if } \mathbf{e}(X)\!\neq\!\emptyset \\
\mathbf{1}(x' = \left[\mathbf{e}_{add}(X) \cup \left[x \ / \ \mathbf{e}_{del}(X)\right]\right]) & \text{if } \mathbf{e}(X)\!=\!\emptyset \ \text{ and } \ \neg \ \mathbf{e}_{clear}(X) \\
\mathbf{1}(x' = \mathbf{e}_{add}(X))  &  \text{if } \mathbf{e}(X)\!=\!\emptyset, \mathbf{e}_{add}(X)\!\neq\!\emptyset \ \text{ and }  \mathbf{e}_{clear}(X) \\
\mathbf{1}(x' = \mathit{None}) & \text{otherwise}
\end{dcases}\label{eq:outputdist4}
\end{align}

%Other types of collections such as sets with no duplicate elements can be exploited in the same manner. 

\subsection{Operations on strings}

Many of the data structures present in the dialogue state are strings -- the most prominent ones being the last user utterance $u_u$ and the last system utterance $u_m$. It is therefore also useful to provide special-purpose functionalities for manipulating strings within the conditions and effects of probabilistic rules. In particular, rules can be extended to perform template-based string matching operations.  The idea is to include a new type of conditions that checks whether a string matches a given template.  Both full and partial matching can be employed. Templates can comprise slots to fill. These slots are conceptually similar to the quantified variables discussed in Sections \ref{sec:quantification} and \ref{sec:applicationquantif}. A successful match will thus generate values for the filled slots, which will be included as part of the groundings extracted in Algorithm \ref{algo:getgroundings}. 

Figure \ref{fig:stringmanip} illustrate how such rules are applied in practice.  $\{OBJ\}$ denotes a slot that is to be filled through matching the template with the value specified in $u_u$. 
\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{imgs/stringmanip.pdf}
\caption{Example of rule using string matching operations.}
\label{fig:stringmanip}
\end{figure}

\section{Relation to previous work}
\label{sec:relatedwork}

The idea of using structural knowledge in probabilistic models has been explored in many directions, both in the fields of decision-theoretic planning and reinforcement learning \citep{Hauskrecht98,Pineau2004,KerstingR04,lang10jair,Otterlo2012} and in statistical relational learning \citep{Jaeger01,Richardson:2006,getoor:srlbook07}.  The introduced structure may be hierarchical, relational, or both. As in our approach, most of these frameworks rely on the use of expressive representations as high-level templates for the generation of classical probabilistic models.  The surveys of \cite{Otterlo2006,Otterlo2012} provide a complete overview of relational and first-order logical approaches for reinforcement learning in fully and partially observable Markov decision processes, covering both model-free and model-based methods. 

The probabilistic rules bear several similarities with planning description languages such as the Planning Domain Description Language \citep[PDDL, see ][]{mcdermott1998} and its probabilistic extension, the Probabilistic Problem Description Language \citep[PPDDL, see ][]{younes2004ppddl1}.  These languages are structured through action schemas that specify how (parametrised) actions can yield particular effects under various conditions. As in probabilistic rules, these languages try to carefully balance between the language expressivity and the complexity of the planning algorithm, based on a subset of first-order logic. A relational extension of PDDL, named RDDL, has also been introduced in recent planning competitions \citep{Sanner:RDDL}. The learning techniques presented by \cite{PasulaZK07} to estimate transition functions based on noisy indeterministic deictic rules is directly related to our approach, as is the recent work \cite{lang10jair} on probabilistic noisy planning rules.   Both frameworks rely on conditions associated with probabilistic distributions over effects. Their approach is however restricted to fully observable settings. 

In the dialogue management literature, most structural approaches rely on a clear-cut task decomposition into goals and sub-goals \citep{Allen:2000:AGD:973935.973937,Steedman-Petrick:07,Bohus:2009:RDM:1518321.1518367}, where the completion of each goal is assumed to be fully observable, discarding any remaining uncertainty.  Our own work on multi-policy dialogue management in \cite{multipolicy-sigdial2011} relaxes the assumption of perfect knowledge of task completion, handling multiple policies as a problem of probabilistic inference over activation variables.  Probabilistic rules can be considered an extension of this early work, where the structural knowledge is not confined to task decomposition but is extended to generic rules over state variables.  The formalism presented in this chapter is strongly inspired by information-state approaches to dialogue management \citep{Larsson:2000,Bos2003}, which are similarly based on a shared state that can be updated according to a rich repository of rules.  \cite{Ginzburg2012} also models conversational phenomena by way of update operations that are encoded with rules mapping conditions to effects. However, contrary to the framework presented here, these rules specified in these approaches are generally deterministic and do not include learnable parameters. 
 
The literature on dialogue policy optimisation with reinforcement learning also contains several approaches dedicated to dimensionality reduction for large state-action spaces, such as function approximation \citep{Henderson:2008}, hierarchical reinforcement learning \citep{Cuayahuitl:2010} and summary POMDPs \citep{Young:2010}.  Many of these techniques have already been discussed in Section \ref{sec:application-dm} and will therefore not be repeated here. Most current approaches in dialogue policy optimisation focus on large but weakly structured state spaces (generally encoded as large lists of features), which are suited for slot-filling dialogue applications but are difficult to transfer to more open-ended or relational domains.  The idea of state space partitioning, implemented here via high-level conditions, has also been explored in recent papers \citep{Williams2010}. \cite{Crook:2010} explored the introduction of complex user goal states including disjunction and negation operators. \cite{Heriberto2011} describes a policy optimisation approach based on logic-based representations of the state-action space for relational MDPs. The main difference with our approach lies in his reduction of the belief state to fully observable variables whereas we retain the partial observability associated with each variable.  The work of \cite{Mehta:2010,Raux2011} showed how tree-structured Bayesian networks called probabilistic ontology trees can improve belief tracking performance.  The tree structure is derived in their work from a hierarchical concept structure .  Finally, \cite{neill2011} describe a procedure for dialogue strategy selection based on probabilistic logic.

\section{Conclusion}

This chapter presented the formalism of probabilistic rules, which forms the core of the modelling approach developed in this thesis. We started by arguing that dialogue models are often highly structured, and that such structure can be leveraged by including latent variables, partitioning value assignments for the parent variables, and introducing quantifiers. We then explained how these insights can be transferred into a new framework -- probabilistic rules -- that combines concepts borrowed from both first-order logic and probability theory.  These rules are practically defined as \textit{if...then...else} control structures that associate conditions on input variables to probabilistic effects on output variables.  Multiple extensions of the formalism have been developed to e.g. encode utility distributions, include quantified variables in the conditions and effects, and efficiently manipulate data structures such as lists and strings. 

At runtime, these rules are instantiated in the Bayesian network representing the current dialogue state. The instantiation procedure creates a latent node for each rule, which is conditionally dependent on the input variables of the rule.  For probability rules, this node is a chance node that expresses a probability distribution over the possible effects of the rule. Utility rules are similarly instantiated with a utility node expressing the utility distribution for specific decision variables.  Quantified variables can be included in the conditions and effects of the rules, allowing particular aspects of the rule to be underspecified.  The outlined instantiation procedure allow probability and utility rules to function as high-level templates for the construction of a dynamic decision network (and notably POMDPs, since they are a special case of DDNs). The rules are grouped into models that are attached to the dialogue state and are triggered upon relevant state updates. 

The expressive power of probabilistic rules enables them encode complex relations between variables in a very efficient manner, and thereby reduce the number of parameters to estimate.  We have however not yet detailed how this parameter estimation is practically performed. The next two chapters will provide answers to this important question. 
