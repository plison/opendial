%\documentclass[11pt,norsk,a4paper]{report}
%\documentclass[openright,twoside,12pt,norsk,a4paper]{report}

\documentclass[11pt,a4paper,openright,twoside]{report}

%\usepackage[a4paper,pdftex]{geometry}
%\setlength{\oddsidemargin}{5mm}												% Remove 'twosided' indentation
%\setlength{\evensidemargin}{5mm}


%\documentclass[11pt,twoside,final,english]{report}

%\usepackage{a4wide}
\usepackage[english]{babel}
%\usepackage{natbib}
\usepackage[square]{natbib}

\usepackage{/home/gisle/phd/latex/qtree}
\usepackage{xytree}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
%\usepackage{pst-tree}
%\usepackage[left=1in, right=1in]{geometry}
%\psset{levelsep=*1cm, treesep=0.25cm, nodesep=3pt, treefit=tight}

\usepackage{avm}
\usepackage{url}
\usepackage{color}
\usepackage{itemmacro}
\usepackage[dvips]{epsfig}
\usepackage{verbatim}
\usepackage{multirow}
\usepackage{appendix}
\usepackage{abstract}

\newcommand{\rel}[1]{\spred{#1}}
\newcommand{\attrib}[1]{\mbox{\sc\lowercase{#1}}}
\newcommand{\rtype}[1]{\mbox{\sc\lowercase{#1}}}
\newcommand{\feature}[1]{\mbox{\it #1}}



\usepackage{lscape}


%\usepackage{fixltx2e}
\usepackage{fancyhdr}
%\usepackage{wrapfig}
%\usepackage{subfigure}


%\usepackage{latexsym}
%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{textcomp}
%\usepackage[scaled=0.92]{helvet}
%\usepackage{url}
\usepackage{relsize}
%\usepackage{apacite}
%\usepackage{gb4e}
\usepackage{mrs}
%\usepackage[noload]{qtree}
%\usepackage{stfloats} 


\setlength{\headheight}{14pt}
\usepackage{linguex}
\bibpunct{(}{)}{;}{A}{,}{,}

%\includeonly{kap4}

%\usepackage{babel,graphicx,mathpple,textcomp,varioref}
%%%% MAKE NORWEGIAN LETTERS WORK %%%%
%\catcode`�=\active\catcode`�=\active\catcode`�=\active
%\catcode`�=\active\catcode`�=\active\catcode`�=\active
%\def�{\ae}\def�{\o}\def�{\aa}
%\def�{\AE}\def�{\O}\def�{\AA}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\avmoptions{active}
\qtreecenterfalse
%opening
\title{\Huge{Transition-Based Parsing for Large-Scale Head-Driven Phrase Structure Grammars}}

\author{\\
Doctoral Dissertation by\\
\\
\Large{Gisle Ytrest\o l} \\
\\
\\
 \includegraphics[scale=0.5]{../../uio-segl/Apollonseglet/Apollonseglet/UiO_Segl-preg.eps}
\\
\\
\\
\\
\\
\\
 Department of Informatics\\
Faculty of Mathematics and Natural Sciences\\
University of Oslo\\
\\
Submitted for the degree of Philosophiae Doctor}

\date{April 2012}

%\author{Gisle Ytrest\o l}

%\renewcommand{\abstracttextfont}{\Huge}
\renewcommand{\abstractnamefont}{\Huge\bfseries}
\begin{document}

\noitemsep
%\fancyhead[LO]{\leftmark}

%\fancyhead[RE]{\emph{Chapter \thechapter}}

% Define pagestyle



\pagestyle{empty}
\begin{titlepage}
\bibliographystyle{plainnat}

%%%%%%%

\maketitle



%\begin{center}
%\textit{I see only one move ahead, but it's always the correct one.}\\
%Jose Raul Capablanca, World Chess Champion 1921-1927
%\end{center}
 
\end{titlepage}


\null
\vfill
\begin{center}
    \copyright 2012 -- Gisle Ytrest\o l\\
    All Rights Reserved.
\end{center}

%\pagestyle{plain}
%\pagenumbering{roman}

\begin{abstract}
Deterministic, transition-based parsing has seen a surge of interest over the recent decade, with research efforts targeting Dependency Grammar, Context-Free Grammar, Head-Driven Phrase Structure Grammar (HPSG), and Combinatory Categorial Grammar. 
Previous work, however, has not applied the transition-based approach to parsing with hand-crafted, large-scale unification-based grammars.

Basing our studies on the English Resource Grammar (ERG), we evaluate the feasibility of transferring strategies and methods from other transition-based approaches  to a semantically `deep', hand-crafted HPSG.
Our parsing platform, dubbed CuteForce, constitutes a pipeline which assumes pretokenized sentences, and produces syntacto-semantic representations in accordance with the ERG framework. The components in this pipeline include a preprocessing and supertagging stage and a transition-based parsing stage where both deterministic and near-deterministic strategies are evaluated. We  evaluate the supertagger in isolation, and compare our overall parsing results to other ERG parsers. This allows us to assess the trade-offs a transition-based parsing approach for large-scale HPSGs may have in terms of parser precision, robustness and efficiency, compared to `classic' parsing approaches. 

Both the preprocessing stage and the transition-based parser rely on large amounts of training data. To ensure that we had sufficient linguistic resources for our data-driven platform, the first part of the project was committed to extracting a corpus from Wikipedia, and convert this data to a gold standard treebank (WeScience Treebank) and a `silver standard' parsed corpus (WikiWoods). Utilizing Wiki\-pedia as a linguistic resource has received increased attention, and we expect that the methodology for corpus acquisition presented in this thesis could also prove useful to other research initiatives. 

We find that large amounts of `silver standard' training data allows us to train a supertagger that reaches a previously unmatched level of supertagging accuracy for the ERG. Further, our evaluation shows that although the transition-based parser does not obtain state-of-the-art accuracy, it still reaches a high level of accuracy, coupled with much higher parsing efficiency than other parsers based on the same grammar, making it a suitable choice amongst others when speed has high priority.


%there have been comparatively few research initiatives targeting large-scale, broad-coverage precision grammars. 

%In this thesis we will explore transi\-tion-based parsing for a unification-based,  semantically `deep' hand-crafted grammar.
 %framework, namely head-driven phrase structure grammar (HPSG). 

 %, and evaluate the trade-offs that can be observed with respect to parser precision, robustness and efficiency. 
%and an optional post-processing stage which creates approximative semantic representation through the use of default unification.



%etc


%For training a competitive statistical parsing system, the amount or available language resources of ERG proved insufficient, and a substantial part of the project was committed to extracting and treebanking data from Wikipedia, and convert this data into goldstandard treebanks (WeScience treebank) and silver-standard parsed corpus (WikiWoods). Utilizing Wikipedia as a language resource has received increased awareness, and we expect that the methodology for corpus acquisition presented in this thesis can be useful for other research initiatives. 

%Further, we develop a supertagger for ERG, as an initial stage in the processing pipeline for the parsing platform. This supertagger, trained exclusively on parsed, silver-standard data, reaches a very high level of precision, attesting the feasibility of using non-gold standard data in a competitive statistical tagger. 

%Our project demonstrates that deterministic, transition-based parsing is feasible strategy for deep, blah,blah


\end{abstract}

\cleardoublepage
\renewcommand{\abstractname}{Acknowledgements}


\begin{abstract}
After four years of working on this PhD project, there are many people to whom I owe a debt of gratitude.
First I would like to thank my advisor Stephan Oepen. His relentless enthusiasm, support, assistance and cryptic scribblings have followed me and been my most important inspiration throughout the PhD project.
Further, I would like to extend my gratitude to my two co-supervisors. Jan Tore L\o nning has given me invaluable feedback during this PhD project, always prepared to share  his vast insights and expertise in areas such as parsing and mathematical aspects. Joakim Nivre most generously agreed to take on the role as co-supervisor during the middle of this project, and he has, with his in-depth knowledge of (amongst other things) transition-based parsing, been an invaluable resource for me in the project. 
With this distinguished team of (co-)advisers, there were no external factors to blame if I did not successfully pull this project through. 

In this context, I would also like to thank all of my colleagues at the Language Technology Group at the University of Oslo. From this group, I would particularly like to emphasize the support from Rebecca Dridan, who I have been asking about every aspect of supertagging, and who also helped me with many a linguistic challenge that arose for a non-native speaker attempting to write a thesis in English. Additionally, I want to thank Erik Velldal, who has always been prepared to help me out with anything ranging from machine learning questions to caffeine deficit. 

The DELPH-IN community has been an outstanding resource, and in this context I would especially like to  express my gratitude to Dan Flickinger, who has been involved in  all aspects regarding the linguistic resources that have been applied or developed in this project. I would also like to thank Ivan Sag, who, and together with Flickinger made my stay as a visiting scholar at CSLI at Stanford University a most enlightening  and comfortable experience. I also want to thank Yi Zhang for sharing both his robust meaning composition framework and parsing results for the Jigsaw parser.

Further, I would like to extend my thanks to GSLT (Swedish National Graduate School of Language Technology) for facilitating and sharing their excellent courses, which have been most valuable for me during this PhD. Experimentation and engineering was made possible through access to the TITAN high-performance computing facilities at the University of Oslo (UiO), and I am  grateful to the Scientific Computation staff at UiO, as well as to the Norwegian Metacenter for Computational Science. This project relies heavily on  a range of open source software, and I am most grateful for the developers who make their software and source code freely available.

Finally, I would like to thank Mareike for constantly reminding me that, regardless of the importance of deep semantic grammars, there are after all more important things in life. 

\end{abstract}




\pagestyle{plain}
\pagenumbering{roman}
\setcounter{tocdepth}{1}
\tableofcontents


\cleardoublepage


%\include{technical}

%\include{kap1}

\pagenumbering{arabic}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\chaptermark}[1]{\markboth{ \emph{#1}}{}}
%\fancyhead[LO]{}
\fancyhead[RE]{\leftmark}
\fancyhead[LO]{\emph{Chapter \thechapter}}
\fancyfoot[LE,RO]{\thepage}

\include{introduction}


%\include{background}

\include{backgroundReorganized}
 

\input{ergResources}


\include{supertagging}
\include{hpsgparsing}




\include{noneDeterm}

\include{resEval}
\include{summary}

\cleardoublepage

\appendix
\renewcommand{\appendixtocname}{Appendix}
\addappheadtotoc
\fancyhead[LO]{\emph{Appendix \thechapter}}
\input{genLexTypesAppend}
\input{appendTransStatCb}
\input{appendSR}


\cleardoublepage
%\pagestyle{plain}
\fancyhead[LO]{}


\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{/home/gisle/phd/latex/master,/home/gisle/phd/latex/gisle}
\cleardoublepage




%\include{linklist}

%\include{seedarticles}

\end{document}
