
\chapter{User evaluation}
\label{chap:user-evaluation}
\index{user evaluation|textbf}

This chapter presents the last (and most extensive) experiment carried out in this thesis. The empirical studies described so far in Chapters \ref{chap:wozlearning} and \ref{chap:rllearning} essentially focused on the learning performance of rule-structured models compared to more classical representations. However, although such experiments can provide useful insights about the model adequacy for various dialogue domains, they are not by themselves sufficient to assess the suitability of a particular modelling approach and its practical effects on the quality of the produced interactions. 

In order to corroborate our claims regarding the benefits of probabilistic rules in dialogue management and contrast them to traditional methods, we conducted a user evaluation experiment with a total of 37 users. The aim of the experiment was to compare three alternative approaches to dialogue management (two baselines and our own approach) in a human--robot interaction domain.  The three approaches respectively correspond to a purely hand-crafted approach, a purely statistical approach, and a hybrid approach based on probabilistic rules. The empirical results collected via this experiment indicate that the  hybrid, rule-structured approach\index{dialogue management!hybrid approach to} is able to outperform the two baselines on most of the metrics defined for the domain.  The metrics include not only objective metrics extracted from the interaction logs but also subjective metrics of user satisfaction, based on a survey completed by the participants after each interaction. 

The chapter is structured into three sections.  The first section describes the human--robot interaction scenario within which the experiment is carried out, and details both the state variables, system architecture, and training data made available for the domain. The second section then exposes the three alternative dialogue management approaches developed for the experiment, with a particular emphasis on the design choices that characterise each strategy.   Finally, the third section presents the user evaluation itself, and surveys the experimental setup, quality metrics, and of course the empirical results and their detailed analysis. 

\section{Interaction scenario}
\index{human--robot interaction}
The interaction scenario employed for the user evaluation is similar in most respects to the one presented in Section \ref{sec:rllearning-experiments}, modulo some minor adaptations to circumscribe more precisely the task to be performed.\footnote{This change was suggested by several participants in the previous study.} The interactions revolve around a human user and a Nao robot\index{Nao robot} standing on a large table, as illustrated in Figure \ref{fig:scenario}.  The purpose of the interaction is to instruct the robot to (1) walk to the other end of the table without bumping into the imaginary walls placed in the middle of the scene, (2) pick up the designated object, (3) bring it back to the left side of the table, and (4) release it on a yellow landmark.  The robot is able to perceive the presence, colour and location of each physical object thanks to visual markings placed at their top.  It can also grasp each object with the help of permanent magnets fixed inside the hands of the robot. 

\begin{figure}[ht]
\vspace{3mm}
\centering
\includegraphics[scale=0.13]{imgs/scenario.jpg} \vspace{3mm}
\caption{Interaction scenario for the experiment.  Two graspable objects (one red and one blue) stand on the right side of the table.  Imaginary walls (in green) are also placed on the table, as well as a yellow landmark depicting the final destination of the robot. }
\label{fig:scenario}
\end{figure}


\subsection{Dialogue domain}
\index{dialogue domain}
\subsubsection*{User intentions and actions}
\index{user intention}
As for the previous experiment, we represent each basic instruction (such as moving forward, turning left, picking up an object, etc.) as a distinct user intention $i_u$. The possible user intentions for the domain are shown in Table \ref{table:userintents_exp3}.  
%After the fulfilment of each instruction, the user intention is reinitialised to represent the next instruction to perform.  

We introduced a few minor changes compared to the Wizard-of-Oz study presented in Section \ref{sec:rllearning-experiments}. The basic movements are now augmented with a second argument specifying the duration of the movement (short or long). A new type of movement has also been added, $\mathrm{Move(Turn)}$ to make the robot turn around by 180 degrees.  Finally, specific intentions are used to capture the opening (engagement) and closing (disengagement) phases of the dialogue. 

The possible user actions\index{user action} $a_u$, listed in Table \ref{table:userdas_exp3} correspond to the verbal realisations of these user intentions. The user can directly utter an instruction, ask the robot to repeat its last movement, or respond to a clarification or confirmation request.   The user action $\mathrm{Ask(Stop)}$ can also be used to command the robot to stop its current movement, while grounding actions represent explicit feedback provided by the user on the robot behaviour.

\subsubsection*{System actions}
\index{system action}
As shown in Table \ref{table:systemdas_exp3}, the system actions $a_m$ include: \begin{itemize}
\item Physical movements to move around in various directions and lengths, pick up or put down physical objects on the table, or stop the current move.
\item Verbal responses to describe the robot's current perception or tell the user that it cannot perform a requested movement due to some impeding factor (e.g.\ not being able to pick up an object whose location is currently unknown). 
\item Clarification and confirmation requests, such as asking the users to repeat their last utterance, or confirming a particular intention.
\end{itemize}

Instead of representing grounding actions in a separate category of actions (as in Table \ref{table:systemdas_exp2}), the grounding acts are now directly coupled to the system actions. The selection of the system action $\mathrm{Do}(X)$ thus triggers both a grounding action (\utt{Ok, now doing $X$}) and the simultaneous execution of the corresponding physical movement. 

\renewcommand{\arraystretch}{1.3}

\begin{table}[p!]
\begin{footnotesize}
\begin{tabular}{p{60mm}} 
$\cdot$ $\mathrm{Move}(x,y) $ \\ $ \ \ \ \ \ \text{ where } x=\{\mathrm{Left,Right,Forward,}$ \\ $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathrm{Backward}\} $ \\ $ \ \ \ \ \ \ \text{ and } y = \{\mathrm{Short, Long}\}$ \\
$\cdot$ $\mathrm{Move(Turn)} $ \\
$\cdot$ $\mathrm{PickUp}(x) $ \\ $\ \ \ \ \  \text{ where } x \text{ is }   \mathrm{AtFeet} \text{ or  an object identifier}$
\end{tabular}
\hspace{2cm}
\begin{tabular}{p{60mm}} 
$\cdot$ $\mathrm{ReleaseObject} $ \\
$\cdot$ $\mathrm{WhatDoYouSee}$ \\
$\cdot$ $\mathrm{DoYouSee}(x) $ \\ $\ \ \ \ \  \text{ where } x \text{ is an object identifier}$ \\
$\cdot$ $\mathrm{Greeting}$ \\
$\cdot$ $\mathrm{Closing}$ 
\end{tabular}
\end{footnotesize}
 \caption{List of user intentions $i_u$.} 
\label{table:userintents_exp3}
\end{table}


\begin{table}[p!]
\begin{footnotesize}
\begin{tabular}{p{60mm}} 
$\cdot$ $\mathrm{Ask(Move(x,y))} $ \\ $ \ \ \ \ \ \text{ where } x=\{\mathrm{Left,Right,Forward,}$ \\ $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathrm{Backward}\} $ \\ $ \ \ \ \ \ \ \text{ and } y = \{\mathrm{Short,Long}\}$ \\
$\cdot$ $\mathrm{Ask(Move(Turn))} $ \\
$\cdot$ $\mathrm{Ask(PickUp(x))} $ \\ $\ \ \ \ \  \text{ where } x \text{ is }   \mathrm{AtFeet} \text{,  an object identifier,}$ \\ $\ \ \ \ \  \text{ or }  \mathrm{Other} \text{ if the reference is not resolved}$ \\
$\cdot$ $\mathrm{Ask(ReleaseObject)} $ \\
$\cdot$ $\mathrm{RepeatLast}$ \\
$\cdot$ $\mathrm{Ask(Stop)}$ 
\end{tabular}
\hspace{2cm}
\begin{tabular}{p{60mm}} 
$\cdot$ $\mathrm{Ask(WhatDoYouSee)}$ \\
$\cdot$ $\mathrm{Ask(DoYouSee(x))} $ \\ $\ \ \ \ \  \text{ where } x \text{ is an object identifier }$ \\ $\ \ \ \ \  \text{ or }  \mathrm{Other} \text{ if the reference is not resolved}$ \\
$\cdot$ $\mathrm{Confirm}$ \\
$\cdot$ $\mathrm{Disconfirm}$ \\
$\cdot$ $\mathrm{Grounding}$ \\
$\cdot$ $\mathrm{Greet}$ \\
$\cdot$ $\mathrm{Close}$ \\
$\cdot$ $\mathrm{Other}$ 
\end{tabular}
\end{footnotesize}
 \caption{List of user actions $a_u$.} 
\label{table:userdas_exp3}
\end{table}

\begin{table}[p!]
\begin{footnotesize}
\begin{tabular}{p{60mm}} 
$\cdot$ $\mathrm{Do}(x) $ \\ $ \ \ \ \text{ where } x=\{\mathrm{Move}(y,z), \mathrm{Move(Turn)},$ \\ $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathrm{PickUp}(a),  \mathrm{ReleaseObject},$ \\ $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathrm{Stop}\} $ \\ $ \ \ \ \ \ \ \text{ and } y = \{\mathrm{Left,Right,Forward,}$ \\ $ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathrm{Backward}\} $ \\ $ \ \ \ \ \ \ \text{ and } z = \{\mathrm{Short,Long}\}$ \\ $ \ \ \ \ \ \  \text{ and } a =    \mathrm{AtFeet} \text{ or  an object identifier}$ \\
$\cdot$ $\mathrm{Excuse}(x) $ \\ $ \ \ \  \text{ where } x = \{\mathrm{UnseenObject}$ \\ $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathrm{UnresolvedReference},$  \\ $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathrm{NoCarriedObject,}$ \\ $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathrm{AlreadyCarryObject}\}$ \\
$\cdot$ $\mathrm{Greet}$ \\
$\cdot$ $\mathrm{Goodbye}$ 
\end{tabular}
\hspace{2cm}
\begin{tabular}{p{60mm}} 
$\cdot$ $\mathrm{Describe}(x) $ \\ $ \ \ \  \text{ where } x = \ \text{a (possibly empty) list}$ \\ $ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \text{of object identifiers}$ \\
$\cdot$ $\mathrm{ConfirmDetection}$ \\
$\cdot$ $\mathrm{DisconfirmDetection}$  \\
$\cdot$ $\mathrm{AskRepeat}$ \\
$\cdot$ $\mathrm{AskConfirm}(x) $\\ $ \ \ \ \text{ where } x=\{\mathrm{Move}(y,z), \mathrm{Move(Turn)},$ \\ $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathrm{PickUp}(a),  \mathrm{ReleaseObject},$ \\ $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathrm{Stop}\} $ \\ $ \ \ \ \ \ \ \text{ and } y = \{\mathrm{Left,Right,Forward,}$ \\ $ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \mathrm{Backward}\} $ \\ $ \ \ \ \ \ \ \text{ and } z = \{\mathrm{Short,Long}\}$ \\ $ \ \ \ \ \ \  \text{ and } a = \ \text{an object identifier}$ \end{tabular}
\end{footnotesize}
 \caption{List of system actions $a_m$.} 
\label{table:systemdas_exp3}
\end{table}

\subsubsection*{Dialogue state}
\index{dialogue state}

The dialogue state $\mathcal{B}$ designed for the dialogue domain is factored into eleven distinct state variables, encoding both the user intentions and actions, the external context, the recent interaction history, and the physical status of the robot. Table \ref{table:statevariables} details the name, purpose, range of possible values, observability and dependencies of each state variable. As illustrated in the table, the dialogue state contains both fully and partially observed variables. 


%One can distinguish three distinct types of state variables in this factorisation:
%\begin{enumerate}
%\item State variables such as $u_u$, $\mathit{perceived}$, $\mathit{carried}$ and $\mathit{motion}$ represent observations arising from the robot perception, including speech recognition. %Observations such as $\mathit{carried}$ and $\mathit{motion}$ are (near-)certain, while the last user utterance and the set of perceived objects are uncertain observations expressed through categorical distributions over alternative values. 
%\item State variables such as $i_u$, $a_u$, $a_{u\mbox{-}prev}$ and $\mathit{completed\mbox{-}task}$ correspond to hidden variables that must be indirectly inferred from the accumulated observations and actions.
%\item Finally, variables such as $a_m$, $u_m$ and $\mathit{lastMove}$ represent (present or past) decisions made by the system. These decisions are naturally known to the system and are thus fully observed. 
%\end{enumerate}

\renewcommand{\arraystretch}{1.9}
\setlength{\tabcolsep}{8pt}
\begin{sidewaystable}
\begin{center}
\begin{tabular}{|p{35mm}||p{7cm}p{5mm}|p{34mm}p{5mm}|p{27mm}|p{4cm}|} \hline
\textbf{Variable label:} & \textbf{Description:}  && \textbf{Range of values:} && \textbf{Observability: } & \textbf{Dependencies: }\\ \hline\hline
$\mathit{perceived}$ & (Possibly empty) set of perceived objects && Object identifiers && Partial& $\emptyset$ \\\hline
$\mathit{carried}$ & (Possibly empty) set of carried objects && Object identifiers && Full  & $\emptyset$ \\\hline
$i_u$ & Current user intention && cf. Table \ref{table:userintents_exp3} && Partial  & $\mathit{completed\mbox{-}task}$, $\mathit{perceived}$, $\mathit{carried}$ \\\hline
$u_u$ & Last user utterance && ASR hypotheses && Partial & $\emptyset$ \\ \hline
$a_u$ & Last dialogue act from the user && cf. Table \ref{table:userdas_exp3} && Partial & $u_u$, $i_u$, $a_m$, $\mathit{lastMove}$ \\\hline
$a_{u\mbox{-}prev}$ & Next-to-last dialogue act from the user && cf. Table \ref{table:userdas_exp3} && Partial & $\emptyset$ \\\hline
$a_m$ & Last system action && cf. Table \ref{table:systemdas_exp3} && Full & $i_u$, $a_u$, $\mathit{motion}$, \ \ \ \ \  $\phantom{a}$ $\mathit{perceived}$, $\mathit{carried}$ \\ \hline
$\mathit{motion}$ & Activity status of the robot (denoting whether the robot is currently moving) && $\mathit{true}$ or $\mathit{false}$ && Full  & $\emptyset$  \\\hline
$\mathit{lastMove}$ & Last physical movement executed by robot && System actions of \ \ \ \ $\phantom{a}$ the form $\mathrm{Do}(x)$ && Full & $a_m$ \\\hline
$u_m$ & Last system utterance && NLG candidates && Full & $a_m$ \\ \hline
$\mathit{completed\mbox{-}task}$ & Completion status of the user intention (denoting whether the current intention $i_u$ has been fulfilled by the last action) \vspace{2mm} && $\mathit{true}$ or $\mathit{false}$ && Partial & $i_u$, $a_m$ \\ \hline
\end{tabular}
\end{center}
\caption{List of state variables defined for the domain.}
\label{table:statevariables}
\end{sidewaystable}

Even when ignoring the two variables $u_u$ and $u_m$ (which have a virtually infinite set of possible values, as they accept arbitrary strings) and assuming the presence of only two objects, the total size of the joint state space is non-trivial to tackle: 
\begin{align}
|\mathcal{S}| & = & 4 \ \ \ \ \ \ \ \ \ \ &&& \ \ \ \ \ \ \ \ \ \ [\text{size of } \mathit{Val}(\mathit{perceived})] \nonumber \\
 && \times 3 \ \ \ \ \ \ \ \ \ \ &&&  \ \ \ \ \ \ \ \ \ \ [\text{size of } \mathit{Val}(carried)] \nonumber \\
 && \times 18 \ \ \ \ \ \ \ \ \ \ &&&\ \ \ \ \ \ \ \ \ \  [\text{size of } \mathit{Val}(i_u)] \nonumber \\
&&  \times 26 \ \ \ \ \ \ \ \ \ \ &&&\ \ \ \ \ \ \ \ \ \  [\text{size of } \mathit{Val}(a_u)] \nonumber \displaybreak[3] \\
&&  \times 26 \ \ \ \ \ \ \ \ \ \ &&&\ \ \ \ \ \ \ \ \ \  [\text{size of } \mathit{Val}(a_{u\mbox{-}prev})] \nonumber \\
&&  \times 41 \ \ \ \ \ \ \ \ \ \ &&&\ \ \ \ \ \ \ \ \ \  [\text{size of } \mathit{Val}(a_m)] \nonumber \\
&&  \times 2 \ \ \ \ \ \ \ \ \ \ &&&\ \ \ \ \ \ \ \ \ \  [\text{size of } \mathit{Val}(\mathit{motion})] \nonumber \\
&&  \times 14 \ \ \ \ \ \ \ \ \ \ &&&\ \ \ \ \ \ \ \ \ \  [\text{size of } \mathit{Val}(\mathit{lastMove})] \nonumber \\
&&  \times 2 \ \ \ \ \ \ \ \ \ \ &&&\ \ \ \ \ \ \ \ \ \  [\text{size of } \mathit{Val}(\mathit{completed\mbox{-}task})] \nonumber \\
 & =  & \!\!\!\!\!\!\!\!\!\! 335 \, 252 \, 736  \ \ \ \ \ \ \ \ \ \ &&&\ \ \ \ \ \ \ \ \ \ \nonumber 
  \end{align}


\subsection{System architecture}
\index{dialogue system architecture}

The dialogue system employed for the experiment follows the architecture described in the previous chapter (Section \ref{sec:system-integration}).  We briefly describe below the configuration of each component. It should be stressed that the only component that varies across the three evaluated approaches is the dialogue manager.  All other modules are fixed and remain identical for all interactions.

\begin{description}
\item[Speech recognition: ]\index{speech recognition} The speech recognition engine relies on a hand-written recognition grammar of limited coverage. The grammar used for the experiment is shown in Table \ref{table:asr}.  The speech recognition engine runs directly on the robot platform based on the audio signals captured by four microphones on the robot head.

\setlength{\grammarindent}{9em} 

\begin{table}[p!]
\vspace{2cm}
\begin{grammar}

<TopRule> ::= <Move-1> | <Move-2> | <PickUp> | <Release> | <Perception-1> | <Perception-2> | <Confirmation> | <Grounding> | <Repeat> | <Opening> | <Closing> | <Stop> 

<Move-1> ::= <Front> (walk | go | move | continue)? <Translation> <Back>

<Move-2> ::= <Front> (go | move | turn | rotate)? <Rotation> <Back>

<Translation> ::= <Modifier>? (forward | backward | back | straight (forward | ahead)?) <Modifier>?

<Rotation> ::= (<Modifier>? (to the)? (left | right) <Modifier>?) | around | 180 degrees

<Modifier> ::= (just)? a (little)? bit (more)? 

<PickUp> ::= <Front> ((take | pick up | grasp) <Object>) | pick it up | take it) <Back>

<Release> ::= <Front> ((release | put down | drop) <Object>) | (put <Object> down)) <Back> 

<Perception-1> ::= what (can | do) you (see | perceive)

<Perception-2> ::= (can | do) you (see | perceive) (<Object> | something | anything)

<Object> ::= (the | a) <Colour>? (object | cylinder | food can) (at your feet)? 

<Colour> ::= red | blue | green | yellow | white | black 

<Confirmation> ::= yes (please)? | no | exactly | ok 

<Grounding> ::= (that is | this is)? (correct  | incorrect | wrong | right | perfect | great | good) 
%| (thanks | thank you) <Name>?

<Repeat> ::= <Front> (do (it | that) (again | once more | one more time) | repeat that)

<Opening> ::= (hi | hello ) <Name>

<Closing> ::= (bye | goodbye) <Name>?

<Stop> ::= stop (it | that)?
 
<Front> ::= (and)? (could you)? (now)? (please)?

<Back> ::= (please)? (again)?

<Name> ::= robot | nao | lenny

\end{grammar}
\caption{Speech recognition grammar (in Bachus-Naur form) employed for the experiment.}
\label{table:asr}
\end{table}

\item[Natural language understanding: ]\index{natural language understanding} Natural language understanding is implemented in the dialogue system through probability rules.  A total of 17 rules (of varying size) have been written for the domain, based on simple template-matching methods. One rule is specifically responsible for the resolution of referring expressions based on the current visual context. The probability rules are listed in Appendix \ref{chap:domainspecs} (Section \ref{sec:domainspecs-usereval}). 

\item[Dialogue management: ]\index{dialogue management} Three distinct types of dialogue managers have been developed for this experiment: a finite-state automata, a purely statistical dialogue manager, and a dialogue manager based on probabilistic rules.  The development of these three dialogue management approaches is presented in Section \ref{sec:dialmodels_exp3}. 

\item[Generation and synthesis: ]\index{natural language generation}\index{speech synthesis} The natural language generation is realised through the application of one single rule in charge of the mapping between the selected system action and its verbal realisation. The rule is also detailed in Appendix \ref{chap:domainspecs}.  The generated sentence is then converted to a speech signal using the speech synthesis module.

\item [Other components: ] As detailed in the previous chapter, the integrated dialogue system also comprises specific modules dedicated to object detection and motor control.  The detection and localisation of the physical objects is facilitated by the use of visual markings on top of the objects.  The robot employs permanent magnets to pick up and hold objects in its hand. 

\end{description}

\subsection{Wizard-of-Oz data collection}
\label{sec:wozcollection-exp3}
\index{Wizard-of-Oz interaction}

In order to design empirically accurate dialogue management models for the experiment, we started by collecting a set of Wizard-of-Oz interactions for the dialogue domain. We recorded a total of 10 interactions, each with a distinct participant. All interactions were performed in English. The participants to the Wizard-of-Oz study (5 males and 5 females) were selected amongst students and employees working at the Department of Informatics. All but one participant were non-native speakers of English. The author of the present thesis served as the wizard.

The interactions were once again encoded as a sequence of pairs $\langle \mathcal{B}_i, a_i \rangle$, where each system turn $i$ is represented by the selected wizard action $a_i$ and the dialogue state $\mathcal{B}_i$ in effect at the time the selection was made. The interactions had on average 84.2 system turns.  39 \% of these turns resulted in an void action (i.e.\ no action at all), 40 \% to a physical action (complemented by a feedback describing the action being performed), and the remaining 21 \% to a verbal response such as a factual answer or a clarification request. 

The variables expressed in the dialogue states $\mathcal{B}_i$ of the collected data are directly extracted from the interaction logs. As a consequence, the recorded dialogue state $\mathcal{B}_i$ includes only variables that are directly observed by the system during the Wizard-of-Oz study: the last and next-to-last user actions $a_u$ and $a_{u\mbox{-}prev}$ (as provided by the ASR and NLU modules), the last system action $a_m$, the set of objects currently perceived and carried by the robot, the last physical movement of the robot, and the $\mathit{motion}$ variable denoting whether the robot is currently in movement. 

Here is an example of a data point extracted from the collected data, corresponding to the user utterance \utt{yes} after a confirmation request, followed by the system action $\mathrm{Do(Move(Right))}$: 

\begin{align*}
\mathcal{B} = \begin{cases} u_u = \langle (``\text{yes}", p\!=\!.95), (\mathit{None}, p\!=\!.05) \rangle \\
a_u = \langle (\mathrm{Confirm}, p\!=\!.95), (\mathit{None}, p\!=\!.05) \rangle \\
a_{u\mbox{-}prev} = \langle (\mathrm{Ask(Move(Right))}, p\!=\!.63), (\mathit{None}, p\!=\!.37) \rangle \\
a_m = \langle (\mathrm{AskConfirm(Move(Right))}, p\!=\!1) \rangle \\
u_m = \langle (``\text{should I move right?}", p\!=\!1) \rangle \\
\mathit{perceived} = \langle ([\mathit{RedObj}], p\!=\!1) \rangle \\
\mathit{carried} = \langle ([], p\!=\!1) \rangle \\
\mathit{motion} = \langle (\mathit{false}, p\!=\!1) \rangle \\
 \end{cases} \!\!\!\!\!\Rightarrow a_m' = \mathrm{Do(Move(Right))}
\end{align*}


\section{Dialogue management models}
\label{sec:dialmodels_exp3}

In order to evaluate how the choice of a dialogue management framework affects the interaction quality, we developed three competing dialogue management approaches for the domain:
\begin{itemize} 
\item The first approach is a purely hand-crafted dialogue manager using a finite-state automaton to process the user inputs and determine the corresponding responses. 
\item The second approach is a purely statistical dialogue manager that encodes the transition and utility model of the domain based on classical, factored probabilistic models.
\item Finally, the third approach relies on a hybrid modelling strategy and uses the formalism of probabilistic rules to structure the domain models.
\end{itemize}

We detail below the exact design of each approach.

\subsection{Approach 1: hand-crafted model}
\label{sec:approach1}
\index{dialogue management!symbolic approaches to}
\index{finite-state automaton}

The first approach relies on a traditional finite state automaton to determine the current conversational situation and its corresponding system action.  Although more complex logic- or plan-based methods (such as the ones described in Section \ref{sec:hand-crafted}) could in principle also be applied, a finite-state automaton augmented with a basic support for logical reasoning was in practice found to be sufficient to capture the key characteristics of the dialogue domain. 

The finite state automaton is triggered upon each new user dialogue act $a_u$. Attached to each edge is a logical condition that determines when the edge can be traversed. The conditions are defined on the basis of the current dialogue act $a_u$, previous dialogue act $a_{u\mbox{-}prev}$, and contextual variables $\mathit{perceived}$ and $\mathit{carried}$. It should be noted that the finite state automaton directly operates at the level of the user dialogue acts $a_u$ and $a_{u\mbox{-}prev}$ without explicitly computing the underlying user intention $i_u$, since the user intention corresponds in this domain to a hidden variable and cannot be adequately modelled by purely symbolic (i.e.\ non-probabilistic) representations. 

\subsubsection*{Finite state automaton}

The automaton constructed for the domain operates on the basis of three specified threshold values $T_1$, $T_2$ and $T_3$ (whose estimation is described shortly):
\begin{enumerate}
\item If the incoming dialogue act contains an hypothesis whose probability is higher than a given threshold $T_3$, the system action associated to the user request is directly selected.  For instance, if the new user act contains a hypothesis $a_u = \mathrm{Ask(Move(Left))}$ with probability $p > T_3$, the action $a_m = \mathrm{Do(Move(Left)})$ will be executed.
\item If the probability of the top hypothesis for $a_u$ lies between $T_2$ and $T_3$ and corresponds to a request for a physical movement, the system will ask the users to confirm their intention.  The system response to a user act $a_u = \mathrm{Ask(Move(Left))}$ with probability $T_2 < p < T_3$ will hence be set to $a_m = \mathrm{AskConfirm(Move(Left)})$.
\item If the probability of the top hypothesis lies between $T_1$ and $T_2$, the system will ask the users to repeat their utterance by triggering the action $a_m = \mathrm{AskRepeat}$. 
\item Finally, if no hypothesis reaches the minimal threshold $T_1$, the user action is assumed to correspond to a spurious recognition result and is simply ignored.
\end{enumerate}

A graphical representation of the finite state automaton is provided in Figure \ref{fig:fsa-exp3}.  The automaton contains 16 distinct states.  Depending on the probability of the top hypothesis in the user dialogue act $a_u$, the automaton will either directly transit from the state $s_1$ to the states $s_4, \dots, s_{15}$ or be redirected to state $s_2$ or $s_3$ for clarification. After each system action in the states $s_4, \dots, s_{14}$, the current state is re-initialised to state $s_1$ via empty transitions. 

Since spurious speech recognition results are often produced when the robot is executing physical movements, the finite-state automaton employs a higher threshold ($P(a_u) > T_2$ instead of $T_1$) for moving from state $s_1$ to $s_3$ whenever the variable $\mathit{motion}$ is set to true.


\begin{figure}[p]
\centering
\includegraphics[scale=0.36]{imgs/fsa-exp3.pdf} 
\caption{Finite state automata designed for the domain (slightly simplified for clarity's sake). The dashed arrows denote empty transitions. For presentation purposes, the conditions applied to the edges going from $s_1,s_2,s_3$ to $s_4, \cdots, s_{15}$ are decomposed in two parts: the first part from the incoming node to the small white circle, and the second part from the circle to the outgoing node. $T_1$, $T_2$ and $T_3$ are threshold values determined empirically.}
\label{fig:fsa-exp3}
\end{figure}


\subsubsection*{Probability thresholds}

The threshold values $T_1$, $T_2$ and $T_3$ are determined empirically on the basis of the actual probability values generated by the speech recogniser and NLU module during the Wizard-of-Oz study.  This estimation was achieved in two steps.  The first step is to extract all probability values associated with the most likely hypothesis of the user dialogue act $a_u$ in the recorded Wizard-of-Oz interactions and derive a probability density from these values through kernel density estimation\index{kernel density estimation}. The resulting density function is shown in Figure \ref{fig:asrconfidence-exp3}. As a second step, we divided the distribution over probability values for the top user act hypothesis into four non-overlapping regions:
\begin{itemize}
\item Region 1 (cf. figure) corresponds to user actions that most likely arise from spurious recognition and should hence be ignored. Based on a detailed inspection of the wizard actions in such cases, we mapped this region to the lowest quintile of the distribution (that is, all values with a cumulative density between $0$ and $0.2$). 
\item Region 2 corresponds to user actions that are more likely to reflect real user inputs, but do not have sufficient confidence to be directly executed.  This region was mapped to the second quintile of the distribution (values with a cumulative density between $0.2$ and $0.4$).
\item Region 3 corresponds to user actions that have relatively high confidence, but should nevertheless be confirmed once before execution.  This region was constrained to the third quintile of the distribution (values with a cumulative density between $0.4$ and $0.6$).
\item Finally, Region 4 corresponds to user actions that have a sufficiently high confidence to be directly executed without user confirmation. This region defines the top 40 \% of the probability mass, with cumulative densities between $0.6$ and $1.0$.
\end{itemize}

The estimated thresholds for the density function shown in Figure \ref{fig:asrconfidence-exp3} were derived to be $T_1 = 0.49$, $T_2 = 0.61$ and $T_3 = 0.73$. These values reflect the three thresholds employed in the finite state automaton. 

\begin{figure}[h!]
\centering
\includegraphics[scale=0.45]{imgs/asrconfidence.pdf} 
\caption{Probability density function for the probability values of the top hypothesis specified in the user dialogue act $a_u$, divided into four non-overlapping regions that respectively correspond to the first, second, third, and fourth+fifth quintiles of the distribution. Red crosses on the X axis represent the actual probability values used to construct the kernel density function.}
\label{fig:asrconfidence-exp3}
\end{figure}


\subsection{Approach 2: factored statistical model}
\label{sec:approach2}
\index{dialogue management!statistical approaches to}

The second approach developed for this experiment consists of a statistical model whose parameters are estimated from the Wizard-of-Oz data set collected for the experiment. We assume that both the transition model and utility model of the domain are initially unknown. 

Both this approach and Approach 3 rely on a planning horizon limited to the present time step.  In other words, the approaches do not use a forward planner (which has proven to be practically difficult to apply in live interactions) and directly selects the action yielding the highest utility in the current dialogue state.\footnote{In other words, the utility model represents what is called a $Q$-value model in reinforcement learning.} The approach stands relatively close to the POMDP-based systems described in Chapter \ref{chap:probmodelling}, with the difference that the estimation of model parameters in this experiment is performed on the basis of Wizard-of-Oz data rather than through a  simulator. 

\subsubsection*{Domain modelling}
\index{domain modelling}

Figure \ref{fig:unstructuredmodel-exp3} illustrates the factored model employed to structure the dialogue domain of the experiment.  As shown in the figure, the domain models are factored in four major distributions (the three first ones being part of the transition model, while the fourth one is a utility model): 
\begin{itemize}
\item The task completion model\index{task completion model} $P(\mathit{completed\mbox{-}task}\, | \, i_u, a_m)$ describes the probability that the current intention $i_u$ is fulfilled by the last system action $a_m$.  The task completion model $P(\mathit{completed\mbox{-}task}\, | \, i_u, a_m)$ is essentially a deterministic distribution that marks the task as completed when the executed action $a_m$ fulfils the user intention $i_u$,  and as not completed in all other cases. %The task completion model is therefore defined without parameters. 
\item The user goal model\index{user goal model} $P(i_u' \, | \, i_u, \mathit{completed\mbox{-}task}, \mathit{perceived'}, \mathit{carried'})$ encodes the likelihood of a new user intention $i_u'$ given the current intention $i_u$, task completion $\mathit{completed\mbox{-}task}$, as well as the two contextual variables $\mathit{perceived}'$ and $\mathit{carried}'$. 
\item The user action model\index{user action model} $P(a_u'\, | \, i_u', a_m)$ describes the probability of the next user dialogue act $a_u'$ given the user intention $i_u'$ and the last system action $a_m$.
\item Finally, the utility model\index{utility model} $U(a_m', i_u', a_u', \mathit{carried'}, \mathit{perceived'}, \mathit{motion'})$ describes the utility of a given system action $a_m'$ given the user intention $i_u'$, last user dialogue act $a_u'$, robot motion status $\mathit{motion'}$ and contextual variables $\mathit{carried}'$ and $\mathit{perceived}'$. 
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{imgs/unstructuredmodel.pdf} 
\caption{Factored transition and utility models for the dialogue domain.}
\label{fig:unstructuredmodel-exp3}
\end{figure}

\subsubsection*{Dimensionality reduction}
\index{dimensionality reduction}

% As shall be explained shortly, the probabilistic models developed in this approach do include a fair amount of internal structure and rely on a number of assumptions to limit the number of parameters, but without resorting to the more advanced modelling techniques that characterise rule-structured models (such as latent rule nodes and quantification). The purpose of this model is thus to evaluate the performance of a classical statistical approach when provided with the Wizard-of-Oz interaction data set at our disposal. 

Unfortunately, the four factored models listed above remain too large to be directly estimated from the limited amounts of data made available from the Wizard-of-Oz interactions.  We therefore introduced a range of additional constraints and simplifying assumptions to further reduce the number of parameters associated with the model.

%\begin{align}
%&&&P(\mathit{completed\mbox{-}task}\!=\!\mathit{true} \, | \, i_u, a_m) = \nonumber \\
%&&& \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \begin{cases} 1 & \text{if } \ a_m \  %\text{fulfills the intention } i_u \\
%0 & \text{if } \ a_m\!=\!\mathrm{AskRepeat} \text{ or } \mathrm{AskConfirm} \\
%\theta_{\text{changeOfMind}} & \text{otherwise} \end{cases} \label{eq:completedtask-exp3}
%\end{align} 

The first simplifying assumption pertains to the user goal model $P(i_u' \, | \, i_u, \mathit{completed\mbox{-}task},$ $\mathit{perceived}', \mathit{carried}')$, and rests on the idea that the user intention remains more or less constant -- modulo a small probability of revision to account for sudden changes of mind on the part of the user -- until the task is marked as completed. The values of the two contextual variables $\mathit{perceived}'$ and $\mathit{carried}'$ variables are furthermore grouped into two partitions: empty set or non-empty set. These two simplifications reduce the number of parameters required for the user goal model to four Dirichlet distributions, each with 18 dimensions (which is the number of possible user intentions). 

%\begin{align}
%&&&P(i_u'  = x \, | \, i_u, \mathit{completed\mbox{-}task}, \mathit{perceived}, \mathit{carried}) = \nonumber \\
%&&& \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \begin{cases} \theta_{i_u'\!=\!x|\mathit{perceived}, \mathit{carried}} & \text{if } \  \mathit{completed\mbox{-}task}= true \\
%\mathbf{1}(x = i_u) & \text{otherwise} \end{cases}
%\end{align}

We also compressed the size of the user action model $P(a_u'\, | \, i_u', a_m)$ by exploiting the fact that only a fraction of user dialogue acts $a_u'$ are locally relevant for a given user intention $i_u'$ and last system action $a_m$.  A manually specified function $relevant: \mathit{Val}(i_u') \times \mathit{Val}(a_m) \rightarrow 2^{\mathit{Val}(a_u)}$ is used to derive the set of relevant user actions given a particular pair of user intention and system action. For instance, the relevant dialogue acts for the user intention $\mathrm{Move(Left,Short)}$ are $\mathrm{Ask(Move(Left,Short))}$, $\mathrm{RepeatLast}$, $\mathrm{Confirm}$, $\mathrm{Disconfirm}$, $\mathrm{Stop}$ and $\mathrm{Grounding}$. The resulting set of parameters associated with the user action model contains 133 Dirichlet distributions, each with 6 dimensions (the number of relevant actions allowed per intention). 

The third and final heuristic is to reduce the number of relevant system actions in the utility distribution $U(a_m', i_u', a_u', \mathit{carried}', \mathit{perceived}'$, $\mathit{motion'})$.  Similarly to the user action model, we can exploit the fact that only a fraction of system actions are relevant in a given state, and that the non-relevant ones have a constant negative utility. In order to express this constraint, we first define a function $best: \mathit{Val}(i_u') \times \mathit{Val}(\mathit{perceived}') \times \mathit{Val}(\mathit{carried}') \rightarrow \mathit{Val}(a_m)$ that maps each user intention and context to the ``best'' action that can be executed at that state if we were to assume no uncertainty. This best action is identical to the action selected by the finite state automaton in Figure \ref{fig:fsa-exp3}. In addition to this best action, we also allow the actions $\mathrm{AskConfirm(i_u')}$, $\mathrm{AskRepeat}$ and $Do(Stop)$ to be executed at any state. The utility distribution is then expressed as: 
\begin{align}
&&& U(a_m', i_u', a_u', \mathit{carried}', \mathit{perceived}', \mathit{motion'}) = \nonumber \\
&&& \ \ \ \ \ \ \  \begin{cases} \ \theta_{\text{best}(i_u', \mathit{carried}, \mathit{perceived})} & \text{if } \ a_m = best(i_u', \mathit{carried}, \mathit{perceived}) \\ 
\ \theta_{\text{askConfirm}(i_u', \mathit{carried}, \mathit{perceived})} & \text{if } \ a_m = \mathrm{Confirm}(i_u') \\
\ \theta_{\text{askRepeat}(a_u, \mathit{motion'})} & \text{if } \ a_m = \mathrm{AskRepeat} \\
\ \theta_{\text{stop}(a_u,\mathit{motion'})} & \text{if } \ a_m = \mathrm{Stop} \\ 
\mbox{-}10 & \text{otherwise}
\end{cases} \label{eq:utilunstruct-exp3}
\end{align}

The factorisation of the utility model in Equation \eqref{eq:utilunstruct-exp3} results in 296 parameters. In total, the numbers of parameters associated with all factored models in this approach (including both univariate and multivariate parameters) equals 433.

\subsubsection*{Parameter estimation}
\index{parameter estimation}

The parameter estimation follows the Bayesian learning\index{Bayesian learning} procedure presented in Chapter \ref{chap:wozlearning}. The goal of the estimation process is to find the parameters for the domain models that provide the best fit for the Wizard-of-Oz data set\index{Wizard-of-Oz interaction}. The learning algorithm operates by cycling through the Wizard-of-Oz training data and updating the parameter distributions after each data point using Bayesian inference. The wizard is assumed to act rationally in most cases and select the action that yields the highest utility given the current dialogue state. 

It should be noted that, in contrast to the experiment presented in Section \ref{sec:wozlearning-experiments} (which concentrated on the estimation of a single model), the parameters to estimate in this setting include both the transition model and the utility model.  The learning algorithm is thus an instance of a joint optimisation problem, as the system must simultaneously optimise both models in order to find the parameters that provide the best fit for the Wizard-of-Oz dataset as a whole. 

The parameters of categorical distributions are all encoded by Dirichlet distributions initialised with weakly informative priors, while the utility parameters are encoded by uniform distributions with a support range of $[-10, 30]$.  The learning curve followed during the estimation process and the agreement results are provided in Section \ref{sec:learningcurve-exp3}.

% as well as the degree of agreement between the learned model and the ``gold standard'' actions selected by the wizard. 

\subsection{Approach 3: rule-structured model}
\index{dialogue management!hybrid approach to}
\index{probabilistic rule}

The final dialogue management approach developed in this experiment is couched in the formalism of probabilistic rules presented in this thesis.  

%approach relies here on the specification of probabilistic rules to express these models, and the parameter of these rules are optimised on the basis of the Wizard-of-Oz data set.

\subsubsection*{Domain modelling}
\index{domain modelling}

The factorisation of the domain models remains identical to the one shown in Figure \ref{fig:unstructuredmodel-exp3}.  As in the second approach, the models are decomposed into a task completion model, a user goal model, a user action model, and a utility model.  Action selection is again formalised as a search for the highest-utility action in the current state (assuming a planning horizon limited to the current time step).  However, in contrast to Approach 2, the internal structure of the domain models is here encoded through probabilistic rules instead of standard categorical distributions:
\begin{itemize}
\item The task completion model\index{task completion model} $P(\mathit{completed\mbox{-}task}\, | \, i_u, a_m)$ is encoded by one single deterministic rule that defines when the current intention $i_u$ is fulfilled by the system action $a_m$. 
\item The user goal model\index{user goal model} $P(i_u' \, | \, i_u, \mathit{completed\mbox{-}task}, \mathit{perceived'}, \mathit{carried'})$ consists of a small collection of rules that define the probability of the new user intention $i_u'$ given the dialogue context. One rule defines the prior probability of the user intentions $\mathrm{Move(\mathit{x})}$, four rules specify the prior probability of $\mathrm{PickUp(\mathit{x})}$, $\mathrm{WhatDoYouSee}$ and $\mathrm{DoYouSee(\mathit{x})}$ depending on the value of the $\mathit{perceived}'$ variable, one rule specifies the prior probability of $\mathrm{Release}$ depending on the value of the $\mathit{carried}'$ variable, and one final rule specifies the probability of the $\mathrm{Greeting}$ and $\mathrm{Closing}$ intentions depending on the last system move. 

\item The user action model\index{user action model} $P(a_u'\, | \, i_u', a_m)$ is formalised with two rules that map user intentions to their potential verbal realisations in terms of user dialogue acts. The first rule expresses the likelihood of various user dialogue acts depending on the user intention and last system action (in particular, whether the system uttered a clarification or confirmation request).  The second rule expresses the likelihood of the $\mathrm{Ask(Stop)}$ and $\mathrm{Disconfirm}$ actions during the execution of a physical movement.

\item Finally, the utility model\index{utility model} $U(a_m', i_u', a_u', \mathit{carried'}, \mathit{perceived'}, \mathit{motion'})$ is encoded with a total of ten utility rules.  The model essentially contains one rule for each family of possible system actions.  There is therefore one rule to define the relative utility of movement actions, one rule for grasping actions, one rule for the release action, two rules for the $\mathrm{Describe(\mathit{x})}$, $\mathrm{ConfirmDetection}$ and $\mathrm{DisconfirmDetection}$ actions, one rule to handle the  $\mathrm{Greet}$ and $\mathrm{Goodbye}$ actions, two rules for the clarification and confirmation requests, and one rule to stop the current movement. The last rule fixes an initial, negative utility for all actions to ensure that non-relevant actions have a utility lower than zero.  

\end{itemize}

The probability and utility models described by these rules include a total of 28 parameters, amongst which 15 are probability parameters and 13 utility parameters.  The complete specification of the domain is provided in Appendix \ref{chap:domainspecs} (Section \ref{sec:domainspecs-usereval}). 

\subsubsection*{Parameter estimation}
\index{parameter estimation}

The rule parameters are optimised on the basis of the collected Wizard-of-Oz data following the same procedure as for the second approach.  The parameters of probability rules are all encoded by Dirichlet distributions of varying dimensions (from 2 to 10) initialised with weakly informative priors, while the parameters of utility rules are encoded by uniform distributions on the support range $[-10, 30]$.  


\subsection{Learning curves}
\label{sec:learningcurve-exp3}
\index{learning curve}

Figure \ref{fig:curve-exp3} presents the learning curves for the three dialogue management approaches presented in the previous pages. The Wizard-of-Oz data set described in Section \ref{sec:wozcollection-exp3} was first divided into a training set of 9 interactions (summing up to 770 system turns) and a test set with one single interaction containing a total of 71 system turns. Based on this division, we measured the degree of agreement between the action selected by the system (on the basis of its model parameters) and the one selected by the wizard in the test set. This measurement is repeated at regular intervals during the parameter estimation process.  As the finite-state approach is entirely hand-crafted and does not include any parameters to optimise, its agreement with the wizard actions remains constant at all time steps. 

\begin{figure}[ht]
\vspace{3mm}\centering\includegraphics[scale=0.35]{imgs/curve-exp3.pdf}
\caption{Learning curves for the three dialogue management approaches on the Wizard-of-Oz data set.  The figure shows how the agreement between the best system action and the actual wizard action evolves as a function of the number of processed training samples.  The agreement is calculated on a separate test set of 71 system turns.}
\label{fig:curve-exp3}
\end{figure}


The learning curves in Figure \ref{fig:curve-exp3} illustrate the evolution of the degree of agreement\index{Wizard-of-Oz interaction!agreement measure} as the function of the number of training samples processed by the learning algorithm.  We can notice from the observation of the curves that both statistical approaches (Approach 2 and 3) do improve their agreement as they process more and more training samples, but do so at different learning speeds.  While the rule-structured model is able to converge to a high-quality dialogue policy after observing only a fraction of the training data, the traditional, factored statistical model converges at a slower rate due to its larger set of parameters and weaker generalisation capacity.\index{generalisation capacity}

The final agreement results obtained after processing the complete training set are shown in Table \ref{table:learning-exp3}.  As one can observe, the rule-structured approach is the one that is best able to imitate the conversational behaviour of the wizard. 

\begin{table}[ht]
\begin{center}
\begin{tabular}{|l|c|} \hline
\textit{Type of model} & \textit{Agreement with the wizard actions (in \%) } \\ \hline \hline
Finite state automaton & 43.66 \\ \hline
Factored statistical model & 54.93 \\ \hline
Rule-structured model & \textbf{71.83} \\ \hline
\end{tabular}
\end{center}
\vspace{-2mm}
\caption{Agreement results on the test test for the three approaches.}
\vspace{-2mm}
\label{table:learning-exp3}
\end{table}

It should be stressed once more that system actions that do not correspond to the ones selected by the wizard are not necessarily wrong or inappropriate, as they may reflect different but perfectly legitimate conversational strategies. The degree of agreement between the model and the wizard examples is, however, a good indication of the model ability to capture the dynamics of the interaction as well as the trade-offs of the action selection process. 

\section{User trials}
\index{user trial|see{user evaluation}}

In order to assess the relative performance of the three dialogue management approaches presented in the previous section, we conducted a total of $3 \times 37$ user trials (every participant carried out one separate interaction for each of the three approaches), which we now describe in detail. We start by laying out the experimental setup employed in the user trials, and then present and discuss the empirical results. 

\subsection{Experimental setup}

\subsubsection*{Participants}

The experiment was carried out with a total of 37 participants (16 males and 21 females).  The average age of the participants was 30.6 years (standard deviation: 7.8). The participants were recruited mainly amongst students and employees of the Department of Informatics at the University of Oslo. All participants were non-native speakers of English. To facilitate the recruitment of participants, a small monetary reward was offered to the participants upon the completion of the experiment. 

Before the start of the experiment, we briefed the participants about the interaction scenario and the physical and conversational capabilities of the robot. To give the participants a better sense of what the robot can understand (and what falls outside the scope of the dialogue domain), we provided each participant with an instruction sheet detailing the different kinds of movements the robot can perform, as well as a few examples of verbal instructions for each.

\subsubsection*{User expectations}
\index{user expectations}

The participants were asked to fill out a short survey about their age, gender, and their expectations regarding the robot verbal behaviour. The user expectations were surveyed with a set of six multiple-answer questions as shown in Figure \ref{fig:expectations}. The questions cover multiple aspects that may affect the interaction quality in the domain, such as the comprehension abilities of the robot (Q1), its ability to select appropriate reactions (Q2), its use of clarification requests (Q3), its ability to distinguish between noise and actual utterances (Q4 and Q5), and finally the overall naturalness of the interaction (Q6). One should note that the participants were told during the briefing phase to be aware of the technical limitations of the speech recogniser. The user responses must therefore be interpreted in the light of this cautionary statement. 


\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.5]{imgs/expectations.pdf}
\end{center} 
\caption{Prior user expectations about the robot's conversational abilities. The Y axis corresponds to the number of user responses (out of 37 participants). }
\label{fig:expectations}
\end{figure}

The first and most evident purpose of this preliminary survey is to allow us to compare the prior expectations of the users with their actual experiences during the interactions.  As shown by e.g.\ \cite{JokinenH06}, such contrastive analysis can yield interesting insights on the factors that empirically influence user satisfaction and their variability amongst different groups of users.  In addition, the survey also served as an indirect mean to direct the user attention towards the aspects of the robot behaviour that are of particular relevance for this study -- namely, the robot's choice of dialogue moves. 


\subsubsection*{Trials}

After completing the survey, each participant was requested to carry out three distinct dialogues (one for each dialogue management approach).  We shuffled the order in which the three approaches were tested before each trial to mitigate the effects of the ordering sequence on the results. The interactions indeed become increasingly efficient as the user gets more accustomed to the robot and the task to perform.  The average duration of the first dialogue amounts to 6:31 minutes, while the average duration of the second dialogue is reduced to 5:08 minutes, and the duration of the last dialogue goes down even further to 4:40.

The participants did not have a time limit to complete the task. Immediately after each dialogue, the participants were asked to fill out a small survey to determine how they subjectively perceived the interaction.  The survey was once more divided into six multiple-answer questions, as explained in the next section.

All interactions were captured on video, totalling about 12 hours of footage. In addition to the videos, the dialogue system also logged all user and system turns occurring in the interaction along with their associated dialogue state.


\subsection{Results}
\label{sec:results-exp3}

\subsubsection*{Objective metrics}
\index{interaction quality!objective metrics of}
In line with previous work on the evaluation of spoken dialogue systems \citep[see e.g.][ch.~6 for an overview]{2009Jokinen}, we defined a set of objective metrics aimed at measuring the quality and efficiency of the recorded dialogues.  The defined metrics and their empirical values for three approaches are shown in Figure \ref{fig:objective}.

Although many evaluation schemes include explicit measures of task completion in their metrics, we found such measures to be difficult to incorporate in our experimental setup, as the participants did not really have the possibility to end a task before its conclusion. Some dialogues had to be terminated before the full completion of the task but these cases were due to external causes such as hardware failures (e.g.\ system crash or physical collapse of the robot).
 
The metrics were calculated on the basis of the interaction logs produced by the dialogue system during the course of the dialogue. As the interaction logs are generated automatically, the counts of user turns correspond to the number of speech recognition results and do not necessarily correspond one-to-one to the actual user turns in the dialogue (due to spurious results caused by ambient noise). We can observe from Figure \ref{fig:objective} that the rule-structured model produces on average shorter and more efficient dialogues than the two baselines (metrics M6-M9).  In particular, on the basis on its parameter estimation on the Wizard-of-Oz training set, the rule-structured model systematically assigned higher utilities to confirmation requests (\utt{should I do X?}) in comparison to repetition requests (\utt{Sorry, could you repeat?}) and 
consequently never asked the user to repeat the last utterance (metric M1). The number of system turns is also notably shorter than for the finite-state and factored statistical model, reflecting the fact that the rule-structured model is better able to filter out spurious recognition results based on the current context (metric M8). 

\begin{figure}[p]
\begin{center}
\includegraphics[scale=0.5]{imgs/objective.pdf}
\end{center} 
\caption{Objective metrics extracted from the collected dialogues. The Y axis corresponds to the number of user responses (out of 37 participants). }
\label{fig:objective}
\end{figure}


\subsubsection*{Subjective metrics}
\index{interaction quality!subjective metrics of}

The subjective metrics reflect the answers to the six multiple-answer questions specified in the user survey. Each question allows five alternative answers on a scale ranked from worst to best.  The six questions and the responses they received from the participants are shown in Figure \ref{fig:subjective}. As one can observe, the responses to the survey do seem to indicate that the rule-structured model was perceived as leading to more effective and natural conversational behaviours. 


%\begin{enumerate}
%\item Did you feel that the robot correctly understood what you said?	\\
%\begin{footnotesize}\textit{Answers}: (1) Not at all \hspace{2mm} (2) Not very often \hspace{2mm}  (3) Half the time \hspace{2mm} (4) Most of the time \hspace{2mm} (5) Nearly always\end{footnotesize}

%\item Did you feel that the robot reacted appropriately to your instructions?	\\
%\begin{footnotesize}\textit{Answers}: (1) Not at all \hspace{2mm} (2) Not very often \hspace{2mm} (3) Half the time \hspace{2mm} (4) Most of the time \hspace{2mm} (5) Nearly always\end{footnotesize}

%\item Did you feel that the robot asked you to repeat or confirm your instructions ...\\	
%\begin{footnotesize}\textit{Answers}: (1) Very often \hspace{2mm} (2) Quite often \hspace{2mm} (3) Occasionally \hspace{2mm} (4) Rarely \hspace{2mm} (5) Never\end{footnotesize}

%\item Did you feel that the robot sometimes ignored when you were speaking?	\\
%\begin{footnotesize}\textit{Answers}: (1) Very often \hspace{2mm} (2) Quite often \hspace{2mm} (3) Occasionally \hspace{2mm} (4) Rarely \hspace{2mm} (5) Never\end{footnotesize}

%\item Did you feel that the robot sometimes thought you were talking when you were not?	\\
%\begin{footnotesize}\textit{Answers}: (1) Very often \hspace{2mm} (2) Quite often \hspace{2mm} (3) Occasionally \hspace{2mm} (4) Rarely \hspace{2mm} (5) Never\end{footnotesize}

%\begin{footnotesize}\item Did you feel that the interaction with the robot flowed in a pleasant and natural manner? \\
%\textit{Answers}: (1) Not at all \hspace{2mm} (2) Not very much \hspace{2mm} (3) More or less \hspace{2mm} (4) For the most part \hspace{2mm} (5) Definitely\end{footnotesize}
%\end{enumerate}


\begin{figure}[p]
\begin{center}
\includegraphics[scale=0.5]{imgs/subjective.pdf}
\end{center} 
\caption{Survey responses obtained from the participants in the user trial after each interaction. }
\label{fig:subjective}
\end{figure}

\subsubsection*{Summary of results}

Table \ref{table:results_exp3} summarises the results of both the objective and subjective metrics and specifies the statistical significance of each observed difference between measures.  The results from the subjective metrics are calculated by mapping the qualitative answers to a scale from 1 to 5 (1 corresponding to the worst behaviour and 5 to the best) and averaging the results over all participants. 

In order to analyse the statistical significance of the results obtained for each metric, the table indicates the $p$-values for two distinct test statistics:\footnote{A $p$-value corresponds to the probability of obtaining a test statistic at least as extreme as the one actually observed if we presuppose that the null hypothesis is true.  We can therefore reject the null hypothesis whenever the derived $p$-value is lower than a certain significance level $\alpha$ (often set to 0.05 or 0.01).}
\begin{itemize}
\item The first $p$-value is derived via the ANOVA procedure (Analysis of Variance) with one single factor corresponding to the dialogue management approach selected for the interaction. The null hypothesis  corresponds in this case to the assumption that the observed measures extracted with the three approaches (denoted below $A_1$, $A_2$ and $A_3$) are drawn from populations with the same mean:
\begin{equation}
H_0 : \mu_{A_1} = \mu_{A_2} = \mu_{A_3}
\end{equation}  

\item As we are more specifically interested in the difference between the rule-structured approach and the two baselines, we also calculated a second test statistic that compares for each metric the mean of the best approach against the second best approach. The test statistic is in this case derived using Bonferroni-corrected Student's paired $t$-tests.  The null hypothesis corresponds to the assumption that the observed measures for the best approach and the second-best  approach are drawn from populations with the same mean:
\begin{equation}
H_0 : \mu_{\mathit{best}} = \mu_{\mathit{second}} \ \  \ \ \text{where } \begin{cases}\mu_{\mathit{best}} = \max(\{\mu_{A_1}, \mu_{A_2}, \mu_{A_3}\}) \\ 
\mu_{\mathit{second}} = \max(\{\mu_{A_1}, \mu_{A_2}, \mu_{A_3}\} \setminus \mu_{\mathit{best}}) 
\end{cases}
\end{equation}

\end{itemize}

\renewcommand{\arraystretch}{1.2}

\begin{sidewaystable}
\begin{tabular}{|cp{110mm}c|r:r:r|p{16mm}|p{16mm}|} \hline
&\centering \multirow{2}{*}{Metrics} & & \multicolumn{3}{c|}{Approaches} & \multirow{2}{*}{\parbox{16mm}{$\phantom{0000000}$ $p$-values ANOVA }} & \multirow{2}{*}{\parbox{16mm}{$\phantom{0000000}$ $p$-values $1^{\text{st}}$ vs. $2^{\text{nd}}$ ($t$-test) }} \\ \cline{4-6} 
&&& \multicolumn{1}{p{14mm}:}{\centering\begin{footnotesize}Finite-state automata\end{footnotesize}} & \multicolumn{1}{p{14mm}:}{\centering\begin{footnotesize}Factored statistical model\end{footnotesize}} & \multicolumn{1}{p{14mm}|}{\centering\begin{footnotesize}Rule- structured model\end{footnotesize}} & & \\ \hline \hline
& \vspace{-2mm} \textbf{Objective metrics}:  && & & & & \\ 
M1.& Average number of repetition requests per dialogue && 18.68 & 12.24 & \textbf{0}$^{\mathbf{*}}$ & $9\!\times\!10^{-19}$ & $1\!\times\!10^{-16}$ \\
M2.& Average number of confirmation requests per dialogue &&9.16 & 10.32 & \textbf{5.78}$^{\mathbf{*}}$ & $1.7\!\times\!10^{-4}$& $0.001$ \\ 
M3.& Average number of repeated instructions per dialogue && 3.73 &
7.97 & \textbf{2.78}$^{\phantom{*}}$ & $3.8\!\times\!10^{-9}$ & $0.18$ \\ 
M4.& Average number of user rejections per dialogue &&\textbf{2.16} & 2.59 & 2.59$^{\phantom{*}}$ & $0.65$ & $0.33$ \\ 
M5.& Average number of physical movements per dialogue & & \textbf{26.68}
 & 29.89 & 27.08$^{\phantom{*}}$ & $0.13$ &  0.80 \\ 
M6.& Average number of (user and system) turns between movements &&3.63 & 3.10 & \textbf{2.54}$^{\mathbf{*}}$ & $4.\!\times\!10^{-4}$ & $2.2\!\times\!10^{-4}$ \\ 
M7.& Average number of user turns per dialogue && 78.95 & 77.30 & \textbf{69.14}$^{\phantom{*}}$ & 0.17 & 0.11 \\ 
M8.& Average number of system turns per dialogue && 57.27 & 54.59 & \textbf{35.11}$^{\mathbf{*}}$ & $4.4\!\times\!10^{-9}$ &  $5.6\!\times\!10^{-8}$\\ 
M9.& Average duration of each dialogue  (in minutes) \vspace{3mm} && 6:18 & 7:13 &\textbf{ 5:24}$^{\mathbf{*}}$ & $1.4\!\times\!10^{-4}$ & 0.02 \\ \hdashline
& \vspace{-2mm} \textbf{Subjective metrics}:  && & & & & \\ 
&\textit{``Did you feel that ...}  && & & & & \\ 
Q1.&\ \ \textit{... the robot correctly understood what you said?''}  && 3.32 & 2.92 &  \textbf{3.68} & $1.3\!\times\!10^{-4}$ & 0.03 \\
Q2.& \ \ \textit{.. the robot reacted appropriately to your instructions?''}   && 3.70 & 3.32 & \textbf{3.86}$^{\phantom{*}}$ & $7.6\!\times\!10^{-3}$ & 0.23 \\
Q3.&\ \ \textit{... the robot asked you to repeat or confirm your instructions?''}   & & 2.16 & 2.19 & \textbf{3.30}$^{\mathbf{*}}$ & $1.7\!\times\!10^{-9}$ & $4.7\!\times\!10^{-7}$ \\
Q4.&\ \ \textit{... the robot sometimes ignored when you were speaking?''}  & & 3.24 & 2.76 & \textbf{3.43}$^{\phantom{*}}$ & $6.7\!\times\!10^{-3}$ & 0.21 \\
Q5.&\ \ \textit{... the robot sometimes thought you were talking when you were not?''}  & & 3.43 & 3.14 & \textbf{4.41}$^{\mathbf{*}}$ & $3.4\!\times\!10^{-6}$ & $4.7\!\times\!10^{-5}$ \\
Q6.&\ \ \textit{... the interaction flowed in a pleasant and natural manner?''} \vspace{3mm}  & & 2.97 & 2.46 & \textbf{3.32} & $8.6\!\times\!10^{-4}$ & 0.03 \\ \hline
\end{tabular} \vspace{3mm}
\caption{Empirical results obtained for the user evaluation with a total of 37 participants, based on a set of 15 metrics (9 objective and 6 subjective). The $\mathbf{*}$ symbol indicates results that outperform the two other approaches with a level of statistical significance $\alpha = 0.05$ and Bonferroni correction. }
\label{table:results_exp3}
\end{sidewaystable}

Results for which a statistically significant difference with $\alpha = 0.05$  is found are decorated with a star sign. As one can observe from the table, 7 of the 15 metrics demonstrate statistically significant differences for the rule-structured model, while 6 metrics show higher means for the rule-structured model but without achieving full statistical significance, and 2 metrics yield better results for the finite-state approach, although both without statistical significance. 

It is also useful to contrast the results obtained from the survey with the initial user expectations, as shown in Table \ref{table:compexpectation-exp3}.  We can observe that for all six questions, the actual system behaviour of the rule-structured approach exceeded the initial expectations of the user. 

\begin{table}[ht]
\begin{center}
\begin{tabular}{|cp{50mm}c|r|r:r:r|p{0mm}} \cline{1-7}
& Survey questions: $\phantom{00000000}$ {\small\textit{``Did you feel that ...}} && \multicolumn{1}{p{17mm}|}{\footnotesize\centering Initial user expectations} & \multicolumn{1}{p{15mm}:}{\footnotesize\centering Finite-state automaton} & \multicolumn{1}{p{15mm}:}{\footnotesize\centering Factored statistical model} & \multicolumn{1}{p{15mm}|}{\footnotesize\centering Rule-structured model} & \\ \cline{1-7} 
\rule{0pt}{4ex}Q1.& {\small\textit{... the robot correctly understood what you said?''}}  &&  3.22 &  3.32 & 2.92  & \textbf{3.68}$^{\mathbf{*}}$ &   \\ %(0.025)
Q2.& {\small\textit{... the robot reacted appropriately to your instructions?''}}   && 3.35 &   3.70 & 3.32 & \textbf{3.86}$^{\mathbf{*}}$  & \\ %(0.004)
Q3.& {\small\textit{... the robot asked you to repeat or confirm your instruction?''}}   && 2.68 &2.16$\mathbf{\dagger}$  & 2.19$\mathbf{\dagger}$ & \textbf{3.30}$^{\mathbf{*}}$  & \\ %($4.6 \times 10^{-4}$)
Q4.& {\small\textit{... the robot sometimes ignored when you were speaking?''}}  && 2.89 &  3.24 & 2.76 & \textbf{3.43}$^{\mathbf{*}}$ &  \\ % (0.015)
Q5.& {\small\textit{... the robot sometimes thought you were talking when you were not?''}}  && 3.81 & 3.43 & 3.14$\mathbf{\dagger}$ & \textbf{4.41}$^{\mathbf{*}}$  & \\ %(0.001)
Q6.& {\small\textit{... the interaction flowed in a pleasant and natural manner?''}} \vspace{1mm}  && 2.92 &2.97 & 2.46$\mathbf{\dagger}$ &  \textbf{3.32}$^{\mathbf{*}}$  & \\ \cline{1-7} % (0.041)
\end{tabular} 
\end{center}
\caption{Comparison between the initial expectations and the actual performance of the rule-structured approach. The $\mathbf{*}$ symbol indicates better-than-expected performance with $\alpha = 0.05$, and the $\mathbf{\dagger}$ symbol worse-than-expected performance (also with $\alpha = 0.05$). \vspace{4mm} }
\label{table:compexpectation-exp3}
\end{table}

\subsection{Discussion}

\subsubsection*{Error analysis}

The empirical results derived from the user trials highlight the benefits of a hybrid modelling framework based on probabilistic rules compared to traditional approaches. Through a follow-up error analysis, we found this difference to be imputable to several factors, which we now describe.

Although the finite-state automaton functioned well in most trials, it often required multiple repetitions in order to reach sufficient confidence regarding the instruction to perform.  In comparison, the rule-structured model was able to accumulate evidence from several information sources (including the prior probabilities of the various instructions given the context) thanks to its explicit, probabilistic representation of the user intention.  This possibility to accumulate evidence and exploit contextual knowledge enables the rule-structured model to zero in on the right instruction faster and more efficiently than the hand-crafted model. One can observe this difference in the metrics M1 (number of repetition requests), M6 (number of turns per instructions) and Q3 (frequency of clarification requests) of the user trial. 

Another interesting element we found in our analysis was that the factored statistical model operated well in the most common conversational situations, but was frequently led astray when encountering events that had not been observed in the Wizard-of-Oz training set. In particular, although the user action model $P(a_u'\, | \, i_u', a_m)$ offered reasonable estimates for the most prevalent user intentions and dialogue acts, this was not the case for less common instructions which had few or no occurrences in the training set.  This disparity in the estimated distributions often resulted in the dialogue system getting stuck in unseen dialogue states and selecting suboptimal actions.  Although it was trained on the exact same data, the rule-structured approach did not suffer from this data sparsity in the same extent due to its more powerful generalisation abilities. This difference can be clearly noticed in the metrics M3 (number of repeated instructions), M9 (average duration), Q1 (quality of the robot's understanding) and Q2 (appropriateness of the robot's behaviour). 

The finite-state automaton and the factored statistical model also encountered difficulties in filtering out spurious speech recognition hypotheses, especially the ones produced by the robot's own noise when executing physical movements.  This often led the robot to request the user to repeat an instruction when no instruction was actually uttered.  The rule-structured model proved to be better at filtering out these undesired inputs.  The metrics M8 (number of system turns) and Q5 (ability of the robot to distinguish noise from speech) illustrate this discrepancy. 

Finally, it is worth noting that the rule-structured model systematically assigned higher utilities to confirmation requests (\utt{should I do X?}) than repetition requests (\utt{sorry, could you repeat?}) when faced with unclear user instructions, as evidenced by the metric 1. While this characteristic of the estimated parameters for the rule-structured model initially came as a surprise (since the wizard did use some repetition requests in the Wizard-of-Oz interactions, although admittedly much less than confirmations), it proved to be an excellent conversational strategy for the domain, as participants perceived these confirmation requests to be much more natural and informative than repetitions. By contrast, the two baseline approaches relied much more heavily on repetition requests to elicit the user instructions, a strategy that many users found irritating. 

\subsubsection*{Interpretation of the results}

The empirical results obtained from the user trials demonstrate that the hybrid modelling framework presented in this thesis is a viable and competitive alternative to classical dialogue management approaches.  Nevertheless, the results are subject to two cautionary remarks.  Most importantly, the results obtained for the two baselines are contingent on the particular design choices described in Section \ref{sec:approach1} and \ref{sec:approach2}.  Although these design choices are in our view relatively uncontroversial and do follow standard practices in the field of dialogue modelling, there is no doubt that these approaches could be further optimised through more elaborate estimation methods and parameter tuning. The intended purpose of the user evaluation is, however, \textit{not} to compare probabilistic rules with the ``best possible'' hand-crafted and statistical techniques, as such a characterisation would make little sense and remain impossible to prove in practice. Instead, the empirical results presented in the previous pages are best interpreted as a comparison between approaches featuring the same level of sophistication in their representation of the dialogue domain, but differing with one another along two dimensions:
\begin{itemize}
\item The finite-state automaton and the rule-structured model share the same domain knowledge and mappings between user instructions and corresponding system actions.  However, these two approaches diverge in their account of uncertainty: While the automaton relies on confidence thresholds, the rule-structured model employs probabilistic reasoning to infer the most likely instruction given the observed inputs, prior likelihoods and external context.
\item Analogously, the factored statistical model and the rule-structured model share the same conditional dependencies between variables, domain-specific constraints, and training data. However, they differ in the amount of internal domain structure introduced into the probabilistic model, and therefore in the numbers of parameters to estimate.
\end{itemize}

Furthermore, we must also highlight the fact that the results were obtained in a context of a dialogue domain that exhibits three properties: (1) high levels of noise and uncertainty, (2) a non-trivial relational structure and (3) a limited availability of in-domain training data.  Although these properties are common to many dialogue domains, one should be wary of extrapolating these results to domains that do not share these properties.  For instance, dialogue domains in which all state variables are fully observed and behave in a deterministic manner might be better addressed by purely symbolic techniques instead of adopting a hybrid approach. Similarly, dialogue domains that can draw on large amounts of training data (or can rely on a pre-existing user simulator)\footnote{The experiment presented in this chapter focused on the comparison between dialogue management models directly tuned from Wizard-of-Oz data. An interesting question for future work would be evaluate the practical effects of our modelling approach on empirical measures of dialogue quality when the parameters are optimised from user simulators such as the one detailed in Chapter \ref{chap:rllearning}.} might not have a direct need for the kind of expert domain knowledge incorporated in probabilistic rules.   In our experience, such cases constitute nevertheless the exception rather than the rule: Most dialogue domains \textit{do} have substantial levels of uncertainty to confront, a rich internal structure, and only limited access to domain-specific interaction data (if any at all). 


\section{Conclusion}

This chapter presented an extensive user evaluation of the hybrid dialogue modelling framework developed in this thesis.  The experiment was designed to compare a dialogue manager relying on probabilistic rules against two baselines that respectively correspond to a purely hand-crafted dialogue manager (expressed as a finite-state automaton) and a purely statistical dialogue manager (based on factored models). 

The experiment was again set up in a human--robot interaction domain.  The participants were asked to instruct the robot to locate a given object, pick it up and bring it back to the starting position.  Each of the 37 participants performed 3 dialogues (one for each dialogue management approach), thus amounting to $3 \times 37$ collected interactions.  After each interaction, the participants completed a survey with 6 questions inquiring about their perception of the dialogue. 

The three approaches were compared on the basis of 15 metrics with the aim to measure the quality and efficiency of the dialogues. The metrics comprised both the user answers to the survey as well as 9 objective metrics extracted from the interaction logs.   The empirical results indicate that the rule-structured approach outperforms the two baselines on both objective and subjective metrics, with statistically significant differences for 7 metrics. The outcome of this user evaluation provides further empirical support for the central argument developed in this thesis, namely that the formalism of probabilistic rules is well-suited to capture the structure of dialogue domains and can contribute to the development of more robust and conversationally competent dialogue systems through a combination of expert domain knowledge and probabilistic reasoning. 


% note about our approach: generalisation enable a better account of the data sparsity problem.  plus, the state dynamics are not lost since we perform belief update.  Finally, the appraoch can be seen as an initial boostrapping that can then be further refined through online reinforcement learning (Bayesian prior), as in Williams etc. also, we learn utilities, not a direct classification. Also: a user simulator is difficult for situated and open-ended environments.  we learn a POMDP policy by simulation
