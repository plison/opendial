\chapter{Concluding remarks}
\label{chap:conclusions}

We summarise in this final chapter the main contributions of this thesis.  We briefly review the theoretical underpinnings of our work, its practical implementation in the \opendial{} software toolkit and its empirical validation in three consecutive experimental studies.  In addition, we also enumerate a number of research themes that have not been directly addressed in this dissertation but constitute interesting and important topics for future work. 

\section{Summary of contributions}

The introduction chapter enumerated three research questions at the core of this thesis:
\begin{enumerate}
\item How can we integrate prior domain knowledge into probabilistic models of dialogue?
\item How can the parameters of these structured probabilistic models be estimated from data, using both supervised and reinforcement learning methods?  
\item What is the empirical effect of such modelling techniques on the quality and efficiency of verbal interactions?
\end{enumerate}

The first question was addressed in Chapter \ref{chap:rules}, which presented the formalism of probabilistic rules and its use in dialogue management.  We also showed in Chapters \ref{chap:wozlearning} and \ref{chap:rllearning} how to optimise the parameters of these rules through a process of Bayesian inference.  Finally, Chapter \ref{chap:user-evaluation} described an empirical evaluation of our modelling approach based on user trials in a human--robot interaction domain.

\subsubsection*{A new, hybrid approach to dialogue management}

The single most important scientific contribution made in this thesis is the development of a new theoretical framework for dialogue modelling and control based on the notion of \textit{probabilistic rules}.  As we have seen, probabilistic rules are defined as structured mappings between conditions and effects and can be used to describe both probability and utility distributions.  The conditions and effects are encoded as arbitrarily complex logical formulae and can make use of universal quantification as well as special-purpose operators to manipulate strings and sets of elements. At runtime, these probabilistic rules are instantiated as latent nodes in the graphical model representing the current dialogue state.  

The formalism allows the system designer to express the internal structure of a given dialogue domain in a compact set of rules. This modelling methodology offers two important benefits for dialogue management.  The first benefit is a substantial diminution of the number of parameters associated with the model, thereby allowing probabilistic rules to be optimised from limited amounts of interactions. This advantage is in our view crucial for many application domains, as high-quality, in-domain interaction data is usually scarce and difficult to acquire.

In addition, probabilistic rules are also well-suited to capture the particular constraints, conventions and assumptions that characterise a given dialogue domain.  Due to its combination of logical and probabilistic reasoning, we believe the formalism of probabilistic rules can serve as a useful bridge between symbolic and statistical approaches to dialogue management.  The presented framework is very general and can express a wide spectrum of models, from classical probabilistic models fully estimated from data to manually designed models associated with only a handful of parameters. The choice of model within this spectrum is therefore essentially a design decision dependent on the relative availabilities of training data and domain knowledge.


\subsubsection*{Estimation of rule parameters}

Chapters \ref{chap:wozlearning} and \ref{chap:rllearning} elaborated a number of new learning algorithms employed to automatically estimate the parameter of probabilistic rules from data. We argued that the general skeleton of the rules -- that is, the mapping between the conditions and their associated effects -- is best structured by the system designer, while the numeric probabilities or utilities mentioned in the rules are best estimated empirically on the basis of observed dialogue data. 

We developed in this thesis two complementary approaches to the estimation of these rule parameters:
\begin{enumerate}
\item The first approach is a supervised learning approach in which the rule parameters are estimated from collected Wizard-of-Oz data.  The learning algorithm gradually adjusts the rule parameters in order to best ``imitate'' the conversational behaviour of the wizard. This approach builds on the assumption that the wizard will tend to select the action that yields the highest utility in the current situation. 
\item The second approach is couched within a reinforcement learning framework.  In this setting, the learning agent is not provided with examples of expert behaviour but optimises the rule parameters through repeated interactions with a real or simulated user. We reviewed both model-based and model-free reinforcement learning methods to this task.  The model-based approach operates by estimating explicit models of the domain and subsequently selecting the system actions through online planning, while its model-free counterpart skips this intermediate estimation step in order to directly optimise a dialogue policy from experience. 
\end{enumerate}

Both approaches are grounded in Bayesian inference.  Each parameter is thus represented as a random variable and associated with an initial prior distribution over a (usually continuous) range of possible values. This distribution is then iteratively refined and narrowed down on the basis of the observed data points. The mathematical foundations for this process are directly derived from Bayes' rule. Assuming a set of parameters $\boldsymbol\theta$, some observed data $\mathcal{D}$ and a prior distribution $P(\boldsymbol\theta)$ over the parameter values, the posterior distribution $P(\boldsymbol\theta \, | \, \mathcal{D})$ of the parameters given the evidence provided by the data is calculated as:
\begin{equation}
P(\boldsymbol\theta \, | \, \mathcal{D}) = \eta \ P(\mathcal{D} \,; \boldsymbol\theta) \ P(\boldsymbol\theta)
\end{equation}
where $\eta$ is again a normalisation factor. Depending on the rule type and availability of expert knowledge, the parameter priors can be encoded through multiple families of distributions such as Dirichlet, Gaussian or Uniform distributions. 

\subsubsection*{Experimental studies}

We conducted a series of practical experiments to evaluate the practical viability of our modelling framework. All experiments took place in the context of a human--robot interaction domain and employed the Nao robot as development and testing platform. 

We carried out three distinct experimental studies.  The first experiment, described in Section \ref{sec:wozlearning-experiments}, sought to empirically analyse the learning performance of a rule-structured utility model compared to less structured representations based on either plain utility tables or linear models.  Using a small set of Wizard-of-Oz interactions as training data, the empirical results attested to the ability of the rule-structured model to quickly converge towards a high-quality policy and imitate the conversational behaviour of the wizard.

The second experiment, outlined in Section \ref{sec:rllearning-experiments}, demonstrated how to transfer this learning method into a reinforcement learning context.  The aim of the experiment was to automatically estimate the transition model of a human--robot interaction domain from repeated interactions with a user simulator bootstrapped from Wizard-of-Oz data. The experiment contrasted the learning performance of a rule-structured model against an unstructured baseline.  An online forward planning algorithm was used in both cases to select the next action to perform given all sources of uncertainty (i.e.\ state uncertainty, uncertain action effects, and model uncertainty). As in the first experiment, the results indicated that the rule-structured model could learn the interaction dynamics (and thus achieve higher rewards) much faster than its unstructured counterpart.

The third and final experiment, which we detailed in the previous chapter, evaluated the modelling framework in the context of a full-scale user study.   The objective of the experiment was to analyse the practical effects of the framework on measures of dialogue quality and efficiency (instead of focusing solely on the learning performance). The interaction scenario consisted of a simple task in which the participants instructed the robot to pick up a designated object and bring it to another location. Three dialogue management approaches were developed: one handcrafted finite-state automaton, one statistical dialogue manager with factored models, and one hybrid dialogue manager based on probabilistic rules.  Each participant carried out three separate dialogues (one for each approach).  The results obtained from this user evaluation showed that the rule-structured model outperformed the two baselines on a large range of objective and subjective metrics.

%  highlighted the positive role played by the domain knowledge incorporated in the rules. 


\subsubsection*{Development of the \opendial{} toolkit}

The final contribution of this thesis is the development of a new software toolkit for dialogue management called \opendial{}. The toolkit implements all the algorithms and data structures described in this dissertation and can be used to develop various types of dialogue systems through probabilistic rules. All domain- and task-specific knowledge is declaratively specified in the form of rule-structured models encoded in an XML format, thereby reducing the dialogue architecture to a small set of generic modules for state update and action selection.  We argued in Chapter \ref{chap:opendial} that the reliance on a shared   formalism (probabilistic rules) to represent multiple aspects of the dialogue domain offers several advantages in terms of transparency, flexibility and portability across domains and tasks in comparison to black-box architectures.

\section{Future work}

The approach presented in this thesis can be extended in several important directions, which we describe below. 

\subsubsection*{Reinforcement learning with live interactions}

As shown in Chapter \ref{chap:rllearning}, the parameters of probabilistic rules can be optimised from relatively small amounts of interactions.  This characteristic opens up the possibility of directly estimating rule-structured domain models from live interactions with human users, without relying on a user simulator.  However, although theoretically appealing, this possibility has not yet been validated experimentally and would certainly constitute an important direction for future work.

In practice, the conduct of such an experiment will require algorithmic improvements to the online planner to ensure that the action selection can meet real-time constraints. One possible solution, already suggested in Section \ref{rrlearning-exp22}, would be to blend model-based and model-free approaches and employ the model-free policy as a heuristic function to guide the forward search of the online planner.  Such mixture of offline and online planning has notably been investigated by \cite{RossC07}. 


\subsubsection*{Combination of supervised and reinforcement learning}

This thesis presented two distinct learning methods to the optimisation of rule parameters, respectively based on supervised and reinforcement learning.  Each method brings its own advantages and shortcomings: supervised learning is a simple and straightforward method for parameter estimation, but hinges on the availability of Wizard-of-Oz data and may not always lead to optimal decisions when the wizard behaviour is inconsistent or difficult to imitate.  Reinforcement learning, for its part, can directly optimise parameters without having access to ``gold standard'' examples, but can be slow and difficult to apply from scratch to live interactions.\footnote{In the absence of reasonable initial estimates for the domain models, the dialogue policy followed by the system runs the risk of being completely inadequate in the first round of interactions.  While such low starting point is not problematic in simulated interactions, it becomes a more serious issue when interacting with real users.} 

As already shown by \cite{williams2003}, \cite{rieser2006} and \cite{Henderson:2008}, supervised and reinforcement learning methods can be combined to get the best of both worlds. This is often achieved by deriving initial estimates for the domain models via supervised learning and then refining these estimates through reinforcement learning. Such combination would be facilitated in our framework by the fact that both learning techniques are already grounded in the same theoretical foundations, namely Bayesian inference. 

\subsubsection*{Joint optimisation of dialogue models}

As noted in Chapter \ref{chap:opendial}, probabilistic rules can be employed to structure many reasoning tasks and are not necessarily restricted to dialogue management. The dialogue system developed for our experiments indeed already applied probability and utility rules for natural language understanding and generation tasks, although the parameters of these rules were handcrafted. However, nothing prevents these parameters to also be optimised in parallel with the dialogue management models. As argued by e.g.\ \cite{Lemon:2011}, such joint optimisation of dialogue models brings several theoretical and practical advantages over isolated, component-by-component optimisations.

\subsubsection*{Incrementality and turn-taking}

An important aspect of dialogue architectures that has not been covered in this dissertation is the question of incremental processing. As argued by numerous researchers \citep[see e.g.\ ][]{schlangen2009general}, spoken dialogue systems should be able to process their inputs in a continuous manner, without having to wait for the end of an utterance to trigger its interpretation.  Extending the state update algorithm outlined in Section \ref{sec:processing-workflow} to operate in an incremental manner would certainly constitute an important topic for future work.\footnote{One suggestion would be to explicitly distinguish between the \textit{insertion} of a new variable in the dialogue state and its \textit{commitment}, as in the incremental units model of \cite{schlangen2009general}.  The pruning of the dialogue state would hence only be triggered when all its variables are committed.}

Similarly, the turn-taking functionalities integrated in the \opendial{} toolkit are still rudimentary and would certainly benefit from a more principled account of the conversational floor to handle common dialogue phenomena such as barge-ins and fragmentary utterances. 

\subsubsection*{Scaling up to more complex domains}

Last but not least, the approach presented in this thesis should be evaluated in the context of larger and more complex dialogue domains. While the human--robot interaction domain employed for the experiments presented a number of non-trivial challenges to address (notably regarding the omnipresence of speech recognition errors and the necessity to physically perceive and act upon a real physical environment), it remained restricted to a limited size. Scaling up the modelling framework to more complex domains -- i.e.\ with larger state--action spaces and more elaborate interaction dynamics -- would allow us to cast a new light on the expressive power of the formalism and its suitability in practical dialogue applications. 

