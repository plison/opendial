\chapter{Concluding remarks}
\label{chap:conclusions}

We summarise in this final chapter the main contributions of this thesis.  We briefly review the theoretical underpinnings of our work, its practical implementation in the \opendial software toolkit and its empirical validation in various experimental studies.  In addition, we also enumerate a number of research themes that have not been directly addressed in this dissertation but would constitute interesting and important topics for future work. 

\section{Summary of contributions}

The introduction chapter enumerated three research questions at the core of this thesis:
\begin{enumerate}
\item How can we integrate prior domain knowledge into probabilistic models of dialogue?
\item How can the parameters of these structured probabilistic models be estimated from data, using both supervised and reinforcement learning methods?  
\item What is the empirical effect of such modelling techniques on the quality and efficiency of verbal interactions?
\end{enumerate}

The first question was addressed in Chapter \ref{chap:rules}, which presents the formalism of probabilistic rules and its use in dialogue management.  We also showed in Chapters \ref{chap:wozlearning} and \ref{chap:rllearning} how to optimise the parameters of these rules through a process of Bayesian inference.  Finally, Chapter \ref{chap:user-evaluation} presented an extensive empirical evaluation of our modelling approach based on user trials in a human--robot interaction domain.

\subsubsection*{A new, hybrid approach to dialogue management}

The single most important scientific contribution made in this thesis is (in our view) the development of a new theoretical framework for dialogue modelling and control based on the notion of \textit{probabilistic rules}.  As we have seen, probabilistic rules are defined as structured mappings between conditions and effects and can be used to describe both probability and utility distributions.  The conditions and effects are encoded as arbitrarily complex logical formulae and can make use of universal quantification, as well as special-purpose operations to easily manipulate strings and sets of elements. At runtime, these probabilistic rules are instantiated as latent nodes in the graphical model representing the current dialogue state.  

The formalism allows the system designer to express the internal structure of a given dialogue domain in a compact set of rules. This modelling methodology offers two important benefits for dialogue management.  The first benefit is a substantial diminution of the number of parameters associated with the model, thereby allowing probabilistic rules to be optimised from limited amounts of interactions. This advantage is in our view crucial for many important application domains, as high-quality, in-domain interaction data is usually scarce and difficult to acquire.

In addition, probabilistic rules are also well-suited to capture the particular constraints, conventions and assumptions that characterise a given dialogue domain.  Due to its combination of logical and probabilistic reasoning, we believe the formalism of probabilistic rules can serve as a useful bridge between symbolic and statistical approaches to dialogue management.  The presented framework is very general and can express a wide spectrum of models, from classical probabilistic models fully estimated from data to manually designed models associated with only a handful of parameters. The choice of model within this spectrum is therefore essentially a design decision dependent on the relative availabilities of training data and domain knowledge.


\subsubsection*{Estimation of rule parameters}

Chapters \ref{chap:wozlearning} and \ref{chap:rllearning} elaborated a number of new learning algorithms employed to automatically estimate the parameter of probabilistic rules from data. The key hypothesis shared by all algorithms is that the general skeleton of the rules -- that is, the mapping between the conditions and their associated effects -- is best structured by the system designer, while the numeric probabilities or utilities mentioned in the rules are best estimated empirically on the basis of observed dialogue data. 

We developed in this thesis two complementary approaches to the estimation of these rule parameters, which differ in the type of learning strategy employed in the estimation process:
\begin{enumerate}
\item The first approach is a supervised learning approach in which the rule parameters are estimated from collected Wizard-of-Oz data.  This approach builds on the assumption that the wizard is rational and will tend to select in most cases the action that yields the highest utility in the current situation. The parameters are therefore gradually adjusted in order to best ``imitate'' the conversational behaviour of the wizard.
\item The second approach is couched in reinforcement learning.  In this setting, the learning agent is not provided with examples of expert behaviour but optimise its parameters through repeated interactions with a real or simulated user. We outlined both model-based and model-free reinforcement learning methods to this task.  The model-based method seeks to estimate explicit models of the domain and subsequently select the system actions through online planning, while the model-free method skips this intermediate estimation step in order to directly optimise a dialogue policy from the collected interactions.
\end{enumerate}

Both approaches are grounded in Bayesian inference.  Each parameter is thus represented as a random variable and associated with an initial prior distribution over its (usually continuous) range of possible values. This distribution is then iteratively refined and narrowed down on the basis of the observed data points. The mathematical foundations for this process are directly derived from the well-known Bayes' rule. Assuming a set of parameters $\boldsymbol\theta$, some observed data $\mathcal{D}$ and a prior distribution $P(\boldsymbol\theta)$ over the parameter values, the posterior distribution $P(\boldsymbol\theta \, | \, \mathcal{D})$ of the parameters given the evidence provided by the data is calculated as:
\begin{equation}
P(\boldsymbol\theta \, | \, \mathcal{D}) = \eta \ P(\mathcal{D} \,; \boldsymbol\theta) \ P(\boldsymbol\theta)
\end{equation}
where $\eta$ is again a normalisation factor. Depending on the rule type and availability of expert knowledge, the parameter priors can be encoded through multiple families of distributions such as Dirichlet, Gaussian or Uniform distributions. 

\subsubsection*{Experimental studies}

We conducted a series of practical experiments to evaluate the practical viability of our modelling framework. All experiments took place in the context of a human-robot interaction domain and used the Nao robot as development and testing platform. 

We carried out three distinct experimental studies.  The first experiment, described in Section \ref{sec:wozlearning-experiments}, sought to empirically analyse the learning performance of a rule-structured utility model compared to less structured representations based on either plain utility tables or linear models.  Using a small amount of Wizard-of-Oz interactions as training data, the empirical results attested to the ability of the rule-structured model to quickly converge towards a high-quality policy and imitate the conversational behaviour of the wizard.

The second experiment, outlined in Section \ref{sec:rllearning-experiments}, demonstrated how to transfer this learning method in a reinforcement learning setting.  The aim of the experiment was to automatically estimate the transition model of a human--robot interaction domain from repeated interactions with a user simulator bootstrapped from Wizard-of-Oz data. The experiment contrasted the learning performance of a rule-structured model against an unstructured baseline.  An online forward planning algorithm was used in both cases to select the best action to perform given all sources of uncertainty (i.e.\ state uncertainty, uncertain action effects, and model uncertainty). As in the first experiment, the results showed that the rule-structured model could learn the interaction dynamics (and thus achieve higher rewards) much faster than its unstructured counterpart.

The third and final experiment, which we detailed in the previous chapter, evaluated the modelling framework in the context of a full-scale user study.  The objective was here to analyse the practical effects of the framework on the dialogue quality and efficiency (instead of focusing solely on the learning performance). Three dialogue management approaches were developed for this purpose: one handcrafted finite-state automaton, one statistical dialogue manager with factored models, and one hybrid dialogue manager based on probabilistic rules.  The participants had to instruct the robot to complete a simple task and carried out three separate dialogues (one for each approach).  The results obtained from this user evaluation demonstrates that the rule-structured model outperformed the two baselines on a large range of objective and subjective metrics.

%  highlighted the positive role played by the domain knowledge incorporated in the rules. 


\subsubsection*{Development of the \opendial toolkit}

The final contribution of this thesis is the development of a new software toolkit for dialogue management called \opendial. The toolkit implements all the algorithms and data structures described in this dissertation and can be used to develop various types of dialogue systems through probabilistic rules. All domain- and task-specific knowledge is declaratively specified in the form of rule-structured models encoded in an XML format, thereby reducing the dialogue architecture to a small set of generic modules for state update and action selection.  We argued in Chapter \ref{chap:opendial} that the reliance on a shared   formalism (probabilistic rules) to represent multiple aspects of the dialogue domain offers several advantages in terms of transparency, flexibility and portability across domains and tasks in comparison to black-box architectures.

\section{Future work}

The approach presented in this thesis can be extended in several important directions, which we describe below. 

\subsubsection*{Reinforcement learning with live interactions}

As shown in Chapter \ref{chap:rllearning}, the parameters of probabilistic rules can be optimised from relatively small amounts of interactions.  This characteristic opens up the possibility of directly estimating rule-structured domain models from live interactions with human users, without relying on a user simulator.  Although theoretically appealing, this possibility has however not yet been validated experimentally. Such an investigation would most likely require several technical improvements to the planning algorithm to ensure that the action selection can meet real-time constraints. One interesting direction for future work, already suggested in Section \ref{rrlearning-exp22}, would be to blend model-based and model-free reinforcement learning approaches and employ the model-free policy as a heuristic function to guide the search of the model-based online planner.  Such mixture of offline and online planning has notably been proposed by \cite{RossC07}. 


\subsubsection*{Combination of supervised and reinforcement learning}

This thesis presented the supervised and reinforcement learning approaches to the optimisation of rule parameters as distinct and separate learning methods. Each approach brings its own advantages and shortcomings: supervised learning is a simple and straightforward method for parameter estimation, but hinges on the availability of Wizard-of-Oz data and may not always lead to optimal decisions when the wizard behaviour is inconsistent or difficult to imitate.  Reinforcement learning, for its part, can directly optimise parameters without having access to ``gold standard'' examples, but is difficult to apply from scratch to live interactions.\footnote{In the absence of reasonable initial estimates for the domain models, the dialogue policy followed by the system runs the risk of being completely inadequate in the first round of interactions.  While such low starting point is not problematic in simulated interactions, it becomes a more serious issue when interacting with real users.} 

As already argued by \cite{williams2003,rieser2006,Henderson:2008}, supervised and reinforcement learning methods can be combined together. This is often achieved by first calculating some initial estimates for the domain models via supervised learning and then refining these estimates through reinforcement learning.  As both learning techniques rely in our framework on Bayesian inference for parameter estimation, such a combination of approaches could yield interesting and promising results. 


\subsubsection*{Joint optimisation of dialogue models}

\note{joint optimisation of models}

\subsubsection*{Incrementality and turn-taking}

\subsubsection*{Scaling up to more complex domains}

\note{more complex domains, with more variables and more complex dynamics}

\note{find ways to optimise performance of \opendial}


