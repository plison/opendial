\chapter{Concluding remarks}
\label{chap:conclusions}

We summarise in this final chapter the main contributions of this thesis.  We briefly review the theoretical underpinnings of our work, its practical implementation in the \opendial{} software toolkit and its empirical validation in three consecutive experimental studies.  In addition, we also enumerate a number of research themes that have not been directly addressed in this dissertation but constitute interesting and important topics for possible follow-up work. 

\section{Summary of contributions}

The introductory chapter listed three research questions at the core of this thesis:
\begin{enumerate}
\item How can we integrate prior domain knowledge into probabilistic models of dialogue?
\item How can the parameters of these structured probabilistic models be estimated from data, using both supervised and reinforcement learning methods?  
\item What is the empirical effect of such modelling techniques on the quality and efficiency of verbal interactions?
\end{enumerate}

The first question was addressed in Chapter \ref{chap:rules}, which presented the formalism of probabilistic rules and its application to dialogue management.  We also showed in Chapters \ref{chap:wozlearning} and \ref{chap:rllearning} how to optimise the parameters of these rules through a process of Bayesian inference.  Finally, Chapter \ref{chap:user-evaluation} described an empirical evaluation of our modelling approach based on user trials in a human--robot interaction domain.

%Chapter \ref{chap:opendial} discussed the design of the \opendial{} toolkit and its integration in a full-fledged spoken dialogue system.

\subsubsection*{A new, hybrid approach to dialogue management}
\index{dialogue management!hybrid approach to}

The single most important scientific contribution made in this thesis is (in our view) the development of a new theoretical framework for dialogue modelling and control based on the notion of \textit{probabilistic rules}.\index{probabilistic rule}  As we have seen, probabilistic rules are defined as structured mappings between conditions and effects and can encode both probability and utility distributions.  The conditions and effects are encoded as arbitrarily complex logical formulae and can make use of universal quantification as well as special-purpose operators to manipulate strings and lists of elements. At runtime, these probabilistic rules are instantiated as latent nodes in the graphical model representing the current dialogue state.  

The formalism allows the system designer to express the internal structure of a given dialogue domain in a compact set of rules. This modelling methodology offers two important benefits for dialogue management.  The first benefit is a substantial reduction of the number of parameters associated with the model, thereby allowing probabilistic rules to be optimised from limited amounts of interactions. This advantage is in our opinion crucial for many application domains, as high-quality, in-domain interaction data is typically scarce and difficult to acquire.

In addition, probabilistic rules are also well-suited to capture the particular constraints, conventions and assumptions that characterise a given dialogue domain.  Due to its combination of logical and probabilistic inference, we believe the formalism of probabilistic rules can serve as a useful bridge between symbolic and statistical approaches to dialogue management.  The presented framework is very general and can express a wide spectrum of models, from classical probabilistic models fully estimated from data to manually designed models associated with only a handful of parameters. The choice of model within this spectrum boils down to a design decision dependent on the relative availabilities of training data and domain knowledge.


\subsubsection*{Estimation of rule parameters}
\index{rule parameters!estimation of}
Chapters \ref{chap:wozlearning} and \ref{chap:rllearning} elaborated a number of specialised learning algorithms employed to automatically estimate the parameter of probabilistic rules from data. We argued that the general skeleton of the rules -- that is, the mapping between the conditions and their associated effects -- is best structured by the system designer, while the numeric probabilities or utilities mentioned in the rules are best estimated empirically on the basis of observed dialogue data. 

We developed and discussed two complementary approaches to the statistical estimation of these rule parameters:
\begin{enumerate}
\item The first approach is a supervised learning\index{rule parameters!supervised learning of} approach in which the rule parameters are estimated from collected Wizard-of-Oz data.  In this approach, the learning algorithm gradually adjusts the rule parameters in order to best imitate the conversational behaviour of the wizard. This approach builds on the assumption that the wizard will tend in most cases to select the action that yields the highest utility in the current situation. 
\item The second approach is couched in a reinforcement learning framework.\index{rule parameters!reinforcement learning of}  In this setting, the learning agent is not provided with examples of expert behaviour but optimises the rule parameters through repeated interactions with a real or simulated user. We reviewed both model-based and model-free reinforcement learning methods to this task.  The model-based approach operates by estimating explicit models of the domain and subsequently selecting the system actions through online forward planning\index{online planning}, while the model-free approach skips this intermediate estimation step in order to directly optimise a dialogue policy from interaction experience. 
\end{enumerate}

Both approaches are grounded in Bayesian inference.\index{Bayesian learning}  Each parameter is represented as a random variable and associated with an initial prior distribution\index{parameter priors} over a (usually continuous) range of possible values. This distribution is iteratively refined and narrowed down on the basis of the observed data points. The mathematical foundations for this process are directly derived from Bayes' rule. Assuming a set of parameters $\boldsymbol\theta$, some observed data $\mathcal{D}$ and a prior distribution $P(\boldsymbol\theta)$ over the parameter values, the posterior distribution $P(\boldsymbol\theta \, | \, \mathcal{D})$ of the parameters given the evidence provided by the data is calculated as:
\begin{equation}
P(\boldsymbol\theta \, | \, \mathcal{D}) = \eta \ P(\mathcal{D} \,; \boldsymbol\theta) \ P(\boldsymbol\theta)
\end{equation}
where $\eta$ is a normalisation factor. Depending on the rule type and availability of expert knowledge, the parameter priors can be encoded through multiple families of distributions such as Dirichlet, Gaussian or uniform distributions. 

\subsubsection*{Experimental studies}

We conducted a series of practical experiments to evaluate the practical viability of our modelling framework. All experiments took place in the context of a human--robot interaction domain and employed the Nao robot as our development and testing platform. 

We carried out three distinct experimental studies.  The first experiment, described in Section \ref{sec:wozlearning-experiments}, sought to empirically analyse the learning performance of a rule-structured utility model\index{rule-structured model!learning performance of} compared to less structured representations based on either plain utility tables or linear models.  Using a small set of Wizard-of-Oz interactions\index{Wizard-of-Oz interaction} as training data, the empirical results attested to the ability of the rule-structured model to quickly converge towards a high-quality policy and imitate the dialogue behaviour of the wizard.

The second experiment, outlined in Section \ref{sec:rllearning-experiments}, demonstrated how to transfer this learning method into a reinforcement learning context.  The aim of the experiment was to automatically estimate the transition model of a human--robot interaction domain\index{human--robot interaction} from repeated interactions with a user simulator bootstrapped from Wizard-of-Oz data. The experiment contrasted the learning performance of a rule-structured model with an classical factored baseline.  An online forward planning algorithm was used in both cases to select the next action to perform given all sources of uncertainty (i.e.\ state uncertainty, uncertain action effects, and model uncertainty). As in the first experiment, the results indicated that the rule-structured model could learn the interaction dynamics (and thus achieve higher rewards) much faster than its unstructured counterpart.

The third and final experiment, which we detailed in the previous chapter, evaluated the modelling framework through a full-scale user study.\index{user evaluation}   Instead of focusing on the learning performance, the objective of the experiment was to analyse the practical effects of the framework on measures of dialogue quality and efficiency. The interaction scenario consisted of a simple task in which the participants instructed the robot to pick up a designated object and bring it to another location. Three dialogue management approaches were developed: one hand-crafted finite-state automaton, one statistical dialogue manager with factored models, and one hybrid dialogue manager based on probabilistic rules.  Each participant carried out three separate dialogues (one for each approach).  The results obtained from this user evaluation showed that the rule-structured model outperformed the two baselines on a large range of objective and subjective metrics.

%  highlighted the positive role played by the domain knowledge incorporated in the rules. 


\subsubsection*{Development of the \opendial{} toolkit}
\index{openDial@\opendial{}}

The final contribution of this thesis is the development of a new software toolkit for dialogue management called \opendial{}. The toolkit implements all the algorithms and data structures described in this dissertation and can be used to develop various types of dialogue systems through probabilistic rules. All domain- and task-specific knowledge is declaratively specified in the form of rule-structured models encoded in an XML format, thereby reducing the dialogue architecture to a small set of generic functions for state update and action selection.  We argued in Chapter \ref{chap:opendial} that the reliance on a shared, mathematically principled modelling framework (probabilistic rules) to represent multiple aspects of the dialogue domain offers several advantages in terms of transparency, flexibility and portability across domains in comparison to black-box architectures.

\section{Future work}

The approach presented in this thesis can be extended in several important directions, which we describe below. 

\subsubsection*{Reinforcement learning with live interactions}
\index{reinforcement learning!with live interactions}

As demonstrated in Chapter \ref{chap:rllearning}, the parameters of probabilistic rules can be optimised from relatively small amounts of interactions.  This characteristic opens up the possibility of directly estimating rule-structured domain models from live interactions with human users, without relying on a user simulator.  However, although theoretically appealing, this possibility has not yet been validated experimentally and would certainly constitute an important direction for future work.

In practice, the conduct of such an experiment will require algorithmic improvements to the online planning algorithm in order to enhance its real-time performance. One possible solution, already suggested in Section \ref{rrlearning-exp22}, would be to blend model-based and model-free approaches and employ the model-free policy as a heuristic function to guide the forward search of the online planner.  Such a mixture of offline and online planning has notably been investigated by \cite{RossC07}, and has shown promising results.


\subsubsection*{Combination of supervised and reinforcement learning}

This thesis presented two distinct learning methods to the optimisation of rule parameters, respectively based on supervised and reinforcement learning.  Each method brings its own advantages and shortcomings: supervised learning is a simple and straightforward method for parameter estimation, but hinges on the availability of Wizard-of-Oz data and may not always lead to optimal decisions when the wizard behaviour is inconsistent or difficult to reproduce.  Reinforcement learning, for its part, can directly optimise parameters without having access to ``gold standard'' examples, but can be slow and difficult to apply from scratch to live interactions.\footnote{In the absence of reasonable initial estimates for the domain models, the dialogue policy followed by the system runs the risk of being completely inadequate in the first round of interactions.  While such low starting point is not problematic in simulated interactions, it becomes a more serious issue when interacting with real users.} 

As already shown by \cite{williams2003}, \cite{rieser2006} and \cite{Henderson:2008}, supervised and reinforcement learning methods can be combined with one another in order to get the best of both worlds. This is often achieved by deriving initial estimates for the domain models via supervised learning and then refining these estimates through reinforcement learning. Such a combination of approaches would be facilitated in our framework by the fact that both learning techniques are already grounded in the same theoretical foundations, namely Bayesian inference. 

\subsubsection*{Joint optimisation of dialogue models}
\index{joint optimisation}

As noted in Chapter \ref{chap:opendial}, probabilistic rules can be employed to structure many reasoning tasks and are not necessarily restricted to dialogue management. The dialogue system developed for our experiments indeed already relied on probability and utility rules for natural language understanding and generation tasks, although the parameters of these rules were hand-crafted. However, nothing prevents these parameters from also being estimated in parallel with the dialogue management models in order to jointly optimise the end-to-end behaviour of the dialogue agent.  As argued by e.g.\ \cite{Lemon:2011}, such joint optimisation of dialogue models brings several theoretical and practical advantages over isolated, component-by-component optimisations.

\subsubsection*{Incrementality and turn-taking}
\index{incremental processing}
\index{incrementality|see{incremental processing}}

An important aspect of dialogue architectures that has not been covered in this dissertation is the question of incremental processing. As argued by several researchers \citep[see for instance][]{schlangen2009general}, conversationally competent dialogue systems should be able to process their inputs in a continuous manner, without having to wait for the end of an utterance to trigger the interpretation process.  Extending the state update algorithm outlined in Section \ref{sec:processing-workflow} to operate in an incremental manner would certainly constitute an important topic for future work.\footnote{One suggestion would be to explicitly distinguish between the \textit{insertion} of a new variable in the dialogue state and its \textit{commitment}, as in the incremental units model of \cite{schlangen2009general}.  The pruning of the dialogue state would hence only be triggered when all its variables are committed.}

Similarly, the turn-taking \index{turn-taking} functionalities integrated in the \opendial{} toolkit are still rudimentary and would certainly benefit from a more principled account of the conversational floor in order to handle common dialogue phenomena such as interruptions and fragmented utterances. 

\subsubsection*{Scaling up to more complex domains}

Last but not least, the approach presented in this thesis should be evaluated in the context of larger and more realistic dialogue domains. While the human--robot interaction scenarios employed for the experiments presented a number of non-trivial challenges to address (notably regarding the high proportion of speech recognition errors and the necessity to perceive and act upon a real physical environment), they remained nevertheless restricted to a limited size. Scaling up the modelling framework to more complex domains with larger state--action spaces and more elaborate dialogue dynamics would allow us to cast a new light on the expressive power of the formalism and its suitability for practical dialogue applications. 

