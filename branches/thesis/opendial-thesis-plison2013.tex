\chapter{Implementation}
\label{chap:opendial}

This chapter outlines the most important features of the \opendial toolkit. \opendial\footnote{The name of the toolkit was chosen because of the open design that characterises the framework, and more particularly its extensible, domain-independent architecture with a declarative domain specification, its focus on open-ended interactions, and its release as open source software.}  is a Java-based software toolkit developed to construct and evaluate dialogue systems based on probabilistic rules. The toolkit aims to be fully generic and domain-independent, since all domain-specific knowledge is captured in the declarative specification of the dialogue domain.

The toolkit implements all the data structures and algorithms detailed in the present thesis and served as a experimental platform to carry out the empirical studies presented in Chapters \ref{chap:wozlearning}, \ref{chap:rllearning} and \ref{chap:user-evaluation}. Dedicated components have been developed to interface \opendial with the Nao robotic platform and adapt the architecture to human-robot interaction settings.

The chapter is divided in two sections.  The first section focuses on the design of the \opendial toolkit and describes its general workflow, the specification of dialogue domains in XML, and the concrete implementation of the algorithms employed for approximate inference, sampling and forward planning. We also briefly compare the toolkit to other available architectures and discuss its current limitations.  The second section explains how the toolkit is integrated and extended into a full-fledged dialogue system for human-robot interaction. The section describes the system architecture (built on top of \opendial$\!\!$) as well as the individual components developed to control the verbal and non-verbal behaviour of the robot.  

\section{Toolkit design}
\label{sec:genarchitecture}

\subsection{Generalities}

The \opendial toolkit relies on a blackboard, event-driven architecture. Blackboard architectures are widely used in spoken dialogue systems for their ability to handle flexible workflows where multiple modules ``cooperate'' to  interpret the user inputs, maintain a representation of the dialogue state, and decide on the best actions to perform. In spoken dialogue systems, the blackboard corresponds to the system state, while the attached modules represent specific processing tasks such as speech recognition and understanding, dialogue management, natural language generation and speech synthesis. 

%Information-state approaches are notably based on such architecture. 

\subsubsection*{System workflow}

The modules read and write to the dialogue state in an event-driven manner. After each change, the dialogue state sends an event message to all its attached modules to inform them of the state update. When appropriate, the modules can react to this event by subsequently modifying the dialogue state and generating new updates.  The process continues until no further updates are possible, leading to the stabilisation of the dialogue state.\footnote{Recall that a model can only be applied once per update to avoid infinite cycles (cf. Algorithm \ref{algo:stateupdate}).}   The \opendial toolkit allows system modules to run in parallel, using Java multi-threading. The possibility to execute modules in parallel is particularly useful in dialogue systems, as the agent must be able to react to new inputs and contextual changes occurring at any time -- even while other modules are still busy processing a previous update.  Many of the algorithms developed in the toolkit -- such as planning and probabilistic inference -- also operate in anytime mode, which implies that they can be gracefully interrupted and deliver their outputs at any time.

The implementation of \opendial is currently optimised to run on a single platform.\footnote{System modules can however remotely connect to external resources on the robotic platform to perform tasks related to robot perception and motor control, as explained in Section \ref{sec:system-integration}.} Should such need arise, the framework could however be extended to run on multiple platforms, as the blackboard paradigm generally lends itself well to distributed architectures \citep{Corkill:1988}.

\subsubsection*{Types of modules}

The modules in the \opendial toolkit are divided in two distinct types: \begin{enumerate}
\item  The first type of module is the rule-structured model already outlined in Section \ref{sec:processing-workflow}.  A rule-structured model is simply defined as a collection of probabilistic rules together with a list of trigger variables.  The update of (at least) one of these trigger variables in the dialogue state results in the instantiation of the probabilistic rules in the dialogue state. 
\item The second type of module corresponds to external components such as the speech recogniser, text-to-speech engine and processes for robot perception and control. Given the blackboard architecture of the toolkit, modules can be easily plugged in and out of the system without affecting the rest of the processing pipeline. External components operate very similarly to rule-structured models, through reading and updating the dialogue state when relevant changes are detected. 

\end{enumerate}

In the experiments carried out in this thesis, we found it useful to encode not only dialogue management models with the help of probabilistic rules, but also tasks related to natural language understanding and generation. As argued in \cite{lison-semdial2012}, the expressive power of probabilistic rules allows them to capture the structure of many dialogue processing tasks, not necessarily confined to dialogue management. The dialogue domain designed for the experiments in Chapter \ref{chap:user-evaluation} include for instance a total of six models: one dialogue act classification model, (triggered by the user utterance $u_u$), one action utility model (triggered by the user dialogue act $a_u$), three probability models to predict the effects of the system action on the context, the user intention and the next user action (all triggered by the system action $a_m$), and a generation model (triggered by the system action $a_m$). The parameters of the NLU and NLG models are however hand-crafted since they do not directly constitute the focus of our work.

Compared to traditional architectures in which the components are developed separately and rely on ad hoc representation formats, the use of a shared description formalism (probabilistic rules) to encode multiple reasoning tasks offers several advantages:
\begin{enumerate}
\item \textit{Transparency}:  The reliance on a common representation format provides a unified, transparent semantics for the dialogue state, since all state variables are described and related to one another through a principled framework grounded in probabilistic modelling.  This makes it possible to derive a semantic interpretation for the dialogue state as a whole -- in terms of e.g. of a joint probability distribution over the state variables. 

\item \textit{Domain portability}:   As most domain-specific knowledge is declaratively specified in the rules, the system architecture is essentially reduced to a generic platform for rule instantiation and probabilistic inference.  This declarative design greatly enhances the system portability across domains, since adapting a system to a new domain only requires a rewrite or extension of the domain-specific rules, without having to reconfigure or re-develop other components.  This stands in sharp contrast with ``black-box'' types of architectures where much of the task- and domain-specific knowledge is encoded in procedural form buried inside the implementation of the system components.

\item \textit{Flexible workflow}:  Probability rules allow for very flexible processing pipelines where state variables are allowed to depend or influence each other in any order and direction.  Models can be easily inserted or extended without requiring any change to the underlying platform. Furthermore, several models can be triggered concurrently on the same input/output variables.\footnote{Output distributions can indeed handle effect specifications arising from multiple, sometimes conflicting sources, as we have seen in Section \ref{sec:probruleinstantiation}.} This allows the system to take advantage of multiple, complementary processing strategies while ensuring that the dialogue state remains consistent. 

\item \textit{Joint optimisation}:  Finally, the use of a unified formalism allows domain models to be optimised jointly instead of being tuned in isolation from one another. Joint optimisation has recently gained much attention in the dialogue system community to overcome the fragmentation of current system architectures and attempt to directly optimise the end-to-end conversational behaviour of the system \citep[see also][]{Lemon:2011}. 

\end{enumerate}

Despite these merits, probabilistic rules cannot naturally model all dialogue reasoning tasks.  Modules such as speech recognition or speech synthesis depend in particular on external resources and processes and must be integrated separately in the system. The \opendial toolkit is designed to incorporate both rule-structured models and external components in its architecture. 
\subsection{Specification of dialogue domains}
\label{sec:domain-specification}

\subsubsection*{Dialogue domains}

A dialogue domain is represented in \opendial by a pair $\langle \mathcal{B}_0, \mathcal{M} \rangle$ composed of an initial dialogue state $\mathcal{B}_0$ and a set of rule-structured models $\mathcal{M}$. Dialogue domains are encoded in an XML format\footnote{Extensible Markup Language, cf. \begin{scriptsize}\url{http://www.w3.org/xmlâ€Ž}\end{scriptsize}.} with a specifically designed syntax. The specification of a dialogue domain in XML takes the following form:
\lstset{language=XML}
\begin{lstlisting}
    <domain> 
        <initialstate>
		<!-- initial state variable values -->
        </initialstate>

        <model trigger="trigger variables for model 1">
   		<!-- rules for model 1 -->
     	</model>
     	
     	...
     	
     	<model trigger="trigger variables for model n">
   		<!-- rules for model n -->
     	</model>

    </domain>
\end{lstlisting}

The initial dialogue state is represented by a list of state variables, each being associated with a particular probability distribution.  Most state variables are defined by a categorical distribution, according to the following skeleton:

\begin{lstlisting}
    <variable id="variable name"/>
        <value prob="prob for first value">first value</value>
        <value prob="prob for second value">second value</value>
		...
        <value prob="prob for nth value">nth value</value>
   </variable>  
\end{lstlisting}

State variables with continuous ranges can also be encoded with uniform, Gaussian, or Dirichlet probability density functions (provided the required parameters are specified). 

\subsubsection*{Probabilistic rules}

Each model specification encompasses a number of probability or utility rules also encoded in XML. Listing \ref{listing:xml1} illustrates an example of probability rule encoded in XML. The rule expresses the probability of the user intention $\mathit{Release}(X)$, which corresponds to the action of putting down a carried object $X$ on the ground.  The probability of this user intention is naturally dependent on (1) whether the object $X$ is currently carried by the robot and (2) whether the previous task was completed. The values of the Dirichlet parameters $\boldsymbol\theta_2$ and $\boldsymbol\theta_3$ are unknown, but we may presuppose that the Dirichlet $\boldsymbol\theta_3$ is highly skewed towards near-zero values for the first dimension, since the probability of the user asking the robot to release an object which is currently not being carried is most likely very small. 

Each rule is divided in a number of cases, each containing a (possibly empty) condition and a set of (possibly empty) effects.  Conditions can include several sub-conditions combined by conjunction or disjunction operators (the default operator being a conjunction). Each basic condition is denoted by an \texttt{if} markup and is composed of a variable, a value, and a (optionally) relation that must hold between the variable and the value. The default relation is equality, but relations may also correspond to inequalities ($\neq$, $<$ and $>$), set inclusion ($\in$ and $\notin$) or string matching operations.  Note that underspecified variables are wrapped in curly brackets \{ \} to distinguish them from normal values. Underspecified variables can occur both in the variables and values of the conditions. Effects are associated with probabilities that can either be fixed or correspond to parameters to learn such as the Dirichlet distributions $\boldsymbol\theta_2$ and $\boldsymbol\theta_3$ in the example. Effects can similarly contain underspecified variables denoted by curly brackets.  Although the effects in the example only have one single output variable, rules can in the general case include multiple \texttt{set} markups to express complex effects ranging over more than one output variable. 

Utility rules are defined in the same manner.  An example of utility rule specified in XML is given in Listing \ref{listing:xml2}.  The rule defines the utility of asking the user to confirm the correctness of the last system action $\mathit{Demonstrate}(X)$ after a certain amount of silence, divided here in three cases. 

\begin{lstlisting}[label=listing:xml1,caption=Example of probability rule in XML format, float=p,captionpos=b]
    <rule>
        <quantifier id="O"/>
        <case>
            <condition>
                <if var="completed-task" value="true" />
                <if var="carried" value="{O}" relation="contains" />
            </condition>
            <effect prob="theta_2[0]">
                <set var="i_u" value="Release({O})" />
            </effect>
            <effect prob="theta_2[1]" />
        </case>
        <case>
            <condition>
                <if var="completed-task" value="true" />
            </condition>
            <effect prob="theta_3[0]">
                <set var="i_u" value="Release({O})" />
            </effect>
            <effect prob="theta_3[1]" />
        </case>
    </rule>
\end{lstlisting}


\begin{lstlisting}[label=listing:xml2,caption=Example of utility rule in XML format, float=p,captionpos=b]
    <rule>
        <quantifier id="X"/>
        <case>
            <condition>
                <if var="silence" value="3" relation=">"/>
                <if var="a_m" value="Demonstrate({X})"/>
            </condition>
            <effect utility="theta_{confirmation3}">
                <set var="a_m" value="AskConfirmation"/>
            </effect>
        </case>
        <case>
            <condition>
                <if var="silence" value="2" relation=">"/>
                <if var="a_m" value="Demonstrate({X})"/>
            </condition>
            <effect utility="theta_{confirmation2}">
                <set var="a_m" value="AskConfirmation"/>
            </effect>
        </case>
        <case>
            <condition>
                <if var="silence" value="1" relation=">"/>
                <if var="a_m" value="Demonstrate({X})"/>
            </condition>
            <effect utility="theta_{confirmation1}">
                <set var="a_m" value="AskConfirmation"/>
            </effect>
        </case>
    </rule>
\end{lstlisting}

\subsubsection*{Parameters}

When operating in learning mode, dialogue domains must be associated with parameter variables defined together with their prior probability distribution.  This prior can take the form of a categorical, Dirichlet, Gaussian or uniform distributions.  As an illustration, the prior distribution for the parameter variable $\boldsymbol\theta_2 \sim \mathrm{Dirichlet}(1,2)$ is specified as:
\begin{lstlisting}
    <variable id="theta_2">
        <distrib type="dirichlet">
            <alpha>1</alpha>
            <alpha>2</alpha>
        </distrib>
    </variable>
\end{lstlisting}

Other types of prior distributions are defined in a similar manner. 

\subsection{Core algorithms}
\label{sec:corealgorithms}

We survey below some technical questions related to the \opendial implementation of the core algorithms presented through this thesis.  

\subsubsection*{Inference}

Probabilistic inference forms a key element in the state update process.  Two basic types of inference are implemented in \opendial: \begin{enumerate}
\item The first is variable elimination, which is an exact inference method developed in \cite{ZhangP96}.  The algorithm operates by manipulating matrices through summation and pointwise products. The particular implementation in \opendial follows the method presented in \cite[][p. 1101]{Koller+Friedman:09}, which generalises classical variable elimination to also include decision networks.
\item The second inference algorithm is likelihood weighting, which is an approximate inference method relying on importance sampling. As explained in Section \ref{sec:inference}, likelihood weighting generates samples based on the topological ordering of the graphical model. Each sample is associated with a particular weight that represents its likelihood in the light of the provided evidence.  The final estimates correspond to the weighted averages of the samples. 
\end{enumerate}

Each inference algorithm has its own strengths and weaknesses. Variable elimination is able to deliver exact results and is often the most efficient inference method for small graphical models.  It however suffers from scalability problems when applied to models that are densely interconnected and/or include continuous variables. Likelihood weighting is easier to scale to larger networks and can be straightforwardly applied to hybrid models with both discrete and continuous variables. Large numbers of samples are unfortunately required to reach reasonably accurate estimates.

In order to combine the best of both approaches, a switching mechanism has therefore been integrated to \opendial to select the inference method which is best suited to each probabilistic query.  The mechanism proceeds as follows. For each probabilistic query, three elements are extracted: the maximum branching factor of the network, the number of continuous variables, and the number of variables specified in the query. These elements are then matched against predefined thresholds. If at least one threshold is exceeded, likelihood weighting is selected to perform the inference, while variable elimination is chosen in the remaining cases. 

\subsubsection*{Sampling techniques}

The use of likelihood weighting necessitates the implementation of efficient sampling techniques for each possible probability distribution.  In the interest of the statistically inclined reader, we describe below how sampling was implemented for various types of distributions: 
\begin{itemize}
\item \textit{Categorical distributions}:  Sampling a categorical distribution is done through inverse random sampling, following the method described in \cite[][p. 489]{Koller+Friedman:09}. When constructing the distribution, the variable values are sorted (according to e.g. lexicographic order), and a cumulative density function (CDF) calculated relative to this order.  Sample values are then extracted at runtime by (1) generating a pseudo-random float number between 0 and 1 and then (2) locating the greatest number in the CDF that is less than or equal to the number just generated.  The last operation is done through binary search. The value corresponding to this number is then selected as the sample.
\item \textit{Uniform distributions}:  Uniform distributions are directly sampled as $g \times (b-a) + a$, where $g$ is a pseudo-random number between 0 and 1, and $a,b$ are the distribution boundaries.

\item \textit{Normal distributions}:  Normal distributions are sampled in \opendial via the well-known Box-Muller method \citep{rBOX58a}, which is able to produce two sample values for  a given normal distribution based on two pseudo-random numbers.

\item \textit{Dirichlet distributions}:  Sampling Dirichlet distributions relies on a slightly more intricate procedure based on Gamma sampling.   The first step is to derive $K$ samples from $\mathrm{Gamma} (\alpha_i, 1)$ with $K$ denoting the dimension of the Dirichlet and $1 \leq i \leq K$.  This sampling procedure is implemented in \opendial using the method presented in \cite{cheng1979}.  The sampled Dirichlet value is then defined as $[x_1,...x_K]$ where $x_i = \frac{y_i}{\sum_{j=1}^K y_j}$ and $y_i$ is the sampled Gamma value for dimension $i$.
\item \textit{Kernel distributions}:  We sample non-parametric distributions defined via kernel density estimation (KDE) in two steps. The first is to draw at random one point $x_i$ from the set of points $x_1,...x_n$ included in the KDE. A value is then drawn from the kernel associated with the point. In our case, this corresponds to drawing a sample from the normal distribution $\mathcal{N}(x_i,h)$ centered at $x_i$ and of variance $h$ (the bandwidth). 
\end{itemize}

\subsubsection*{Forward planning}

The implementation of the forward planning algorithm closely follows the procedure outlined in Section \ref{sec:modelbased}.  The search tree is constructed in a breadth-first manner, until either the planning horizon has been reached or the planner has run out of time.  The latter condition relies on the use of a time-out function to ensure that the planner does not exceed specific time limits.

In order to generate a set of possible observations in a given state $\mathcal{B}$ (line 6 of Algorithm \ref{algo:planning}), the planner locates all predictive state variables (denoted with a superscript $p$) currently present in the dialogue state and draws a sample value for each.  In the experiment presented in Section \ref{sec:rllearning-experiments}, the predictive variable corresponds to the next user action, and the generated observations will therefore reflect possible values for this user action.

For tractability reasons, \opendial limits the maximum number of actions and observations that are branched out at each point in the search tree.  The actions are selected on the basis of their reward values in the current state -- i.e. only the $n$ actions with highest reward are selected, with $n$ corresponding to an arbitrary threshold. The observations are filtered based on their likelihood of occurrence.  The planner thus only expands the tree with the $m$ most likely observations, where $m$ is another threshold. The two thresholds are currently manually tuned, but could in principle correspond to additional parameters to optimise during learning. 

\subsection{Comparison with other architectures}
\label{sec:archi-comparison}

The construction of generic, domain-independent architectures is a recurring theme in dialogue systems research, and there is a clear trend towards the development of platforms composed of generic or reusable components. We present below the most important architectures currently used in the field and contrast their design with the one followed in the \opendial toolkit. Finally, we discuss the current limitations of the presented framework.

\subsubsection*{Existing software frameworks}

Information-state approaches are closely related to the framework presented in this chapter. The TrindiKit architecture presented by \cite{Larsson:2000} relies on a shared information state accessed by various system modules and a rich repository of rules. A control module is in charge of the system scheduling for the whole architecture. The system modules can also connect to external resources such as databases and plan libraries.   The TrindiKit is a platform for constructing and evaluating dialogue engines and is designed to be fully domain-independent. The related DIPPER architecture described in \citep{Bos2003} is built on similar principles as the TrindiKit, but simplifies the architecture and the update language to its essential features.  DIPPER employs the Open Agent Architecture as communication protocol.

The idea of domain independence is also taken up by plan-based approaches such as the TRIPS \citep[The Rochester Interactive Planning System, cf. ][]{Allen:2000:AGD:973935.973937}. TRIPS uses an agent-oriented architecture comprised of multiple modules working together to recognise the intentions of the human user and fulfilling the system goals.  Most reasoning tasks are explicitly cast as planning problems, from high-level planning to response planning and surface realisation.  

One of the most mature platform for prototyping dialogue systems is Olympus and its associated dialogue management framework Ravenclaw \citep{Bohus:2007,Bohus:2009:RDM:1518321.1518367}.  The Olympus architecture is built on top of a centralised message-passing infrastructure in which modules can be plugged in and out to suit the needs of the application.  Ravenclaw is a plan-based, task-independent dialogue engine which is fully integrated in Olympus.  Ravenclaw supports mixed-initiative interaction and integrates dedicated functions for conversational skills such as error handling, timing and turn-taking. Action selection is based on a hierarchical decomposition of tasks whose execution is sequentially ordered via an agenda. 

The agent-based Jaspis architecture \citep{jaspis2004} also has multiple points of contact with the \opendial toolkit, as it similarly revolves around a shared representation of the system state.  Jaspis components are themselves split into agents (in charge of decision-making), evaluators (in charge of selecting the most suitable agent in a particular situation) and managers (in charge of the general coordination of the components). Jaspis is designed to allow for distributed setups with dedicated mechanisms for the coordination and synchronisation of concurrent modules.  The architecture also aims to facilitate system-level adaptivity.  Compared to the \opendial toolkit, Jaspis offers more advanced support for distributed and parallel setups, but at the cost of an increased system complexity.  

Most MDP and POMDP-based dialogue architectures line up system components in a single processing sequence. The prototype systems developed for the TALK and CLASSIC projects \citep{Henderson:2008,Lemon:2012} and the related BUDS POMDP system\footnote{Bayesian Update of Dialogue State, cf. \begin{scriptsize}\url{http://mi.eng.cam.ac.uk/~mh521/nipsdemo12/}\end{scriptsize}} are structured into pipelines where each component takes a probability distribution over input hypotheses and generates another distribution over possible outputs.  Contrary to the logic-based architectures mentioned above, these approaches explicitly account for state uncertainty and allow for automatic optimisation of dialogue policies. 


\subsubsection*{Comparison and discussion}

The most important difference between \opendial and the TrindiKit, DIPPER, TRIPS, Olympus/Ravenclaw, and Jaspis architectures outlined above lies in the treatment of uncertainty and the use of parametrised models that can be optimised from data. \opendial is designed from the ground up to handle uncertain knowledge and stochastic relations between variables thanks to its probabilistic representation of the system state. The \opendial toolkit can therefore be viewed as an attempt to combine the flexibility of information-state architectures with the robustness and adaptivity of statistically optimised dialogue systems.  

The \opendial toolkit captures uncertainty occurring at all processing stages.  This stands in contrast with other hybrid dialogue architectures where probabilistic reasoning techniques coexist with classical symbolic components that only consider deterministic inputs, typically by reducing N-best lists to their single most-likely hypothesis. An unfortunate consequence of this heterogeneity is that, while speech recognition results often include explicit measures of uncertainty -- in the form of e.g. confidence scores --, this uncertainty is often lost at higher stages of reasoning, such as semantic interpretation and dialogue management. 

It should finally be noted that some dialogue architectures decompose dialogue management into two or more distinct behavioural layers.  The Olympus framework incorporates for instance both an interaction manager in charge for the low-level control of the conversational floor, and a dialogue manager in charge of higher-level dialogue decisions.  The TRIPS architecture similarly represents dialogue management as a cluster of modules encompassing discourse management, discourse context management, plan management, and a behaviour agent in charge of determining the overall behaviour. By contrast, \opendial leaves the system designer free to frame decision-making in one, two or more layers, depending on the particular needs of the domain.\footnote{This can be practically realised by creating distinct utility models for action selection.  Hierarchical decision policies can notably be captured by lining up utility models in a top-down cascade of triggers.}


%Many dialogue architectures have been specifically engineered to handle situated or multimodal domains. The WITAS architecture \citep{LemonBGP01} is one of the first dialogue architecture devoted to multimodal settings (in their case an autonomous mobile robot).  The SmartKom system \citep{smartkom} describes the development of another type of multimodal system combining speech, gesture, and mimics.  Our previous work on situated human-robot interaction \citep{cosybook:dialogue} illustrates how dialogue processing can be coupled to sensorimotor components in a bidirectional manner. The Ariadne multimodal dialogue architecture comes especially close to this work due to its use of a declarative specification of the dialogue domain. In a more theoretical realm, the framework of semiotic schemas developed in \cite{Roy05} attempts to bridge the gap between linguistic understanding, action and perception through the use of grounded schemas, with application to interactive robots.  

\subsubsection*{Limitations}

One aspect of dialogue architectures that has not been really touched upon in our work is the question of incremental processing.  As explained in the background chapter, dialogue processing should ideally be performed incrementally and output partial hypotheses as early as possible in the system workflow. The InproTK architecture presented in \cite{schlangen2009general,Baumann:2012} is specifically designed to support across-the-board incremental processing, from speech recognition to dialogue management and speech synthesis. Although the current implementation of \opendial cannot claim to be incremental, we hypothesise that the formalism of probabilistic rules could be extended to allow for incremental processing without major difficulties, since the chain of related hypotheses is already explicitly captured in the conditional dependencies generated by the instantiated rules.  A probability change in one initial hypothesis (e.g. the user utterance) can therefore be directly reflected in all hypotheses that depend on it (e.g. the corresponding user intention). Modifying the state update algorithm to run in a fully incremental manner without harming the system performance is however a non-trivial engineering task, which could constitute an interesting topic for future work. Similarly, the turn-taking behaviour integrated in \opendial remains relatively rudimentary and could certainly be improved to allow for e.g. barge-ins and fragmented utterances.

While the dialogue system used for our experiments does include basic modules for robot perception and control (cf. next section), the architecture does not yet scale to full-scale multi-modal processing, as such extensions would require dedicated mechanisms for information fusion and fission. Our previous work on situated human-robot interaction \citep{cosybook:dialogue} illustrates how dialogue processing can be coupled to sensorimotor components in a bidirectional manner.  

On a more practical note, it should finally be pointed out that \opendial is at the time of writing at an early stage of development and does not benefit from the years of incremental refactoring and testing shown by more mature architectures such as TrindiKit or Olympus/Ravenclaw. Further development work is certainly needed to move \opendial from being an advanced research prototype to a stable, robust development platform that can be directly employed by the dialogue system community at large. 

%\footnote{In this respect, it is interesting to draw a parallel between dialogue systems and other fields of NLP such as syntactic parsing. Before the sixties, most parsers relied on procedural routines buried in the code.  One of the major advances in parsing has come from the decision to separate the domain knowledge (in this case, the lexicon and grammar) on one hand, and the parsing algorithms on the other hand.  We would argue that dialogue systems could also benefit from a  between declarative knowledge (i.e. task- and domain-specific models) and generic processing functionalities (i.e. algorithms for reasoning, learning and planning under uncertainty.)}

\section{System integration}
\label{sec:system-integration}
The \opendial toolkit was used as a foundation to construct a concrete, end-to-end dialogue system for human-robot interaction.  We detail in the next pages the practical design of this dialogue system as well as the graphical user interface implemented to monitor and control the system behaviour (and its internal dialogue state) in real-time. 

\subsection{Architecture}

In addition to the \opendial core components -- comprising the dialogue state and its update mechanism based on probabilistic rules -- the dialogue system employed in our experiments also integrated specific modules for speech recognition and synthesis, robot perception and motor control.  The generic architecture for the system is shown in Figure \ref{fig:impl_architecture}. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.30]{imgs/impl_architecture.pdf}
\caption{Generic system architecture employed in the experiments.}
\label{fig:impl_architecture}
\end{figure}

The system modules can operate in either synchronous and asynchronous mode: \begin{itemize}
\item Synchronous modules continuously monitor the dialogue state for relevant changes.  Their activation is thus in synchrony with the update events generated by the dialogue state.
\item Asynchronous modules run independently of the dialogue state.  They typically relate to visual or speech perception tasks. When new observations are made, they update the dialogue in accordance with the recognised user utterances or perceived contextual changes.

\end{itemize}

All modules have access to the complete dialogue state and can therefore exploit the full set of state variables (including generic contextual information) in their processing. 

\subsection{Individual modules}

We describe below the modules shown in Figure \ref{fig:impl_architecture} and explain their role and internal structure. It should be emphasised that the focus of the present thesis is on dialogue management.  The other system modules are therefore deliberately limited to simple, ``shallow'' processing methods in order to concentrate the implementation efforts on the dialogue manager. Many of these modules could however be extended to employ more sophisticated techniques, in particular in regard to speech recognition, natural language understanding and generation. 

\subsubsection*{Speech recognition}

Speech recognition is performed on the robot platform, using a commercial, off-the-shelf speech recognition engine (Vocon 3200 from Nuance).  Four microphones placed on the robot head are used for the sound capture.  The placement of the microphones on the robot allows the user to interact with the robot in a natural manner, without needing to resort to head-mounted microphones. The improved naturalness that results from this setup comes however at the price of a slightly degraded sound quality due to the larger (and varying) distance between the sound source and the microphones, combined with the presence of noisy mechanical motors placed at only a few centimetres from the microphones. 

The acoustic model employed in all experiments is provided along with the speech recognition package and is optimised for American English. The language model takes the form of context-free recognition grammars in Bachus-Naur Form (BNF). Distinct grammars were used to cover the domain of discourse of each experiment. The grammars were designed by hand, based on the Wizard-of-Oz transcripts collected in our empirical studies (cf. previous chapters). Grammars can be dynamically attached or removed from the engine at runtime, thereby allowing the system to adapt the recognition to the current dialogue context. This functionality is however not yet exploited in the current architecture -- but see e.g. \cite{ESSLLI2008-springerreprint} for a description of our prior research work on this issue. As the recognition engine only generates hypotheses with raw, unnormalised confidence scores, a normalisation routine is used to convert them to proper probability distributions $P(\tilde{u}_u)$.\footnote{The conversion between confidence scores and probabilities is manually tuned in the current implementation.  Future work may however rely on more principled estimation techniques such as the ones outlined in \cite{Williams08}.}


\subsubsection*{Natural language understanding}

The goal of natural language understanding (NLU) is to map a collection of utterance hypotheses $\tilde{u}_u$ to a corresponding set of dialogue act hypotheses $\tilde{a}_u$ expressing the semantic and pragmatic content of the user input. This understanding step is decomposed in our implementation in two tasks, dialogue act recognition and visual reference resolution.  

The goal of dialogue act recognition is to construct the logical form representing the pragmatic meaning of the utterance. One should note that user utterances may contain more than one dialogue act, as for instance in \utt{yes and now pick the blue object}.  A collection of domain-specific templates was designed by hand to convert surface forms into logical representations of dialogue acts.  Although this approach does not allow for ``deep'' semantic extraction, it was shown to perform well in our dialogue domains. Future work may replace this template-based method with a data-driven semantic parser based on e.g.  dependency parsing \citep{Nivre:Etal07}.  

Reference resolution is used to map linguistic expressions referring to objects in the visual context to their corresponding object identifier. The properties stated in the linguistic expressions are first matched against the set of possible references.  In case the description remains ambiguous (i.e. more than one object matches the linguistic expression), the references can be further ranked according to their visual saliency, defined here in terms of their physical distance to the robot. 

Natural language understanding is practically implemented in \opendial via probabilistic rules.  As seen in Section \ref{sec:amodelling}, the formalism of probabilistic rules already includes special-purpose operators for string manipulation and can thus readily encode the shallow templates used for dialogue act recognition.  Rule $r_{15}$ below is an example of such rule.  The rule lists three regular expression patterns associated with the dialogue act $\mathrm{MoveArm(Left,Down)}$.  If the value for the user utterance variable $u_u$ matches at least one of the patterns, the dialogue act $a_u$ is classified as $\mathrm{MoveArm(Left,Down)}$:
\begin{align*}
r_{15}: &\;\;\textbf{if} \ (u_u \textit{ matches } \text{`` (*) left arm down''} ) \\ 
& \lor (u_u \textit{ matches } \text{`` (*) lower (the\,|\,your) left arm''} ) \\
& \lor (u_u \textit{ matches } \text{`` (*) down (the\,|\,your) left arm''}   )  \ \ \textbf{then} \\ 
& \; \; \begin{cases} P\left(a_u = \mathrm{MoveArm(Left,Down)}\right) = 1.0 \end{cases}
\end{align*}

\subsubsection*{Dialogue management}

Dialogue management follows the procedure outlined in Section \ref{sec:processing-workflow} and will thus not be repeated here. For each update of the dialogue state, the dialogue manager triggers the corresponding rule-structured models, and selects the next action to perform (if any). 

\subsubsection*{Natural language generation}

If the system action selected by the dialogue manager is non-empty and denotes a verbal action, the natural language generation module is triggered.  As for natural language understanding, the generation component of \opendial is based on a manually designed collection of templates.  The processing direction is however the reverse one of  natural language understanding, as the templates are applied to convert a logical representation of the communicative goal into a surface form. 

The generation templates are also encoded with probabilistic rules.  The rules correspond here to utility rules, since generation reflects a decision-making task.  As an example, rule $r_{16}$ generates the system response $u_m$ given the system act $a_m=\mathrm{Acknowledgement}$.  The rule specifies in this case three alternatives with equal utility:
\begin{align*}
r_{16}: &\;\;\textbf{if} \ (a_m = \mathrm{Acknowledgement} )  \ \ \textbf{then} \\ 
& \;\; \begin{cases} U(u_m'=\text{``ok''}) = 1 \\ U(u_m'=\text{``great''}) = 1 \\ U(u_m'=\text{``thanks''}) = 1 \end{cases}
\end{align*}

The specification of multiple realisations allows for some variation in the system behaviour, since the system will automatically select one realisation at random, due to the equal utility assigned to the alternative realisations.

\subsubsection*{Speech synthesis}

Speech synthesis is performed on the robot, using an off-the-shelf speech synthesis engine developed by Acapela\footnote{\begin{scriptsize}\url{http://www.acapela-group.com}\end{scriptsize}}. The synthesis engine is based on unit selection. The output speech signal is then sent to two speakers placed on the robot head. To avoid spurious recognition results, the speech recognition is automatically disabled when the robot is speaking.   
 
\subsubsection*{Robot perception}

The robot can detect simple physical objects present in the visual scene. The object detection is done based on the vision libraries bundled with the robotic platform. Special markings are placed on top of the objects to facilitate the object recognition and the visual servoing. 

\subsubsection*{Robot motion control}

Various types of physical movements were developed in our experiments, including: \begin{itemize}
\item generic body movements: rotating the arms and the head in various directions,
\item spatial navigation: moving forward and backward, turning left and right,
\item object manipulation: grasping and releasing objects.  
\end{itemize}

All the movements were programmed using the motion control libraries available on the robot. The object manipulation relies on the use of permanent magnets attached to the robot hands. 


\subsection{Graphical interface}

The graphical user interface developed for the \opendial toolkit allows the system designer to monitor and control in real-time the state of the system.  The interface is divided in two  views, shown as distinct tabs in the application window: the chat window and the dialogue state monitor.

\subsubsection*{Chat window}
The chat window presents the interaction history as a chat window, as illustrated by the screenshot in Figure \ref{fig:gui-chatbox}.


\begin{figure}[h]
\centering
\includegraphics[scale=0.40]{imgs/gui-chatbox.png}
\caption{Graphical user interface showing the interaction history.}
\label{fig:gui-chatbox}
\end{figure}

  The user inputs are shown as N-best lists together with their corresponding probabilities. 
In addition to monitoring the interactions, the chat window can also be used to test the dialogue system by typing new user and system inputs in the input field at the bottom of the window.  The agent role can be switched in the drop-down field in the bottom right corner. 


\subsubsection*{Dialogue state monitor}

To allow the system designer to easily inspect the content of the dialogue state, a state visualisation tool has also been integrated into \opendial .  The monitor provides a visual representation of the dialogue state in the form of a directed graph with nodes corresponding to the state variables and directed edges corresponding to conditional dependencies.\footnote{The graphs are rendered with JUNG, an open source toolkit for drawing graphs: \begin{scriptsize}\url{http://jung.sourceforge.net}\end{scriptsize}.} An example of graph layout is shown in Figure \ref{fig:gui-bn}. The graph is dynamically refreshed after each update of the dialogue state, using standard graph drawing algorithms to optimise the layout of nodes on the screen. 

In addition to depicting the current dialogue state, the dialogue state monitor can also record and store previous dialogue states.  The dialogue state to visualise can be selected among the list on the left side of the window. This functionality is particularly useful to compare dialogue states with one another and analyse how the dialogue state is evolving over time. 

\begin{figure}[p!] 
\begin{center}
\includegraphics[scale=0.40]{imgs/gui-bn.png}
\end{center} 
\caption{Visualisation of the current dialogue state.}
\label{fig:gui-bn}
\end{figure}

\begin{figure}[p!] 
\begin{center}
\includegraphics[scale=0.40]{imgs/gui-distribviewer.png}
\end{center} 
\caption{Visualisation of a discrete probability distribution $P(a_u^p)$ and a continuous probability distribution $P(\theta_{15})$ in the dialogue state monitor.}
\label{fig:gui-distribviewer}
\end{figure}


The graph can be manipulated by the user in multiple ways in order to e.g. analyse the content of specific state variables, add or remove evidence, or request the calculation of marginal distributions on selected set of variables.  The inference results are shown in the text area at the bottom of the window.  In addition, the system designer can also directly view the shape of selected probability distributions using the distribution viewer tool illustrated in Figure \ref{fig:gui-distribviewer}. Discrete probability distributions are shown as histograms, while continuous probability distributions are represented by their probability density functions.\footnote{The probability distributions are rendered with the open source toolkit JFreeChart: \begin{scriptsize}\url{http://jfreechart.sourceforge.net}\end{scriptsize}.} 


\section{Conclusion}

This chapter exposed how the formalism of probabilistic rules is practically implemented in the \opendial toolkit and its instantiation in a spoken dialogue system for human-robot interaction. The first section presented the general architecture of the toolkit, the declarative specification of dialogue domains in a XML format, and the integration of efficient algorithms for inference, sampling and planning. We notably discussed the benefits of using a shared description formalism in terms of transparency, portability, flexibility and adaptivity over traditional ``black-box'' architectures. We also compared \opendial to other existing dialogue system architectures, and pointed out a number of limitations in the current implementation of the toolkit, such as its lack of incremental processing and its limited turn-taking behaviour. 

The \opendial toolkit is inspired by both symbolic and statistical approaches to dialogue, and combines an information-state architecture with probabilistic reasoning based on rule-structured models.  As stated in the introduction chapter, the long-term goal of the \opendial framework is to bridge the gap between symbolic approaches to dialogue management, which usually concentrate on capturing rich interaction patterns, and probabilistic approaches, more focused on aspects related to noise, uncertainty, and learnability. 

The hybrid design  of \opendial allows system designers to exploit powerful generalisations in the dialogue domain specification without sacrificing the probabilistic nature of the model. Another important side benefit of probabilistic rules is their improved readability for human experts, which are able to leverage their domain knowledge in the form of pragmatic rules, common sense assumptions, or task-specific constraints. Furthermore, the internal organisation of rules into models enables dialogue domains to be specified in a modular fashion, by clustering rules into distinct models.  Some models may therefore reflect highly domain-specific knowledge while others encode generic interaction patterns that can be easily ported to other applications.  

The last section described the integrated dialogue system used to carry out the practical experiments presented in Chapters \ref{chap:wozlearning}, \ref{chap:rllearning} and \ref{chap:user-evaluation}.  The system comprised both synchronous and asynchronous components and included dedicated modules for speech recognition, speech synthesis, robot perception and motor control.  Natural language understanding, dialogue management and natural language generation were encoded with models structured with probability rules. 

The next chapter will then flesh out the practical deployment of this system in a user evaluation experiment. 